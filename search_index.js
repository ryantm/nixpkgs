const corpus = [{ "url": ".\/", "title": "Preface", "text": "PrefaceThe Nix Packages collection (Nixpkgs) is a set of thousands of packages for the Nix package manager, released\nunder a permissive MIT\/X11 license. Packages are available for several platforms, and can be used with the Nix package\nmanager on most GNU\/Linux distributions as well as NixOS.\n\nThis manual primarily describes how to write packages for the Nix Packages collection (Nixpkgs). Thus it’s mainly for\npackagers and developers who want to add packages to Nixpkgs. If you like to learn more about the Nix package manager\nand the Nix expression language, then you are kindly referred to the Nix manual. The NixOS distribution is documented in\nthe NixOS manual.\n\nOverview of NixpkgsNix expressions describe how to build packages from source and are collected in the nixpkgs\nrepository. Also included in the collection are Nix expressions for NixOS modules. With these expressions the Nix\npackage manager can build binary packages.\n\nPackages, including the Nix packages collection, are distributed through channels. The collection is distributed for\nusers of Nix on non-NixOS distributions through the channel nixpkgs. Users of NixOS generally use one of the nixos-*\nchannels, e.g. nixos-19.09, which includes all packages and modules for the stable NixOS 19.09. Stable NixOS releases\nare generally only given security updates. More up to date packages and modules are available via the nixos-unstable\nchannel.\n\nBoth nixos-unstable and nixpkgs follow the master branch of the Nixpkgs repository, although both do lag the master\nbranch by generally a couple of days. Updates to a channel are distributed as soon as all tests for that channel pass,\ne.g. this table shows the status of tests for the nixpkgs channel.\n\nThe tests are conducted by a cluster called Hydra, which also builds binary packages from the Nix expressions in Nixpkgs\nfor x86_64-linux, i686-linux and x86_64-darwin. The binaries are made available via a binary cache.\n\nThe current Nix expressions of the channels are available in the nixpkgs repository in branches that correspond to the\nchannel names (e.g. nixos-19.09-small).\n" }
,{ "url": "using\/configuration\/", "title": "Global configuration", "text": "Global configurationNix comes with certain defaults about what packages can and cannot be installed, based on a\npackage's metadata. By default, Nix will prevent installation if any of the following criteria are true:\n\n  - The package is thought to be broken, and has had its meta.broken set to true.\n\n  - The package isn't intended to run on the given system, as none of its meta.platforms match the given system.\n\n  - The package's meta.license is set to a license which is considered to be unfree.\n\n  - The package has known security vulnerabilities but has not or can not be updated for some reason, and a list of\n    issues has been entered in to the package's meta.knownVulnerabilities.\n\nNote that all this is checked during evaluation already, and the check includes any package that is evaluated. In\nparticular, all build-time dependencies are checked. nix-env -qa will (attempt to) hide any packages that would be\nrefused.\n\nEach of these criteria can be altered in the nixpkgs configuration.\n\nThe nixpkgs configuration for a NixOS system is set in the configuration.nix, as in the following example:\n\n{\n  nixpkgs.config = {\n    allowUnfree = true;\n  };\n}\n\nHowever, this does not allow unfree software for individual users. Their configurations are managed separately.\n\nA user's nixpkgs configuration is stored in a user-specific configuration file located at ~\/.config\/nixpkgs\/config.nix.\nFor example:\n\n{\n  allowUnfree = true;\n}\n\nNote that we are not able to test or build unfree software on Hydra due to policy. Most unfree licenses prohibit us from\neither executing or distributing the software.\n\nInstalling broken packagesThere are two ways to try compiling a package which has been marked as broken.\n\n  - For allowing the build of a broken package once, you can use an environment variable for a single invocation of the\n    nix tools:\n    \n    $ export NIXPKGS_ALLOW_BROKEN=1\n\n  - For permanently allowing broken packages to be built, you may add allowBroken = true; to your user's configuration\n    file, like this:\n    \n    {\n      allowBroken = true;\n    }\n\nInstalling packages on unsupported systemsThere are also two ways to try compiling a package which has been marked as\nunsupported for the given system.\n\n  - For allowing the build of an unsupported package once, you can use an environment variable for a single invocation\n    of the nix tools:\n    \n    $ export NIXPKGS_ALLOW_UNSUPPORTED_SYSTEM=1\n\n  - For permanently allowing unsupported packages to be built, you may add allowUnsupportedSystem = true; to your user's\n    configuration file, like this:\n    \n    {\n      allowUnsupportedSystem = true;\n    }\n\nThe difference between a package being unsupported on some system and being broken is admittedly a bit fuzzy. If a\nprogram ought to work on a certain platform, but doesn't, the platform should be included in meta.platforms, but marked\nas broken with e.g. meta.broken = !hostPlatform.isWindows. Of course, this begs the question of what \"ought\" means\nexactly. That is left to the package maintainer.\n\nInstalling unfree packagesThere are several ways to tweak how Nix handles a package which has been marked as unfree.\n\n  - To temporarily allow all unfree packages, you can use an environment variable for a single invocation of the nix\n    tools:\n    \n    $ export NIXPKGS_ALLOW_UNFREE=1\n\n  - It is possible to permanently allow individual unfree packages, while still blocking unfree packages by default\n    using the allowUnfreePredicate configuration option in the user configuration file.\n    \n    This option is a function which accepts a package as a parameter, and returns a boolean. The following example\n    configuration accepts a package and always returns false:\n    \n    {\n      allowUnfreePredicate = (pkg: false);\n    }\n\n    For a more useful example, try the following. This configuration only allows unfree packages named roon-server and\n    visual studio code:\n    \n    {\n      allowUnfreePredicate = pkg: builtins.elem (lib.getName pkg) [\n        \"roon-server\"\n        \"vscode\"\n      ];\n    }\n\n  - It is also possible to allow and block licenses that are specifically acceptable or not acceptable, using\n    allowlistedLicenses and blocklistedLicenses, respectively.\n    \n    The following example configuration allowlists the licenses amd and wtfpl:\n    \n    {\n      allowlistedLicenses = with lib.licenses; [ amd wtfpl ];\n    }\n\n    The following example configuration blocklists the gpl3Only and agpl3Only licenses:\n    \n    {\n      blocklistedLicenses = with lib.licenses; [ agpl3Only gpl3Only ];\n    }\n\n    Note that allowlistedLicenses only applies to unfree licenses unless allowUnfree is enabled. It is not a generic\n    allowlist for all types of licenses. blocklistedLicenses applies to all licenses.\n\nA complete list of licenses can be found in the file lib\/licenses.nix of the nixpkgs tree.\n\nInstalling insecure packagesThere are several ways to tweak how Nix handles a package which has been marked as insecure.\n\n  - To temporarily allow all insecure packages, you can use an environment variable for a single invocation of the nix\n    tools:\n    \n    $ export NIXPKGS_ALLOW_INSECURE=1\n\n  - It is possible to permanently allow individual insecure packages, while still blocking other insecure packages by\n    default using the permittedInsecurePackages configuration option in the user configuration file.\n    \n    The following example configuration permits the installation of the hypothetically insecure package hello, version\n    1.2.3:\n    \n    {\n      permittedInsecurePackages = [\n        \"hello-1.2.3\"\n      ];\n    }\n\n  - It is also possible to create a custom policy around which insecure packages to allow and deny, by overriding the\n    allowInsecurePredicate configuration option.\n    \n    The allowInsecurePredicate option is a function which accepts a package and returns a boolean, much like\n    allowUnfreePredicate.\n    \n    The following configuration example only allows insecure packages with very short names:\n    \n    {\n      allowInsecurePredicate = pkg: builtins.stringLength (lib.getName pkg) <= 5;\n    }\n\n    Note that permittedInsecurePackages is only checked if allowInsecurePredicate is not specified.\n\nModify packages via packageOverridesYou can define a function called packageOverrides in your local\n~\/.config\/nixpkgs\/config.nix to override Nix packages. It must be a function that takes pkgs as an argument and returns\na modified set of packages.\n\n{\n  packageOverrides = pkgs: rec {\n    foo = pkgs.foo.override { ... };\n  };\n}\n\nDeclarative Package ManagementBuild an environmentUsing packageOverrides, it is possible to manage packages\ndeclaratively. This means that we can list all of our desired packages within a declarative Nix expression. For example,\nto have aspell, bc, ffmpeg, coreutils, gdb, nixUnstable, emscripten, jq, nox, and silver-searcher, we could use the\nfollowing in ~\/.config\/nixpkgs\/config.nix:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        gdb\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n    };\n  };\n}\n\nTo install it into our environment, you can just run nix-env -iA nixpkgs.myPackages. If you want to load the packages to\nbe built from a working copy of nixpkgs you just run nix-env -f. -iA myPackages. To explore what's been installed, just\nlook through ~\/.nix-profile\/. You can see that a lot of stuff has been installed. Some of this stuff is useful some of\nit isn't. Let's tell Nixpkgs to only link the stuff that we want:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        gdb\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"\/share\" \"\/bin\" ];\n    };\n  };\n}\n\npathsToLink tells Nixpkgs to only link the paths listed which gets rid of the extra stuff in the profile. \/bin and\n\/share are good defaults for a user environment, getting rid of the clutter. If you are running on Nix on MacOS, you may\nwant to add another path as well, \/Applications, that makes GUI apps available.\n\nGetting documentationAfter building that new environment, look through ~\/.nix-profile to make sure everything is there\nthat we wanted. Discerning readers will note that some files are missing. Look inside ~\/.nix-profile\/share\/man\/man1\/ to\nverify this. There are no man pages for any of the Nix tools! This is because some packages like Nix have multiple\noutputs for things like documentation (see section 4). Let's make Nix install those as well.\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"\/share\/man\" \"\/share\/doc\" \"\/bin\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" ];\n    };\n  };\n}\n\nThis provides us with some useful documentation for using our packages. However, if we actually want those manpages to\nbe detected by man, we need to set up our environment. This can also be managed within Nix expressions.\n\n{\n  packageOverrides = pkgs: with pkgs; rec {\n    myProfile = writeText \"my-profile\" ''\n      export PATH=$HOME\/.nix-profile\/bin:\/nix\/var\/nix\/profiles\/default\/bin:\/sbin:\/bin:\/usr\/sbin:\/usr\/bin\n      export MANPATH=$HOME\/.nix-profile\/share\/man:\/nix\/var\/nix\/profiles\/default\/share\/man:\/usr\/share\/man\n    '';\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        (runCommand \"profile\" {} ''\n          mkdir -p $out\/etc\/profile.d\n          cp ${myProfile} $out\/etc\/profile.d\/my-profile.sh\n        '')\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        man\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"\/share\/man\" \"\/share\/doc\" \"\/bin\" \"\/etc\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" ];\n    };\n  };\n}\n\nFor this to work fully, you must also have this script sourced when you are logged in. Try adding something like this to\nyour ~\/.profile file:\n\n#!\/bin\/sh\nif [ -d $HOME\/.nix-profile\/etc\/profile.d ]; then\n  for i in $HOME\/.nix-profile\/etc\/profile.d\/*.sh; do\n    if [ -r $i ]; then\n      . $i\n    fi\n  done\nfi\n\nNow just run source $HOME\/.profile and you can starting loading man pages from your environment.\n\nGNU info setupConfiguring GNU info is a little bit trickier than man pages. To work correctly, info needs a database to\nbe generated. This can be done with some small modifications to our environment scripts.\n\n{\n  packageOverrides = pkgs: with pkgs; rec {\n    myProfile = writeText \"my-profile\" ''\n      export PATH=$HOME\/.nix-profile\/bin:\/nix\/var\/nix\/profiles\/default\/bin:\/sbin:\/bin:\/usr\/sbin:\/usr\/bin\n      export MANPATH=$HOME\/.nix-profile\/share\/man:\/nix\/var\/nix\/profiles\/default\/share\/man:\/usr\/share\/man\n      export INFOPATH=$HOME\/.nix-profile\/share\/info:\/nix\/var\/nix\/profiles\/default\/share\/info:\/usr\/share\/info\n    '';\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        (runCommand \"profile\" {} ''\n          mkdir -p $out\/etc\/profile.d\n          cp ${myProfile} $out\/etc\/profile.d\/my-profile.sh\n        '')\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        man\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n        texinfoInteractive\n      ];\n      pathsToLink = [ \"\/share\/man\" \"\/share\/doc\" \"\/share\/info\" \"\/bin\" \"\/etc\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" \"info\" ];\n      postBuild = ''\n        if [ -x $out\/bin\/install-info -a -w $out\/share\/info ]; then\n          shopt -s nullglob\n          for i in $out\/share\/info\/*.info $out\/share\/info\/*.info.gz; do\n              $out\/bin\/install-info $i $out\/share\/info\/dir\n          done\n        fi\n      '';\n    };\n  };\n}\n\npostBuild tells Nixpkgs to run a command after building the environment. In this case, install-info adds the installed\ninfo pages to dir which is GNU info's default root node. Note that texinfoInteractive is added to the environment to\ngive the install-info command.\n" }
,{ "url": "using\/overlays\/", "title": "Overlays", "text": "OverlaysThis chapter describes how to extend and change Nixpkgs using overlays. Overlays are used to add layers in the\nfixed-point used by Nixpkgs to compose the set of all packages.\n\nNixpkgs can be configured with a list of overlays, which are applied in order. This means that the order of the overlays\ncan be significant if multiple layers override the same package.\n\nInstalling overlaysThe list of overlays can be set either explicitly in a Nix expression, or through <nixpkgs-overlays>\nor user configuration files.\n\nSet overlays in NixOS or Nix expressionsOn a NixOS system the value of the nixpkgs.overlays option, if present, is\npassed to the system Nixpkgs directly as an argument. Note that this does not affect the overlays for non-NixOS\noperations (e.g. nix-env), which are looked up independently.\n\nThe list of overlays can be passed explicitly when importing nixpkgs, for example import <nixpkgs> { overlays = [\noverlay1 overlay2 ]; }.\n\nNOTE: DO NOT USE THIS in nixpkgs. Further overlays can be added by calling the pkgs.extend or pkgs.appendOverlays,\nalthough it is often preferable to avoid these functions, because they recompute the Nixpkgs fixpoint, which is somewhat\nexpensive to do.\n\nInstall overlays via configuration lookupThe list of overlays is determined as follows.\n\n1.  First, if an overlays argument to the Nixpkgs function itself is given, then that is used and no path lookup will be\n    performed.\n\n2.  Otherwise, if the Nix path entry <nixpkgs-overlays> exists, we look for overlays at that path, as described below.\n    \n    See the section on NIX_PATH in the Nix manual for more details on how to set a value for <nixpkgs-overlays>.\n\n3.  If one of ~\/.config\/nixpkgs\/overlays.nix and ~\/.config\/nixpkgs\/overlays\/ exists, then we look for overlays at that\n    path, as described below. It is an error if both exist.\n\nIf we are looking for overlays at a path, then there are two cases:\n\n  - If the path is a file, then the file is imported as a Nix expression and used as the list of overlays.\n\n  - If the path is a directory, then we take the content of the directory, order it lexicographically, and attempt to\n    interpret each as an overlay by:\n    \n      - Importing the file, if it is a .nix file.\n    \n      - Importing a top-level default.nix file, if it is a directory.\n\nBecause overlays that are set in NixOS configuration do not affect non-NixOS operations such as nix-env, the\noverlays.nix option provides a convenient way to use the same overlays for a NixOS system configuration and user\nconfiguration: the same file can be used as overlays.nix and imported as the value of nixpkgs.overlays.\n\nDefining overlaysOverlays are Nix functions which accept two arguments, conventionally called self and super, and return\na set of packages. For example, the following is a valid overlay.\n\nself: super:\n\n{\n  boost = super.boost.override {\n    python = self.python3;\n  };\n  rr = super.callPackage .\/pkgs\/rr {\n    stdenv = self.stdenv_32bit;\n  };\n}\n\nThe first argument (self) corresponds to the final package set. You should use this set for the dependencies of all\npackages specified in your overlay. For example, all the dependencies of rr in the example above come from self, as well\nas the overridden dependencies used in the boost override.\n\nThe second argument (super) corresponds to the result of the evaluation of the previous stages of Nixpkgs. It does not\ncontain any of the packages added by the current overlay, nor any of the following overlays. This set should be used\neither to refer to packages you wish to override, or to access functions defined in Nixpkgs. For example, the original\nrecipe of boost in the above example, comes from super, as well as the callPackage function.\n\nThe value returned by this function should be a set similar to pkgs\/top-level\/all-packages.nix, containing overridden\nand\/or new packages.\n\nOverlays are similar to other methods for customizing Nixpkgs, in particular the packageOverrides attribute described in\nGlobal configuration. Indeed, packageOverrides acts as an overlay with only the super argument. It is therefore\nappropriate for basic use, but overlays are more powerful and easier to distribute.\n\nUsing overlays to configure alternativesCertain software packages have different implementations of the same interface.\nOther distributions have functionality to switch between these. For example, Debian provides DebianAlternatives. Nixpkgs\nhas what we call alternatives, which are configured through overlays.\n\nBLAS\/LAPACKIn Nixpkgs, we have multiple implementations of the BLAS\/LAPACK numerical linear algebra interfaces. They\nare:\n\n  - OpenBLAS\n    \n    The Nixpkgs attribute is openblas for ILP64 (integer width = 64 bits) and openblasCompat for LP64 (integer width\n    = 32 bits). openblasCompat is the default.\n\n  - LAPACK reference (also provides BLAS)\n    \n    The Nixpkgs attribute is lapack-reference.\n\n  - Intel MKL (only works on the x86_64 architecture, unfree)\n    \n    The Nixpkgs attribute is mkl.\n\n  - BLIS\n    \n    BLIS, available through the attribute blis, is a framework for linear algebra kernels. In addition, it implements\n    the BLAS interface.\n\n  - AMD BLIS\/LIBFLAME (optimized for modern AMD x86_64 CPUs)\n    \n    The AMD fork of the BLIS library, with attribute amd-blis, extends BLIS with optimizations for modern AMD CPUs. The\n    changes are usually submitted to the upstream BLIS project after some time. However, AMD BLIS typically provides\n    some performance improvements on AMD Zen CPUs. The complementary AMD LIBFLAME library, with attribute amd-libflame,\n    provides a LAPACK implementation.\n\nIntroduced in PR #83888, we are able to override the blas and lapack packages to use different implementations, through\nthe blasProvider and lapackProvider argument. This can be used to select a different provider. BLAS providers will have\nsymlinks in $out\/lib\/libblas.so.3 and $out\/lib\/libcblas.so.3 to their respective BLAS libraries. Likewise, LAPACK\nproviders will have symlinks in $out\/lib\/liblapack.so.3 and $out\/lib\/liblapacke.so.3 to their respective LAPACK\nlibraries. For example, Intel MKL is both a BLAS and LAPACK provider. An overlay can be created to use Intel MKL that\nlooks like:\n\nself: super:\n\n{\n  blas = super.blas.override {\n    blasProvider = self.mkl;\n  };\n\n  lapack = super.lapack.override {\n    lapackProvider = self.mkl;\n  };\n}\n\nThis overlay uses Intel's MKL library for both BLAS and LAPACK interfaces. Note that the same can be accomplished at\nruntime using LD_LIBRARY_PATH of libblas.so.3 and liblapack.so.3. For instance:\n\n$ LD_LIBRARY_PATH=$(nix-build -A mkl)\/lib:$LD_LIBRARY_PATH nix-shell -p octave --run octave\n\nIntel MKL requires an openmp implementation when running with multiple processors. By default, mkl will use Intel's iomp\nimplementation if no other is specified, but this is a runtime-only dependency and binary compatible with the LLVM\nimplementation. To use that one instead, Intel recommends users set it with LD_PRELOAD. Note that mkl is only available\non x86_64-linux and x86_64-darwin. Moreover, Hydra is not building and distributing pre-compiled binaries using it.\n\nFor BLAS\/LAPACK switching to work correctly, all packages must depend on blas or lapack. This ensures that only one\nBLAS\/LAPACK library is used at one time. There are two versions of BLAS\/LAPACK currently in the wild, LP64 (integer size\n= 32 bits) and ILP64 (integer size = 64 bits). Some software needs special flags or patches to work with ILP64. You can\ncheck if ILP64 is used in Nixpkgs with blas.isILP64 and lapack.isILP64. Some software does NOT work with ILP64, and\nderivations need to specify an assertion to prevent this. You can prevent ILP64 from being used with the following:\n\n{ stdenv, blas, lapack, ... }:\n\nassert (!blas.isILP64) && (!lapack.isILP64);\n\nstdenv.mkDerivation {\n  ...\n}\n\nSwitching the MPI implementationAll programs that are built with MPI support use the generic attribute mpi as an input.\nAt the moment Nixpkgs natively provides two different MPI implementations:\n\n  - Open MPI (default), attribute name openmpi\n\n  - MPICH, attribute name mpich\n\nTo provide MPI enabled applications that use MPICH, instead of the default Open MPI, simply use the following overlay:\n\nself: super:\n\n{\n  mpi = self.mpich;\n}\n" }
,{ "url": "using\/overrides\/", "title": "Overriding", "text": "OverridingSometimes one wants to override parts of nixpkgs, e.g. derivation attributes, the results of derivations.\n\nThese functions are used to make changes to packages, returning only single packages. Overlays, on the other hand, can\nbe used to combine the overridden packages across the entire package set of Nixpkgs.\n\n<pkg>.overrideThe function override is usually available for all the derivations in the nixpkgs expression (pkgs).\n\nIt is used to override the arguments passed to a function.\n\nExample usages:\n\npkgs.foo.override { arg1 = val1; arg2 = val2; ... }\n\nimport pkgs.path { overlays = [ (self: super: {\n  foo = super.foo.override { barSupport = true ; };\n  })]};\n\nmypkg = pkgs.callPackage .\/mypkg.nix {\n  mydep = pkgs.mydep.override { ... };\n  }\n\nIn the first example, pkgs.foo is the result of a function call with some default arguments, usually a derivation. Using\npkgs.foo.override will call the same function with the given new arguments.\n\n<pkg>.overrideAttrsThe function overrideAttrs allows overriding the attribute set passed to a stdenv.mkDerivation call,\nproducing a new derivation based on the original one. This function is available on all derivations produced by the\nstdenv.mkDerivation function, which is most packages in the nixpkgs expression pkgs.\n\nExample usage:\n\nhelloWithDebug = pkgs.hello.overrideAttrs (oldAttrs: rec {\n  separateDebugInfo = true;\n});\n\nIn the above example, the separateDebugInfo attribute is overridden to be true, thus building debug info for\nhelloWithDebug, while all other attributes will be retained from the original hello package.\n\nThe argument oldAttrs is conventionally used to refer to the attr set originally passed to stdenv.mkDerivation.\n\nNote that separateDebugInfo is processed only by the stdenv.mkDerivation function, not the generated, raw Nix\nderivation. Thus, using overrideDerivation will not work in this case, as it overrides only the attributes of the final\nderivation. It is for this reason that overrideAttrs should be preferred in (almost) all cases to overrideDerivation,\ni.e. to allow using stdenv.mkDerivation to process input arguments, as well as the fact that it is easier to use (you\ncan use the same attribute names you see in your Nix code, instead of the ones generated (e.g. buildInputs vs\nnativeBuildInputs), and it involves less typing). \n\n<pkg>.overrideDerivation You should prefer overrideAttrs in almost all cases, see its documentation for the reasons why.\noverrideDerivation is not deprecated and will continue to work, but is less nice to use and does not have as many\nabilities as overrideAttrs. \n\nDo not use this function in Nixpkgs as it evaluates a Derivation before modifying it, which breaks package abstraction\nand removes error-checking of function arguments. In addition, this evaluation-per-function application incurs a\nperformance penalty, which can become a problem if many overrides are used. It is only intended for ad-hoc\ncustomisation, such as in ~\/.config\/nixpkgs\/config.nix. \n\nThe function overrideDerivation creates a new derivation based on an existing one by overriding the original's\nattributes with the attribute set produced by the specified function. This function is available on all derivations\ndefined using the makeOverridable function. Most standard derivation-producing functions, such as stdenv.mkDerivation,\nare defined using this function, which means most packages in the nixpkgs expression, pkgs, have this function.\n\nExample usage:\n\nmySed = pkgs.gnused.overrideDerivation (oldAttrs: {\n  name = \"sed-4.2.2-pre\";\n  src = fetchurl {\n    url = ftp:\/\/alpha.gnu.org\/gnu\/sed\/sed-4.2.2-pre.tar.bz2;\n    sha256 = \"11nq06d131y4wmf3drm0yk502d2xc6n5qy82cg88rb9nqd2lj41k\";\n  };\n  patches = [];\n});\n\nIn the above example, the name, src, and patches of the derivation will be overridden, while all other attributes will\nbe retained from the original derivation.\n\nThe argument oldAttrs is used to refer to the attribute set of the original derivation.\n\nA package's attributes are evaluated before being modified by the overrideDerivation function. For example, the name\nattribute reference in url = \"mirror:\/\/gnu\/hello\/${name}.tar.gz\"; is filled-in before the overrideDerivation function\nmodifies the attribute set. This means that overriding the name attribute, in this example, will not change the value of\nthe url attribute. Instead, we need to override both the name and url attributes. \n\nlib.makeOverridableThe function lib.makeOverridable is used to make the result of a function easily customizable. This\nutility only makes sense for functions that accept an argument set and return an attribute set.\n\nExample usage:\n\nf = { a, b }: { result = a+b; };\nc = lib.makeOverridable f { a = 1; b = 2; };\n\nThe variable c is the value of the f function applied with some default arguments. Hence the value of c.result is 3, in\nthis example.\n\nThe variable c however also has some additional functions, like c.override which can be used to override the default\narguments. In this example the value of (c.override { a = 4; }).result is 6.\n" }
,{ "url": "functions\/", "title": "Functions reference", "text": "Functions referenceThe nixpkgs repository has several utility functions to manipulate Nix expressions.\n" }
,{ "url": "functions\/library\/asserts\/", "title": "Assert functions", "text": "Assert functionslib.asserts.assertMsgassertMsg :: Bool -> String -> Bool\n\nPrint a trace message if pred is false. Intended to be used to augment asserts with helpful error messages.\n\npred\n\nCondition under which the msg should not be printed\n\nmsg\n\nMessage to print\n\nlib.asserts.assertMsg usage example\n\nassertMsg false \"nope\"\n=> false\nstderr> trace: nope\n\nassert (assertMsg (\"foo\" == \"bar\") \"foo is not bar, silly\"); \"\"\nstderr> trace: foo is not bar, silly\nstderr> assert failed at …\n\nlib.asserts.assertOneOfassertOneOf :: String -> ComparableVal -> List ComparableVal -> Bool\n\nSpecialized assertMsg for checking if val is one of the elements of a list. Useful for checking enums.\n\nname\n\nThe name of the variable the user entered val into, for inclusion in the error message\n\nval\n\nThe value of what the user provided, to be compared against the values in xs\n\nxs\n\nThe list of valid values\n\nlib.asserts.assertOneOf usage example\n\nlet sslLibrary = \"libressl\"\nin assertOneOf \"sslLibrary\" sslLibrary [ \"openssl\" \"bearssl\" ]\n=> false\nstderr> trace: sslLibrary must be one of \"openssl\", \"bearssl\", but is: \"libressl\"\n\n" }
,{ "url": "functions\/library\/attrsets\/", "title": "Attrset functions", "text": "Attrset functionslib.attrsets.attrByPathattrByPath :: [String] -> Any -> AttrSet -> Any\n\nReturn an attribute from nested attribute sets.\n\nattrPath\n\nA list of strings representing the path through the nested attribute set set\n\ndefault\n\nDefault value if attrPath does not resolve to an existing value\n\ne\n\nThe nested attributeset to select values from\n\nlib.attrsets.attrByPath usage example\n\nx = { a = { b = 3; }; }\nattrByPath [\"a\" \"b\"] 6 x\n=> 3\nattrByPath [\"z\" \"z\"] 6 x\n=> 6\n\nlib.attrsets.hasAttrByPathhasAttrByPath :: [String] -> AttrSet -> Bool\n\nReturn if an attribute from nested attribute set exists.\n\nattrPath\n\nA list of strings representing the path through the nested attribute set set\n\ne\n\nThe nested attributeset to check\n\nlib.attrsets.hasAttrByPath usage example\n\nx = { a = { b = 3; }; }\nhasAttrByPath [\"a\" \"b\"] x\n=> true\nhasAttrByPath [\"z\" \"z\"] x\n=> false\n\nlib.attrsets.setAttrByPathsetAttrByPath :: [String] -> Any -> AttrSet\n\nReturn nested attribute set in which an attribute is set.\n\nattrPath\n\nA list of strings representing the path through the nested attribute set\n\nvalue\n\nThe value to set at the location described by attrPath\n\nlib.attrsets.setAttrByPath usage example\n\nsetAttrByPath [\"a\" \"b\"] 3\n=> { a = { b = 3; }; }\n\nlib.attrsets.getAttrFromPathgetAttrFromPath :: [String] -> AttrSet -> Value\n\nLike attrByPath without a default value. If it doesn't find the path it will throw.\n\nattrPath\n\nA list of strings representing the path through the nested attribute set set\n\nset\n\nThe nested attribute set to find the value in\n\nlib.attrsets.getAttrFromPath usage example\n\nx = { a = { b = 3; }; }\ngetAttrFromPath [\"a\" \"b\"] x\n=> 3\ngetAttrFromPath [\"z\" \"z\"] x\n=> error: cannot find attribute `z.z'\n\nlib.attrsets.attrValsattrVals :: [String] -> AttrSet -> [Any]\n\nReturn the specified attributes from a set.\n\nnameList\n\nThe list of attributes to fetch from set. Each attribute name must exist on the attrbitue set\n\nset\n\nThe set to get attribute values from\n\nlib.attrsets.attrVals usage example\n\nattrVals [\"a\" \"b\" \"c\"] as\n=> [as.a as.b as.c]\n\nlib.attrsets.attrValuesattrValues :: AttrSet -> [Any]\n\nReturn the values of all attributes in the given set, sorted by attribute name.\n\nlib.attrsets.attrValues usage example\n\nattrValues {c = 3; a = 1; b = 2;}\n=> [1 2 3]\n\nlib.attrsets.getAttrsgetAttrs :: [String] -> AttrSet -> AttrSet\n\nGiven a set of attribute names, return the set of the corresponding attributes from the given set.\n\nnames\n\nA list of attribute names to get out of set\n\nattrs\n\nThe set to get the named attributes from\n\nlib.attrsets.getAttrs usage example\n\ngetAttrs [ \"a\" \"b\" ] { a = 1; b = 2; c = 3; }\n=> { a = 1; b = 2; }\n\nlib.attrsets.catAttrscatAttrs :: String -> [AttrSet] -> [Any]\n\nCollect each attribute named attr from a list of attribute sets. Sets that don't contain the named attribute are\nignored.\n\nlib.attrsets.catAttrs usage example\n\ncatAttrs \"a\" [{a = 1;} {b = 0;} {a = 2;}]\n=> [1 2]\n\nlib.attrsets.filterAttrsfilterAttrs :: (String -> Any -> Bool) -> AttrSet -> AttrSet\n\nFilter an attribute set by removing all attributes for which the given predicate return false.\n\npred\n\nA predicate function that takes the attribute's name (name) and the attribute's value (value), and returns true to\ninclude the attribute or false to exclude the attribute.\n\nset\n\nThe attribute set to filter\n\nlib.attrsets.filterAttrs usage example\n\nfilterAttrs (n: v: n == \"foo\") { foo = 1; bar = 2; }\n=> { foo = 1; }\n\nlib.attrsets.filterAttrsRecursivefilterAttrsRecursive :: (String -> Any -> Bool) -> AttrSet -> AttrSet\n\nFilter an attribute set recursively by removing all attributes for which the given predicate return false.\n\npred\n\nA predicate function that takes the attribute's name (name) and the attribute's value (value), and returns true to\ninclude the attribute or false to exclude the attribute.\n\nset\n\nThe attribute set to filter\n\nlib.attrsets.filterAttrsRecursive usage example\n\nfilterAttrsRecursive (n: v: v != null) { foo = { bar = null; }; }\n=> { foo = {}; }\n\nlib.attrsets.foldAttrsfoldAttrs :: (Any -> Any -> Any) -> Any -> [AttrSets] -> Any\n\nApply fold functions to values grouped by key.\n\nop\n\nA function that takes a value and an accumulator, and returns the an updated accumulator.\n\nnul\n\nThe starting accumulator value\n\nlist_of_attrs\n\nA list of attribute sets to fold together by key\n\nlib.attrsets.foldAttrs usage example\n\nfoldAttrs (n: a: [n] ++ a) [] [{ a = 2; } { a = 3; }]\n=> { a = [ 2 3 ]; }\n\nlib.attrsets.collectcollect :: (Any -> Bool) -> AttrSet -> [x]\n\nRecursively collect values that verify a given predicate named pred from the set `attrs'. The recursion is stopped when\nthe predicate is verified.\n\npred\n\nA predicate function that takes an attributes value attrs, and returns true if the value should be collected, or false\nif the recursive descent should continue.\n\nattrs\n\nAn attrset\n\nlib.attrsets.collect usage example\n\ncollect isList { a = { b = [\"b\"]; }; c = [1]; }\n=> [[\"b\"] [1]]\n\ncollect (x: x ? outPath)\n{ a = { outPath = \"a\/\"; }; b = { outPath = \"b\/\"; }; }\n=> [{ outPath = \"a\/\"; } { outPath = \"b\/\"; }]\n\nlib.attrsets.cartesianProductOfSetscartesianProductOfSets :: AttrSet -> [AttrSet]\n\nReturn the cartesian product of attribute set value combinations.\n\nattrsOfLists\n\nAn attribute set whose values are all lists\n\nlib.attrsets.cartesianProductOfSets usage example\n\ncartesianProductOfSets { a = [ 1 2 ]; b = [ 10 20 ]; }\n=> [\n{ a = 1; b = 10; }\n{ a = 1; b = 20; }\n{ a = 2; b = 10; }\n{ a = 2; b = 20; }\n]\n\nlib.attrsets.nameValuePairnameValuePair :: String -> Any -> AttrSet\n\nUtility function that creates a {name, value} pair as expected by builtins.listToAttrs.\n\nname\n\nThe attribute name\n\nvalue\n\nThe attribute value\n\nlib.attrsets.nameValuePair usage example\n\nnameValuePair \"some\" 6\n=> { name = \"some\"; value = 6; }\n\nlib.attrsets.mapAttrsmapAttrs :: (String -> Any -> Any) -> AttrSet -> AttrSet\n\nApply a function to each element in an attribute set. The function takes two arguments --- the attribute name and its\nvalue --- and returns the new value for the attribute. The result is a new attribute set.\n\nlib.attrsets.mapAttrs usage example\n\nmapAttrs (name: value: name + \"-\" + value)\n{ x = \"foo\"; y = \"bar\"; }\n=> { x = \"x-foo\"; y = \"y-bar\"; }\n\nlib.attrsets.mapAttrs'mapAttrs' :: (String -> Any -> { name = String; value = Any }) -> AttrSet -> AttrSet\n\nLike mapAttrs, but allows the name of each attribute to be changed in addition to the value. The applied function should\nreturn both the new name and value as a nameValuePair.\n\nf\n\nA function when given an attribute's name and value, returns a new name-value pair\n\nset\n\nThe attribute set to map over\n\nlib.attrsets.mapAttrs' usage example\n\nmapAttrs' (name: value: nameValuePair (\"foo_\" + name) (\"bar-\" + value))\n{ x = \"a\"; y = \"b\"; }\n=> { foo_x = \"bar-a\"; foo_y = \"bar-b\"; }\n\nlib.attrsets.mapAttrsToListmapAttrsToList ::(String -> a -> b) -> AttrSet -> [b]\n\nCall a function for each attribute in the given set and return the result in a list.\n\nf\n\nA function when given an attribute's name and value, returns a new value\n\nattrs\n\nThe attribute set to map over\n\nlib.attrsets.mapAttrsToList usage example\n\nmapAttrsToList (name: value: name + value)\n{ x = \"a\"; y = \"b\"; }\n=> [ \"xa\" \"yb\" ]\n\nlib.attrsets.mapAttrsRecursivemapAttrsRecursive ::([String] -> a -> b) -> AttrSet -> AttrSet\n\nLike mapAttrs, except that it recursively applies itself to attribute sets. Also, the first argument of the argument\nfunction is a list of the names of the containing attributes.\n\nlib.attrsets.mapAttrsRecursive usage example\n\nmapAttrsRecursive (path: value: concatStringsSep \"-\" (path ++ [value]))\n{ n = { a = \"A\"; m = { b = \"B\"; c = \"C\"; }; }; d = \"D\"; }\n=> { n = { a = \"n-a-A\"; m = { b = \"n-m-b-B\"; c = \"n-m-c-C\"; }; }; d = \"d-D\"; }\n\nlib.attrsets.mapAttrsRecursiveCondmapAttrsRecursiveCond ::(AttrSet -> Bool) -> ([String] -> a -> b) -> AttrSet ->\nAttrSet\n\nLike mapAttrsRecursive, but it takes an additional predicate function that tells it whether to recursive into an\nattribute set. If it returns false, mapAttrsRecursiveCond does not recurse, but does apply the map function. If it\nreturns true, it does recurse, and does not apply the map function.\n\ncond\n\nA function when given an attribute set, returns true if it should recurse deeper and false if the attrset should be\napplied to f\n\nf\n\nA function given a list of attribute names, name_path and a value, returns a new value\n\nset\n\nThe attribute set to recursively map over\n\nlib.attrsets.mapAttrsRecursiveCond usage example\n\n# To prevent recursing into derivations (which are attribute\n# sets with the attribute \"type\" equal to \"derivation\"):\nmapAttrsRecursiveCond\n(as: !(as ? \"type\" && as.type == \"derivation\"))\n(x: ... do something ...)\nattrs\n\nlib.attrsets.genAttrsgenAttrs :: [ String ] -> (String -> Any) -> AttrSet\n\nGenerate an attribute set by mapping a function over a list of attribute names.\n\nnames\n\nNames of values int he resulting attribute set\n\nf\n\nA function that takes the name of the attribute set and returns the attribute's value\n\nlib.attrsets.genAttrs usage example\n\ngenAttrs [ \"foo\" \"bar\" ] (name: \"x_\" + name)\n=> { foo = \"x_foo\"; bar = \"x_bar\"; }\n\nlib.attrsets.isDerivationisDerivation :: Any -> Bool\n\nCheck whether the argument is a derivation. Any set with { type = \"derivation\"; } counts as a derivation.\n\nx\n\nThe value which is possibly a derivation\n\nlib.attrsets.isDerivation usage example\n\nnixpkgs = import <nixpkgs> {}\nisDerivation nixpkgs.ruby\n=> true\nisDerivation \"foobar\"\n=> false\n\nlib.attrsets.toDerivationtoDerivation :: Path -> Derivation\n\nConverts a store path to a fake derivation.\n\npath\n\nA store path to convert to a derivation\n\nlib.attrsets.optionalAttrsoptionalAttrs :: Bool -> AttrSet\n\nIf cond is true, return the attribute set as, otherwise an empty attribute set.\n\ncond\n\nCondition under which the as attribute set is returned\n\nas\n\nThe attribute set to return if cond is true\n\nlib.attrsets.optionalAttrs usage example\n\noptionalAttrs (true) { my = \"set\"; }\n=> { my = \"set\"; }\noptionalAttrs (false) { my = \"set\"; }\n=> { }\n\nlib.attrsets.zipAttrsWithNameszipAttrsWithNames :: [ String ] -> (String -> [ Any ] -> Any) -> [ AttrSet ] -> AttrSet\n\nMerge sets of attributes and use the function f to merge attributes values.\n\nnames\n\nA list of attribute names to zip\n\nf\n\nA function that takes an attribute name, all the values, and returns a combined value.\n\nsets\n\nA list of attribute sets to zip together\n\nlib.attrsets.zipAttrsWithNames usage example\n\nzipAttrsWithNames [\"a\"] (name: vs: vs) [{a = \"x\";} {a = \"y\"; b = \"z\";}]\n=> { a = [\"x\" \"y\"]; }\n\nlib.attrsets.zipAttrsWithzipAttrsWith :: (String -> [ Any ] -> Any) -> [ AttrSet ] -> AttrSet\n\nMerge sets of attributes and use the function f to merge attribute values. Similar to Attrset functions where all key\nnames are passed for names.\n\nf\n\nAccepts an attribute name, all the values, and returns a combined value.\n\nsets\n\nA list of attribute sets to zip together\n\nlib.attrsets.zipAttrsWith usage example\n\nzipAttrsWith (name: values: values) [{a = \"x\";} {a = \"y\"; b = \"z\";}]\n=> { a = [\"x\" \"y\"]; b = [\"z\"] }\n\nlib.attrsets.zipAttrszipAttrs :: [ AttrSet ] -> AttrSet\n\nLike zipAttrsWith with (name: values: values) as the function.\n\nsets\n\nA list of attribute sets to zip together\n\nlib.attrsets.zipAttrs usage example\n\nzipAttrs [{a = \"x\";} {a = \"y\"; b = \"z\";}]\n=> { a = [\"x\" \"y\"]; b = [\"z\"] }\n\nlib.attrsets.recursiveUpdateUntilrecursiveUpdateUntil :: ( [ String ] -> AttrSet -> AttrSet -> Bool ) -> AttrSet ->\nAttrSet -> AttrSet\n\nDoes the same as the update operator \/\/ except that attributes are merged until the given predicate is verified. The\npredicate should accept 3 arguments which are the path to reach the attribute, a part of the first attribute set and a\npart of the second attribute set. When the predicate is verified, the value of the first attribute set is replaced by\nthe value of the second attribute set.\n\npred\n\nA function when given a list of values on the left and right hand sides (path), a left-hand side value, and a right-hand\nside value, returns true when the the right-hand side value should no-longer be substituted, and false when the\nrecursive update should recurse\n\nlhs\n\nThe left-hand attribute set of the merge\n\nrhs\n\nThe right-hand attribute set of the merge\n\nlib.attrsets.recursiveUpdateUntil usage example\n\nrecursiveUpdateUntil (path: l: r: path == [\"foo\"]) {\n# first attribute set\nfoo.bar = 1;\nfoo.baz = 2;\nbar = 3;\n} {\n#second attribute set\nfoo.bar = 1;\nfoo.quz = 2;\nbaz = 4;\n}\n\nreturns: {\nfoo.bar = 1; # 'foo.*' from the second set\nfoo.quz = 2; #\nbar = 3;     # 'bar' from the first set\nbaz = 4;     # 'baz' from the second set\n}\n\nlib.attrsets.recursiveUpdaterecursiveUpdate :: AttrSet -> AttrSet -> AttrSet\n\nA recursive variant of the update operator ‘\/\/’. The recursion stops when one of the attribute values is not an\nattribute set, in which case the right hand side value takes precedence over the left hand side value.\n\nlhs\n\nThe left-hand attribute set of the merge\n\nrhs\n\nThe right-hand attribute set of the merge\n\nlib.attrsets.recursiveUpdate usage example\n\nrecursiveUpdate {\nboot.loader.grub.enable = true;\nboot.loader.grub.device = \"\/dev\/hda\";\n} {\nboot.loader.grub.device = \"\";\n}\n\nreturns: {\nboot.loader.grub.enable = true;\nboot.loader.grub.device = \"\";\n}\n\nlib.attrsets.matchAttrsAttrSet -> AttrSet -> Bool\n\nReturns true if the pattern is contained in the set. False otherwise.\n\npattern\n\nAn attrset to match\n\nattrs\n\nAn attrset in which to look for the pattern\n\nlib.attrsets.matchAttrs usage example\n\nmatchAttrs { cpu = {}; } { cpu = { bits = 64; }; }\n=> true\n\nlib.attrsets.overrideExistingAttrSet -> AttrSet -> AttrSet\n\nOverride only the attributes that are already present in the old set useful for deep-overriding.\n\nold\n\nAn attribute set containing all the keys that will be present in the result set\n\nnew\n\nAn attribute set containing the keys that might override the old set\n\nlib.attrsets.overrideExisting usage example\n\noverrideExisting {} { a = 1; }\n=> {}\noverrideExisting { b = 2; } { a = 1; }\n=> { b = 2; }\noverrideExisting { a = 3; b = 2; } { a = 1; }\n=> { a = 1; b = 2; }\n\nlib.attrsets.getOutputString -> Derivation -> Derivation\n\nGet a package output. If no output is found, fallback to .out and then to the default.\n\noutput\n\nA string name of a pkg's output\n\npkg\n\nA derivation that might have output output\n\nlib.attrsets.getOutput usage example\n\ngetOutput \"dev\" pkgs.openssl\n=> \"\/nix\/store\/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r-dev\"\n\nlib.attrsets.getBinDerivation -> Derivation\n\nGet a package's \"bin\" output If no output is found, fallback to .out and then to the default.\n\nlib.attrsets.getLibDerivation -> Derivation\n\nGet a package's \"lib\" output If no output is found, fallback to .out and then to the default.\n\nlib.attrsets.getDevDerivation -> Derivation\n\nGet a package's \"dev\" output If no output is found, fallback to .out and then to the default.\n\nlib.attrsets.getManDerivation -> Derivation\n\nGet a package's \"man\" output If no output is found, fallback to .out and then to the default.\n\nlib.attrsets.chooseDevOutputs[Derivation] -> [Derivation]\n\nPick the \"dev\" outputs of packages to place in buildInputs If no output is found, fallback to .out and then to the\ndefault.\n\ndrvs\n\nA list of derivations to get \"dev\" outputs from\n\nlib.attrsets.recurseIntoAttrsAttrSet -> AttrSet\n\nMake various Nix tools consider the contents of the resulting attribute set when looking for what to build, find, etc.\n\nThis function only affects a single attribute set; it does not apply itself recursively for nested attribute sets.\n\nattrs\n\nAn attribute set to be marked for recursing into for derivations\n\nlib.attrsets.dontRecurseIntoAttrsAttrSet -> AttrSet\n\nUndo the effect of recurseIntoAttrs.\n\nattrs\n\nAn attribute set to un-mark for recursing into for derivations\n" }
,{ "url": "functions\/library\/debug\/", "title": "Debugging functions", "text": "Debugging functionslib.debug.traceIftraceIf :: bool -> string -> a -> a\n\nConditionally trace the supplied message, based on a predicate.\n\npred\n\nPredicate to check\n\nmsg\n\nMessage that should be traced\n\nx\n\nValue to return\n\nlib.debug.traceIf usage example\n\ntraceIf true \"hello\" 3\ntrace: hello\n=> 3\n\nlib.debug.traceValFntraceValFn :: (a -> b) -> a -> a\n\nTrace the supplied value after applying a function to it, and return the original value.\n\nf\n\nFunction to apply\n\nx\n\nValue to trace and return\n\nlib.debug.traceValFn usage example\n\ntraceValFn (v: \"mystring ${v}\") \"foo\"\ntrace: mystring foo\n=> \"foo\"\n\nlib.debug.traceValtraceVal :: a -> a\n\nTrace the supplied value and return it.\n\nlib.debug.traceVal usage example\n\ntraceVal 42\n# trace: 42\n=> 42\n\nlib.debug.traceSeqtraceSeq :: a -> b -> b\n\nbuiltins.trace, but the value is builtins.deepSeqed first.\n\nx\n\nThe value to trace\n\ny\n\nThe value to return\n\nlib.debug.traceSeq usage example\n\ntrace { a.b.c = 3; } null\ntrace: { a = <CODE>; }\n=> null\ntraceSeq { a.b.c = 3; } null\ntrace: { a = { b = { c = 3; }; }; }\n=> null\n\nlib.debug.traceSeqNLike traceSeq, but only evaluate down to depth n. This is very useful because lots of traceSeq usages\nlead to an infinite recursion.\n\ndepth\n\nFunction argument\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nlib.debug.traceSeqN usage example\n\ntraceSeqN 2 { a.b.c = 3; } null\ntrace: { a = { b = {…}; }; }\n=> null\n\nlib.debug.traceValSeqFnA combination of traceVal and traceSeq that applies a provided function to the value to be traced\nafter deepSeqing it.\n\nf\n\nFunction to apply\n\nv\n\nValue to trace\n\nlib.debug.traceValSeqA combination of traceVal and traceSeq.\n\nlib.debug.traceValSeqNFnA combination of traceVal and traceSeqN that applies a provided function to the value to be\ntraced.\n\nf\n\nFunction to apply\n\ndepth\n\nFunction argument\n\nv\n\nValue to trace\n\nlib.debug.traceValSeqNA combination of traceVal and traceSeqN.\n\nlib.debug.traceFnSeqNTrace the input and output of a function f named name, both down to depth.\n\nThis is useful for adding around a function call, to see the before\/after of values as they are transformed.\n\ndepth\n\nFunction argument\n\nname\n\nFunction argument\n\nf\n\nFunction argument\n\nv\n\nFunction argument\n\nlib.debug.traceFnSeqN usage example\n\ntraceFnSeqN 2 \"id\" (x: x) { a.b.c = 3; }\ntrace: { fn = \"id\"; from = { a.b = {…}; }; to = { a.b = {…}; }; }\n=> { a.b.c = 3; }\n\nlib.debug.runTestsEvaluate a set of tests. A test is an attribute set {expr, expected}, denoting an expression and its\nexpected result. The result is a list of failed tests, each represented as {name, expected, actual}, denoting the\nattribute name of the failing test and its expected and actual results.\n\nUsed for regression testing of the functions in lib; see tests.nix for an example. Only tests having names starting with\n\"test\" are run.\n\nAdd attr { tests = [\"testName\"]; } to run these tests only.\n\ntests\n\nTests to run\n\nlib.debug.testAllTrueCreate a test assuming that list elements are true.\n\nexpr\n\nFunction argument\n\nlib.debug.testAllTrue usage example\n\n{ testX = allTrue [ true ]; }\n" }
,{ "url": "functions\/library\/lists\/", "title": "List manipulation functions", "text": "List manipulation functionslib.lists.singletonsingleton :: a -> [a]\n\nCreate a list consisting of a single element. singleton x is sometimes more convenient with respect to indentation than\n[x] when x spans multiple lines.\n\nx\n\nFunction argument\n\nlib.lists.singleton usage example\n\nsingleton \"foo\"\n=> [ \"foo\" ]\n\nlib.lists.forEachforEach :: [a] -> (a -> b) -> [b]\n\nApply the function to each element in the list. Same as map, but arguments flipped.\n\nxs\n\nFunction argument\n\nf\n\nFunction argument\n\nlib.lists.forEach usage example\n\nforEach [ 1 2 ] (x:\ntoString x\n)\n=> [ \"1\" \"2\" ]\n\nlib.lists.foldrfoldr :: (a -> b -> b) -> b -> [a] -> b\n\n“right fold” a binary function op between successive elements of list with nul' as the starting value, i.e., foldr op\nnul [x_1 x_2 ... x_n] == op x_1 (op x_2 ... (op x_n nul))`.\n\nop\n\nFunction argument\n\nnul\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.lists.foldr usage example\n\nconcat = foldr (a: b: a + b) \"z\"\nconcat [ \"a\" \"b\" \"c\" ]\n=> \"abcz\"\n# different types\nstrange = foldr (int: str: toString (int + 1) + str) \"a\"\nstrange [ 1 2 3 4 ]\n=> \"2345a\"\n\nlib.lists.foldfold is an alias of foldr for historic reasons\n\nlib.lists.foldlfoldl :: (b -> a -> b) -> b -> [a] -> b\n\n“left fold”, like foldr, but from the left: foldl op nul [x_1 x_2 ... x_n] == op (... (op (op nul x_1) x_2) ... x_n).\n\nop\n\nFunction argument\n\nnul\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.lists.foldl usage example\n\nlconcat = foldl (a: b: a + b) \"z\"\nlconcat [ \"a\" \"b\" \"c\" ]\n=> \"zabc\"\n# different types\nlstrange = foldl (str: int: str + toString (int + 1)) \"a\"\nlstrange [ 1 2 3 4 ]\n=> \"a2345\"\n\nlib.lists.foldl'foldl' :: (b -> a -> b) -> b -> [a] -> b\n\nStrict version of foldl.\n\nThe difference is that evaluation is forced upon access. Usually used with small whole results (in contrast with\nlazily-generated list or large lists where only a part is consumed.)\n\nlib.lists.imap0imap0 :: (int -> a -> b) -> [a] -> [b]\n\nMap with index starting from 0\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.lists.imap0 usage example\n\nimap0 (i: v: \"${v}-${toString i}\") [\"a\" \"b\"]\n=> [ \"a-0\" \"b-1\" ]\n\nlib.lists.imap1imap1 :: (int -> a -> b) -> [a] -> [b]\n\nMap with index starting from 1\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.lists.imap1 usage example\n\nimap1 (i: v: \"${v}-${toString i}\") [\"a\" \"b\"]\n=> [ \"a-1\" \"b-2\" ]\n\nlib.lists.concatMapconcatMap :: (a -> [b]) -> [a] -> [b]\n\nMap and concatenate the result.\n\nlib.lists.concatMap usage example\n\nconcatMap (x: [x] ++ [\"z\"]) [\"a\" \"b\"]\n=> [ \"a\" \"z\" \"b\" \"z\" ]\n\nlib.lists.flattenFlatten the argument into a single list; that is, nested lists are spliced into the top-level lists.\n\nx\n\nFunction argument\n\nlib.lists.flatten usage example\n\nflatten [1 [2 [3] 4] 5]\n=> [1 2 3 4 5]\nflatten 1\n=> [1]\n\nlib.lists.removeremove :: a -> [a] -> [a]\n\nRemove elements equal to 'e' from a list. Useful for buildInputs.\n\ne\n\nElement to remove from the list\n\nlib.lists.remove usage example\n\nremove 3 [ 1 3 4 3 ]\n=> [ 1 4 ]\n\nlib.lists.findSinglefindSingle :: (a -> bool) -> a -> a -> [a] -> a\n\nFind the sole element in the list matching the specified predicate, returns default if no such element exists, or\nmultiple if there are multiple matching elements.\n\npred\n\nPredicate\n\ndefault\n\nDefault value to return if element was not found.\n\nmultiple\n\nDefault value to return if more than one element was found\n\nlist\n\nInput list\n\nlib.lists.findSingle usage example\n\nfindSingle (x: x == 3) \"none\" \"multiple\" [ 1 3 3 ]\n=> \"multiple\"\nfindSingle (x: x == 3) \"none\" \"multiple\" [ 1 3 ]\n=> 3\nfindSingle (x: x == 3) \"none\" \"multiple\" [ 1 9 ]\n=> \"none\"\n\nlib.lists.findFirstfindFirst :: (a -> bool) -> a -> [a] -> a\n\nFind the first element in the list matching the specified predicate or return default if no such element exists.\n\npred\n\nPredicate\n\ndefault\n\nDefault value to return\n\nlist\n\nInput list\n\nlib.lists.findFirst usage example\n\nfindFirst (x: x > 3) 7 [ 1 6 4 ]\n=> 6\nfindFirst (x: x > 9) 7 [ 1 6 4 ]\n=> 7\n\nlib.lists.anyany :: (a -> bool) -> [a] -> bool\n\nReturn true if function pred returns true for at least one element of list.\n\nlib.lists.any usage example\n\nany isString [ 1 \"a\" { } ]\n=> true\nany isString [ 1 { } ]\n=> false\n\nlib.lists.allall :: (a -> bool) -> [a] -> bool\n\nReturn true if function pred returns true for all elements of list.\n\nlib.lists.all usage example\n\nall (x: x < 3) [ 1 2 ]\n=> true\nall (x: x < 3) [ 1 2 3 ]\n=> false\n\nlib.lists.countcount :: (a -> bool) -> [a] -> int\n\nCount how many elements of list match the supplied predicate function.\n\npred\n\nPredicate\n\nlib.lists.count usage example\n\ncount (x: x == 3) [ 3 2 3 4 6 ]\n=> 2\n\nlib.lists.optionaloptional :: bool -> a -> [a]\n\nReturn a singleton list or an empty list, depending on a boolean value. Useful when building lists with optional\nelements (e.g. `++ optional (system == \"i686-linux\") firefox').\n\ncond\n\nFunction argument\n\nelem\n\nFunction argument\n\nlib.lists.optional usage example\n\noptional true \"foo\"\n=> [ \"foo\" ]\noptional false \"foo\"\n=> [ ]\n\nlib.lists.optionalsoptionals :: bool -> [a] -> [a]\n\nReturn a list or an empty list, depending on a boolean value.\n\ncond\n\nCondition\n\nelems\n\nList to return if condition is true\n\nlib.lists.optionals usage example\n\noptionals true [ 2 3 ]\n=> [ 2 3 ]\noptionals false [ 2 3 ]\n=> [ ]\n\nlib.lists.toListIf argument is a list, return it; else, wrap it in a singleton list. If you're using this, you should\nalmost certainly reconsider if there isn't a more \"well-typed\" approach.\n\nx\n\nFunction argument\n\nlib.lists.toList usage example\n\ntoList [ 1 2 ]\n=> [ 1 2 ]\ntoList \"hi\"\n=> [ \"hi \"]\n\nlib.lists.rangerange :: int -> int -> [int]\n\nReturn a list of integers from first' up to and including last'.\n\nfirst\n\nFirst integer in the range\n\nlast\n\nLast integer in the range\n\nlib.lists.range usage example\n\nrange 2 4\n=> [ 2 3 4 ]\nrange 3 2\n=> [ ]\n\nlib.lists.partition(a -> bool) -> [a] -> { right :: [a], wrong :: [a] }\n\nSplits the elements of a list in two lists, right and wrong, depending on the evaluation of a predicate.\n\nlib.lists.partition usage example\n\npartition (x: x > 2) [ 5 1 2 3 4 ]\n=> { right = [ 5 3 4 ]; wrong = [ 1 2 ]; }\n\nlib.lists.groupBy'Splits the elements of a list into many lists, using the return value of a predicate. Predicate should\nreturn a string which becomes keys of attrset `groupBy' returns.\n\ngroupBy' allows to customise the combining function and initial value\n\nop\n\nFunction argument\n\nnul\n\nFunction argument\n\npred\n\nFunction argument\n\nlst\n\nFunction argument\n\nlib.lists.groupBy' usage example\n\ngroupBy (x: boolToString (x > 2)) [ 5 1 2 3 4 ]\n=> { true = [ 5 3 4 ]; false = [ 1 2 ]; }\ngroupBy (x: x.name) [ {name = \"icewm\"; script = \"icewm &\";}\n{name = \"xfce\";  script = \"xfce4-session &\";}\n{name = \"icewm\"; script = \"icewmbg &\";}\n{name = \"mate\";  script = \"gnome-session &\";}\n]\n=> { icewm = [ { name = \"icewm\"; script = \"icewm &\"; }\n{ name = \"icewm\"; script = \"icewmbg &\"; } ];\nmate  = [ { name = \"mate\";  script = \"gnome-session &\"; } ];\nxfce  = [ { name = \"xfce\";  script = \"xfce4-session &\"; } ];\n}\n\ngroupBy' builtins.add 0 (x: boolToString (x > 2)) [ 5 1 2 3 4 ]\n=> { true = 12; false = 3; }\n\nlib.lists.zipListsWithzipListsWith :: (a -> b -> c) -> [a] -> [b] -> [c]\n\nMerges two lists of the same size together. If the sizes aren't the same the merging stops at the shortest. How both\nlists are merged is defined by the first argument.\n\nf\n\nFunction to zip elements of both lists\n\nfst\n\nFirst list\n\nsnd\n\nSecond list\n\nlib.lists.zipListsWith usage example\n\nzipListsWith (a: b: a + b) [\"h\" \"l\"] [\"e\" \"o\"]\n=> [\"he\" \"lo\"]\n\nlib.lists.zipListszipLists :: [a] -> [b] -> [{ fst :: a, snd :: b}]\n\nMerges two lists of the same size together. If the sizes aren't the same the merging stops at the shortest.\n\nlib.lists.zipLists usage example\n\nzipLists [ 1 2 ] [ \"a\" \"b\" ]\n=> [ { fst = 1; snd = \"a\"; } { fst = 2; snd = \"b\"; } ]\n\nlib.lists.reverseListreverseList :: [a] -> [a]\n\nReverse the order of the elements of a list.\n\nxs\n\nFunction argument\n\nlib.lists.reverseList usage example\n\n\nreverseList [ \"b\" \"o\" \"j\" ]\n=> [ \"j\" \"o\" \"b\" ]\n\nlib.lists.listDfsDepth-First Search (DFS) for lists list != [].\n\nbefore a b == true means that b depends on a (there's an edge from b to a).\n\nstopOnCycles\n\nFunction argument\n\nbefore\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.lists.listDfs usage example\n\nlistDfs true hasPrefix [ \"\/home\/user\" \"other\" \"\/\" \"\/home\" ]\n== { minimal = \"\/\";                  # minimal element\nvisited = [ \"\/home\/user\" ];     # seen elements (in reverse order)\nrest    = [ \"\/home\" \"other\" ];  # everything else\n}\n\nlistDfs true hasPrefix [ \"\/home\/user\" \"other\" \"\/\" \"\/home\" \"\/\" ]\n== { cycle   = \"\/\";                  # cycle encountered at this element\nloops   = [ \"\/\" ];              # and continues to these elements\nvisited = [ \"\/\" \"\/home\/user\" ]; # elements leading to the cycle (in reverse order)\nrest    = [ \"\/home\" \"other\" ];  # everything else\n\nlib.lists.toposortSort a list based on a partial ordering using DFS. This implementation is O(N^2), if your ordering is\nlinear, use sort instead.\n\nbefore a b == true means that b should be after a in the result.\n\nbefore\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.lists.toposort usage example\n\n\ntoposort hasPrefix [ \"\/home\/user\" \"other\" \"\/\" \"\/home\" ]\n== { result = [ \"\/\" \"\/home\" \"\/home\/user\" \"other\" ]; }\n\ntoposort hasPrefix [ \"\/home\/user\" \"other\" \"\/\" \"\/home\" \"\/\" ]\n== { cycle = [ \"\/home\/user\" \"\/\" \"\/\" ]; # path leading to a cycle\nloops = [ \"\/\" ]; }                # loops back to these elements\n\ntoposort hasPrefix [ \"other\" \"\/home\/user\" \"\/home\" \"\/\" ]\n== { result = [ \"other\" \"\/\" \"\/home\" \"\/home\/user\" ]; }\n\ntoposort (a: b: a < b) [ 3 2 1 ] == { result = [ 1 2 3 ]; }\n\nlib.lists.sortSort a list based on a comparator function which compares two elements and returns true if the first\nargument is strictly below the second argument. The returned list is sorted in an increasing order. The implementation\ndoes a quick-sort.\n\nlib.lists.sort usage example\n\nsort (a: b: a < b) [ 5 3 7 ]\n=> [ 3 5 7 ]\n\nlib.lists.compareListsCompare two lists element-by-element.\n\ncmp\n\nFunction argument\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nlib.lists.compareLists usage example\n\ncompareLists compare [] []\n=> 0\ncompareLists compare [] [ \"a\" ]\n=> -1\ncompareLists compare [ \"a\" ] []\n=> 1\ncompareLists compare [ \"a\" \"b\" ] [ \"a\" \"c\" ]\n=> 1\n\nlib.lists.naturalSortSort list using \"Natural sorting\". Numeric portions of strings are sorted in numeric order.\n\nlst\n\nFunction argument\n\nlib.lists.naturalSort usage example\n\nnaturalSort [\"disk11\" \"disk8\" \"disk100\" \"disk9\"]\n=> [\"disk8\" \"disk9\" \"disk11\" \"disk100\"]\nnaturalSort [\"10.46.133.149\" \"10.5.16.62\" \"10.54.16.25\"]\n=> [\"10.5.16.62\" \"10.46.133.149\" \"10.54.16.25\"]\nnaturalSort [\"v0.2\" \"v0.15\" \"v0.0.9\"]\n=> [ \"v0.0.9\" \"v0.2\" \"v0.15\" ]\n\nlib.lists.taketake :: int -> [a] -> [a]\n\nReturn the first (at most) N elements of a list.\n\ncount\n\nNumber of elements to take\n\nlib.lists.take usage example\n\ntake 2 [ \"a\" \"b\" \"c\" \"d\" ]\n=> [ \"a\" \"b\" ]\ntake 2 [ ]\n=> [ ]\n\nlib.lists.dropdrop :: int -> [a] -> [a]\n\nRemove the first (at most) N elements of a list.\n\ncount\n\nNumber of elements to drop\n\nlist\n\nInput list\n\nlib.lists.drop usage example\n\ndrop 2 [ \"a\" \"b\" \"c\" \"d\" ]\n=> [ \"c\" \"d\" ]\ndrop 2 [ ]\n=> [ ]\n\nlib.lists.sublistsublist :: int -> int -> [a] -> [a]\n\nReturn a list consisting of at most count elements of list, starting at index start.\n\nstart\n\nIndex at which to start the sublist\n\ncount\n\nNumber of elements to take\n\nlist\n\nInput list\n\nlib.lists.sublist usage example\n\nsublist 1 3 [ \"a\" \"b\" \"c\" \"d\" \"e\" ]\n=> [ \"b\" \"c\" \"d\" ]\nsublist 1 3 [ ]\n=> [ ]\n\nlib.lists.lastlast :: [a] -> a\n\nReturn the last element of a list.\n\nThis function throws an error if the list is empty.\n\nlist\n\nFunction argument\n\nlib.lists.last usage example\n\nlast [ 1 2 3 ]\n=> 3\n\nlib.lists.initinit :: [a] -> [a]\n\nReturn all elements but the last.\n\nThis function throws an error if the list is empty.\n\nlist\n\nFunction argument\n\nlib.lists.init usage example\n\ninit [ 1 2 3 ]\n=> [ 1 2 ]\n\nlib.lists.crossListsReturn the image of the cross product of some lists by a function.\n\nlib.lists.crossLists usage example\n\ncrossLists (x:y: \"${toString x}${toString y}\") [[1 2] [3 4]]\n=> [ \"13\" \"14\" \"23\" \"24\" ]\n\nlib.lists.uniqueunique :: [a] -> [a]\n\nRemove duplicate elements from the list. O(n^2) complexity.\n\nlib.lists.unique usage example\n\nunique [ 3 2 3 4 ]\n=> [ 3 2 4 ]\n\nlib.lists.intersectListsIntersects list 'e' and another list. O(nm) complexity.\n\ne\n\nFunction argument\n\nlib.lists.intersectLists usage example\n\nintersectLists [ 1 2 3 ] [ 6 3 2 ]\n=> [ 3 2 ]\n\nlib.lists.subtractListsSubtracts list 'e' from another list. O(nm) complexity.\n\ne\n\nFunction argument\n\nlib.lists.subtractLists usage example\n\nsubtractLists [ 3 2 ] [ 1 2 3 4 5 3 ]\n=> [ 1 4 5 ]\n\nlib.lists.mutuallyExclusiveTest if two lists have no common element. It should be slightly more efficient than\n(intersectLists a b == [])\n\na\n\nFunction argument\n\nb\n\nFunction argument\n" }
,{ "url": "functions\/library\/options\/", "title": "NixOS \/ nixpkgs option handling", "text": "NixOS \/ nixpkgs option handlinglib.options.isOptionisOption :: a -> bool\n\nReturns true when the given argument is an option\n\nlib.options.isOption usage example\n\nisOption 1             \/\/ => false\nisOption (mkOption {}) \/\/ => true\n\nlib.options.mkOptionCreates an Option attribute set. mkOption accepts an attribute set with the following keys:\n\nAll keys default to null when not given.\n\npattern - structured function argument\n\ndefault\n\nDefault value used when no definition is given in the configuration.\n\ndefaultText\n\nTextual representation of the default, for the manual.\n\nexample\n\nExample value used in the manual.\n\ndescription\n\nString describing the option.\n\nrelatedPackages\n\nRelated packages used in the manual (see genRelatedPackages in ..\/nixos\/lib\/make-options-doc\/default.nix).\n\ntype\n\nOption type, providing type-checking and value merging.\n\napply\n\nFunction that converts the option value to something else.\n\ninternal\n\nWhether the option is for NixOS developers only.\n\nvisible\n\nWhether the option shows up in the manual.\n\nreadOnly\n\nWhether the option can be set only once\n\noptions\n\nDeprecated, used by types.optionSet.\n\nlib.options.mkOption usage example\n\nmkOption { }  \/\/ => { _type = \"option\"; }\nmkOption { defaultText = \"foo\"; } \/\/ => { _type = \"option\"; defaultText = \"foo\"; }\n\nlib.options.mkEnableOptionCreates an Option attribute set for a boolean value option i.e an option to be toggled on or\noff:\n\nname\n\nName for the created option\n\nlib.options.mkEnableOption usage example\n\nmkEnableOption \"foo\"\n=> { _type = \"option\"; default = false; description = \"Whether to enable foo.\"; example = true; type = { ... }; }\n\nlib.options.mkSinkUndeclaredOptionsThis option accepts anything, but it does not produce any result.\n\nThis is useful for sharing a module across different module sets without having to implement similar features as long as\nthe values of the options are not accessed.\n\nattrs\n\nFunction argument\n\nlib.options.mergeEqualOption\"Merge\" option definitions by checking that they all have the same value.\n\nloc\n\nFunction argument\n\ndefs\n\nFunction argument\n\nlib.options.getValuesgetValues :: [ { value :: a } ] -> [a]\n\nExtracts values of all \"value\" keys of the given list.\n\nlib.options.getValues usage example\n\ngetValues [ { value = 1; } { value = 2; } ] \/\/ => [ 1 2 ]\ngetValues [ ]                               \/\/ => [ ]\n\nlib.options.getFilesgetFiles :: [ { file :: a } ] -> [a]\n\nExtracts values of all \"file\" keys of the given list\n\nlib.options.getFiles usage example\n\ngetFiles [ { file = \"file1\"; } { file = \"file2\"; } ] \/\/ => [ \"file1\" \"file2\" ]\ngetFiles [ ]                                         \/\/ => [ ]\n\nlib.options.scrubOptionValueThis function recursively removes all derivation attributes from x except for the name\nattribute.\n\nThis is to make the generation of options.xml much more efficient: the XML representation of derivations is very large\n(on the order of megabytes) and is not actually used by the manual generator.\n\nx\n\nFunction argument\n\nlib.options.literalExampleFor use in the example option attribute. It causes the given text to be included verbatim in\ndocumentation. This is necessary for example values that are not simple values, e.g., functions.\n\ntext\n\nFunction argument\n\nlib.options.showOptionConvert an option, described as a list of the option parts in to a safe, human readable version.\n\nparts\n\nFunction argument\n\nlib.options.showOption usage example\n\n(showOption [\"foo\" \"bar\" \"baz\"]) == \"foo.bar.baz\"\n(showOption [\"foo\" \"bar.baz\" \"tux\"]) == \"foo.bar.baz.tux\"\n\nPlaceholders will not be quoted as they are not actual values:\n(showOption [\"foo\" \"*\" \"bar\"]) == \"foo.*.bar\"\n(showOption [\"foo\" \"<name>\" \"bar\"]) == \"foo.<name>.bar\"\n\nUnlike attributes, options can also start with numbers:\n(showOption [\"windowManager\" \"2bwm\" \"enable\"]) == \"windowManager.2bwm.enable\"\n" }
,{ "url": "functions\/library\/strings\/", "title": "String manipulation functions", "text": "String manipulation functionslib.strings.concatStringsconcatStrings :: [string] -> string\n\nConcatenate a list of strings.\n\nlib.strings.concatStrings usage example\n\nconcatStrings [\"foo\" \"bar\"]\n=> \"foobar\"\n\nlib.strings.concatMapStringsconcatMapStrings :: (a -> string) -> [a] -> string\n\nMap a function over a list and concatenate the resulting strings.\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.strings.concatMapStrings usage example\n\nconcatMapStrings (x: \"a\" + x) [\"foo\" \"bar\"]\n=> \"afooabar\"\n\nlib.strings.concatImapStringsconcatImapStrings :: (int -> a -> string) -> [a] -> string\n\nLike concatMapStrings except that the f functions also gets the position as a parameter.\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nlib.strings.concatImapStrings usage example\n\nconcatImapStrings (pos: x: \"${toString pos}-${x}\") [\"foo\" \"bar\"]\n=> \"1-foo2-bar\"\n\nlib.strings.intersperseintersperse :: a -> [a] -> [a]\n\nPlace an element between each element of a list\n\nseparator\n\nSeparator to add between elements\n\nlist\n\nInput list\n\nlib.strings.intersperse usage example\n\nintersperse \"\/\" [\"usr\" \"local\" \"bin\"]\n=> [\"usr\" \"\/\" \"local\" \"\/\" \"bin\"].\n\nlib.strings.concatStringsSepconcatStringsSep :: string -> [string] -> string\n\nConcatenate a list of strings with a separator between each element\n\nlib.strings.concatStringsSep usage example\n\nconcatStringsSep \"\/\" [\"usr\" \"local\" \"bin\"]\n=> \"usr\/local\/bin\"\n\nlib.strings.concatMapStringsSepconcatMapStringsSep :: string -> (string -> string) -> [string] -> string\n\nMaps a function over a list of strings and then concatenates the result with the specified separator interspersed\nbetween elements.\n\nsep\n\nSeparator to add between elements\n\nf\n\nFunction to map over the list\n\nlist\n\nList of input strings\n\nlib.strings.concatMapStringsSep usage example\n\nconcatMapStringsSep \"-\" (x: toUpper x)  [\"foo\" \"bar\" \"baz\"]\n=> \"FOO-BAR-BAZ\"\n\nlib.strings.concatImapStringsSepconcatIMapStringsSep :: string -> (int -> string -> string) -> [string] -> string\n\nSame as concatMapStringsSep, but the mapping function additionally receives the position of its argument.\n\nsep\n\nSeparator to add between elements\n\nf\n\nFunction that receives elements and their positions\n\nlist\n\nList of input strings\n\nlib.strings.concatImapStringsSep usage example\n\nconcatImapStringsSep \"-\" (pos: x: toString (x \/ pos)) [ 6 6 6 ]\n=> \"6-3-2\"\n\nlib.strings.makeSearchPathmakeSearchPath :: string -> [string] -> string\n\nConstruct a Unix-style, colon-separated search path consisting of the given subDir appended to each of the given paths.\n\nsubDir\n\nDirectory name to append\n\npaths\n\nList of base paths\n\nlib.strings.makeSearchPath usage example\n\nmakeSearchPath \"bin\" [\"\/root\" \"\/usr\" \"\/usr\/local\"]\n=> \"\/root\/bin:\/usr\/bin:\/usr\/local\/bin\"\nmakeSearchPath \"bin\" [\"\"]\n=> \"\/bin\"\n\nlib.strings.makeSearchPathOutputstring -> string -> [package] -> string\n\nConstruct a Unix-style search path by appending the given subDir to the specified output of each of the packages. If no\noutput by the given name is found, fallback to .out and then to the default.\n\noutput\n\nPackage output to use\n\nsubDir\n\nDirectory name to append\n\npkgs\n\nList of packages\n\nlib.strings.makeSearchPathOutput usage example\n\nmakeSearchPathOutput \"dev\" \"bin\" [ pkgs.openssl pkgs.zlib ]\n=> \"\/nix\/store\/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r-dev\/bin:\/nix\/store\/wwh7mhwh269sfjkm6k5665b5kgp7jrk2-zlib-1.2.8\/bin\"\n\nlib.strings.makeLibraryPathConstruct a library search path (such as RPATH) containing the libraries for a set of\npackages\n\nlib.strings.makeLibraryPath usage example\n\nmakeLibraryPath [ \"\/usr\" \"\/usr\/local\" ]\n=> \"\/usr\/lib:\/usr\/local\/lib\"\npkgs = import <nixpkgs> { }\nmakeLibraryPath [ pkgs.openssl pkgs.zlib ]\n=> \"\/nix\/store\/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r\/lib:\/nix\/store\/wwh7mhwh269sfjkm6k5665b5kgp7jrk2-zlib-1.2.8\/lib\"\n\nlib.strings.makeBinPathConstruct a binary search path (such as $PATH) containing the binaries for a set of packages.\n\nlib.strings.makeBinPath usage example\n\nmakeBinPath [\"\/root\" \"\/usr\" \"\/usr\/local\"]\n=> \"\/root\/bin:\/usr\/bin:\/usr\/local\/bin\"\n\nlib.strings.optionalStringoptionalString :: bool -> string -> string\n\nDepending on the boolean `cond', return either the given string or the empty string. Useful to concatenate against a\nbigger string.\n\ncond\n\nCondition\n\nstring\n\nString to return if condition is true\n\nlib.strings.optionalString usage example\n\noptionalString true \"some-string\"\n=> \"some-string\"\noptionalString false \"some-string\"\n=> \"\"\n\nlib.strings.hasPrefixhasPrefix :: string -> string -> bool\n\nDetermine whether a string has given prefix.\n\npref\n\nPrefix to check for\n\nstr\n\nInput string\n\nlib.strings.hasPrefix usage example\n\nhasPrefix \"foo\" \"foobar\"\n=> true\nhasPrefix \"foo\" \"barfoo\"\n=> false\n\nlib.strings.hasSuffixhasSuffix :: string -> string -> bool\n\nDetermine whether a string has given suffix.\n\nsuffix\n\nSuffix to check for\n\ncontent\n\nInput string\n\nlib.strings.hasSuffix usage example\n\nhasSuffix \"foo\" \"foobar\"\n=> false\nhasSuffix \"foo\" \"barfoo\"\n=> true\n\nlib.strings.hasInfixhasInfix :: string -> string -> bool\n\nDetermine whether a string contains the given infix\n\ninfix\n\nFunction argument\n\ncontent\n\nFunction argument\n\nlib.strings.hasInfix usage example\n\nhasInfix \"bc\" \"abcd\"\n=> true\nhasInfix \"ab\" \"abcd\"\n=> true\nhasInfix \"cd\" \"abcd\"\n=> true\nhasInfix \"foo\" \"abcd\"\n=> false\n\nlib.strings.stringToCharactersstringToCharacters :: string -> [string]\n\nConvert a string to a list of characters (i.e. singleton strings). This allows you to, e.g., map a function over each\ncharacter. However, note that this will likely be horribly inefficient; Nix is not a general purpose programming\nlanguage. Complex string manipulations should, if appropriate, be done in a derivation. Also note that Nix treats\nstrings as a list of bytes and thus doesn't handle unicode.\n\ns\n\nFunction argument\n\nlib.strings.stringToCharacters usage example\n\nstringToCharacters \"\"\n=> [ ]\nstringToCharacters \"abc\"\n=> [ \"a\" \"b\" \"c\" ]\nstringToCharacters \"💩\"\n=> [ \"�\" \"�\" \"�\" \"�\" ]\n\nlib.strings.stringAsCharsstringAsChars :: (string -> string) -> string -> string\n\nManipulate a string character by character and replace them by strings before concatenating the results.\n\nf\n\nFunction to map over each individual character\n\ns\n\nInput string\n\nlib.strings.stringAsChars usage example\n\nstringAsChars (x: if x == \"a\" then \"i\" else x) \"nax\"\n=> \"nix\"\n\nlib.strings.escapeescape :: [string] -> string -> string\n\nEscape occurrence of the elements of list in string by prefixing it with a backslash.\n\nlist\n\nFunction argument\n\nlib.strings.escape usage example\n\nescape [\"(\" \")\"] \"(foo)\"\n=> \"\\\\(foo\\\\)\"\n\nlib.strings.escapeShellArgescapeShellArg :: string -> string\n\nQuote string to be used safely within the Bourne shell.\n\narg\n\nFunction argument\n\nlib.strings.escapeShellArg usage example\n\nescapeShellArg \"esc'ape\\nme\"\n=> \"'esc'\\\\''ape\\nme'\"\n\nlib.strings.escapeShellArgsescapeShellArgs :: [string] -> string\n\nQuote all arguments to be safely passed to the Bourne shell.\n\nlib.strings.escapeShellArgs usage example\n\nescapeShellArgs [\"one\" \"two three\" \"four'five\"]\n=> \"'one' 'two three' 'four'\\\\''five'\"\n\nlib.strings.escapeNixStringstring -> string\n\nTurn a string into a Nix expression representing that string\n\ns\n\nFunction argument\n\nlib.strings.escapeNixString usage example\n\nescapeNixString \"hello\\${}\\n\"\n=> \"\\\"hello\\\\\\${}\\\\n\\\"\"\n\nlib.strings.escapeRegexstring -> string\n\nTurn a string into an exact regular expression\n\nlib.strings.escapeRegex usage example\n\nescapeRegex \"[^a-z]*\"\n=> \"\\\\[\\\\^a-z]\\\\*\"\n\nlib.strings.escapeNixIdentifierstring -> string\n\nQuotes a string if it can't be used as an identifier directly.\n\ns\n\nFunction argument\n\nlib.strings.escapeNixIdentifier usage example\n\nescapeNixIdentifier \"hello\"\n=> \"hello\"\nescapeNixIdentifier \"0abc\"\n=> \"\\\"0abc\\\"\"\n\nlib.strings.toLowertoLower :: string -> string\n\nConverts an ASCII string to lower-case.\n\nlib.strings.toLower usage example\n\ntoLower \"HOME\"\n=> \"home\"\n\nlib.strings.toUppertoUpper :: string -> string\n\nConverts an ASCII string to upper-case.\n\nlib.strings.toUpper usage example\n\ntoUpper \"home\"\n=> \"HOME\"\n\nlib.strings.addContextFromAppends string context from another string. This is an implementation detail of Nix.\n\nStrings in Nix carry an invisible context which is a list of strings representing store paths. If the string is later\nused in a derivation attribute, the derivation will properly populate the inputDrvs and inputSrcs.\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nlib.strings.addContextFrom usage example\n\npkgs = import <nixpkgs> { };\naddContextFrom pkgs.coreutils \"bar\"\n=> \"bar\"\n\nlib.strings.splitStringCut a string with a separator and produces a list of strings which were separated by this\nseparator.\n\n_sep\n\nFunction argument\n\n_s\n\nFunction argument\n\nlib.strings.splitString usage example\n\nsplitString \".\" \"foo.bar.baz\"\n=> [ \"foo\" \"bar\" \"baz\" ]\nsplitString \"\/\" \"\/usr\/local\/bin\"\n=> [ \"\" \"usr\" \"local\" \"bin\" ]\n\nlib.strings.removePrefixstring -> string -> string\n\nReturn a string without the specified prefix, if the prefix matches.\n\nprefix\n\nPrefix to remove if it matches\n\nstr\n\nInput string\n\nlib.strings.removePrefix usage example\n\nremovePrefix \"foo.\" \"foo.bar.baz\"\n=> \"bar.baz\"\nremovePrefix \"xxx\" \"foo.bar.baz\"\n=> \"foo.bar.baz\"\n\nlib.strings.removeSuffixstring -> string -> string\n\nReturn a string without the specified suffix, if the suffix matches.\n\nsuffix\n\nSuffix to remove if it matches\n\nstr\n\nInput string\n\nlib.strings.removeSuffix usage example\n\nremoveSuffix \"front\" \"homefront\"\n=> \"home\"\nremoveSuffix \"xxx\" \"homefront\"\n=> \"homefront\"\n\nlib.strings.versionOlderReturn true if string v1 denotes a version older than v2.\n\nv1\n\nFunction argument\n\nv2\n\nFunction argument\n\nlib.strings.versionOlder usage example\n\nversionOlder \"1.1\" \"1.2\"\n=> true\nversionOlder \"1.1\" \"1.1\"\n=> false\n\nlib.strings.versionAtLeastReturn true if string v1 denotes a version equal to or newer than v2.\n\nv1\n\nFunction argument\n\nv2\n\nFunction argument\n\nlib.strings.versionAtLeast usage example\n\nversionAtLeast \"1.1\" \"1.0\"\n=> true\nversionAtLeast \"1.1\" \"1.1\"\n=> true\nversionAtLeast \"1.1\" \"1.2\"\n=> false\n\nlib.strings.getNameThis function takes an argument that's either a derivation or a derivation's \"name\" attribute and\nextracts the name part from that argument.\n\nx\n\nFunction argument\n\nlib.strings.getName usage example\n\ngetName \"youtube-dl-2016.01.01\"\n=> \"youtube-dl\"\ngetName pkgs.youtube-dl\n=> \"youtube-dl\"\n\nlib.strings.getVersionThis function takes an argument that's either a derivation or a derivation's \"name\" attribute and\nextracts the version part from that argument.\n\nx\n\nFunction argument\n\nlib.strings.getVersion usage example\n\ngetVersion \"youtube-dl-2016.01.01\"\n=> \"2016.01.01\"\ngetVersion pkgs.youtube-dl\n=> \"2016.01.01\"\n\nlib.strings.nameFromURLExtract name with version from URL. Ask for separator which is supposed to start extension.\n\nurl\n\nFunction argument\n\nsep\n\nFunction argument\n\nlib.strings.nameFromURL usage example\n\nnameFromURL \"https:\/\/nixos.org\/releases\/nix\/nix-1.7\/nix-1.7-x86_64-linux.tar.bz2\" \"-\"\n=> \"nix\"\nnameFromURL \"https:\/\/nixos.org\/releases\/nix\/nix-1.7\/nix-1.7-x86_64-linux.tar.bz2\" \"_\"\n=> \"nix-1.7-x86\"\n\nlib.strings.enableFeatureCreate an --{enable,disable}- string that can be passed to standard GNU Autoconf scripts.\n\nenable\n\nFunction argument\n\nfeat\n\nFunction argument\n\nlib.strings.enableFeature usage example\n\nenableFeature true \"shared\"\n=> \"--enable-shared\"\nenableFeature false \"shared\"\n=> \"--disable-shared\"\n\nlib.strings.enableFeatureAsCreate an --{enable-=,disable-} string that can be passed to standard GNU Autoconf scripts.\n\nenable\n\nFunction argument\n\nfeat\n\nFunction argument\n\nvalue\n\nFunction argument\n\nlib.strings.enableFeatureAs usage example\n\nenableFeatureAs true \"shared\" \"foo\"\n=> \"--enable-shared=foo\"\nenableFeatureAs false \"shared\" (throw \"ignored\")\n=> \"--disable-shared\"\n\nlib.strings.withFeatureCreate an --{with,without}- string that can be passed to standard GNU Autoconf scripts.\n\nwith_\n\nFunction argument\n\nfeat\n\nFunction argument\n\nlib.strings.withFeature usage example\n\nwithFeature true \"shared\"\n=> \"--with-shared\"\nwithFeature false \"shared\"\n=> \"--without-shared\"\n\nlib.strings.withFeatureAsCreate an --{with-=,without-} string that can be passed to standard GNU Autoconf scripts.\n\nwith_\n\nFunction argument\n\nfeat\n\nFunction argument\n\nvalue\n\nFunction argument\n\nlib.strings.withFeatureAs usage example\n\nwithFeatureAs true \"shared\" \"foo\"\n=> \"--with-shared=foo\"\nwithFeatureAs false \"shared\" (throw \"ignored\")\n=> \"--without-shared\"\n\nlib.strings.fixedWidthStringfixedWidthString :: int -> string -> string -> string\n\nCreate a fixed width string with additional prefix to match required width.\n\nThis function will fail if the input string is longer than the requested length.\n\nwidth\n\nFunction argument\n\nfiller\n\nFunction argument\n\nstr\n\nFunction argument\n\nlib.strings.fixedWidthString usage example\n\nfixedWidthString 5 \"0\" (toString 15)\n=> \"00015\"\n\nlib.strings.fixedWidthNumberFormat a number adding leading zeroes up to fixed width.\n\nwidth\n\nFunction argument\n\nn\n\nFunction argument\n\nlib.strings.fixedWidthNumber usage example\n\nfixedWidthNumber 5 15\n=> \"00015\"\n\nlib.strings.floatToStringConvert a float to a string, but emit a warning when precision is lost during the conversion\n\nfloat\n\nFunction argument\n\nlib.strings.floatToString usage example\n\nfloatToString 0.000001\n=> \"0.000001\"\nfloatToString 0.0000001\n=> trace: warning: Imprecise conversion from float to string 0.000000\n\"0.000000\"\n\nlib.strings.isCoercibleToStringCheck whether a value can be coerced to a string\n\nx\n\nFunction argument\n\nlib.strings.isStorePathCheck whether a value is a store path.\n\nx\n\nFunction argument\n\nlib.strings.isStorePath usage example\n\nisStorePath \"\/nix\/store\/d945ibfx9x185xf04b890y4f9g3cbb63-python-2.7.11\/bin\/python\"\n=> false\nisStorePath \"\/nix\/store\/d945ibfx9x185xf04b890y4f9g3cbb63-python-2.7.11\"\n=> true\nisStorePath pkgs.python\n=> true\nisStorePath [] || isStorePath 42 || isStorePath {} || …\n=> false\n\nlib.strings.toIntstring -> int\n\nParse a string as an int.\n\nstr\n\nFunction argument\n\nlib.strings.toInt usage example\n\ntoInt \"1337\"\n=> 1337\ntoInt \"-4\"\n=> -4\ntoInt \"3.14\"\n=> error: floating point JSON numbers are not supported\n\nlib.strings.readPathsFromFileRead a list of paths from file, relative to the rootPath. Lines beginning with # are\ntreated as comments and ignored. Whitespace is significant.\n\nNOTE: This function is not performant and should be avoided.\n\nlib.strings.readPathsFromFile usage example\n\nreadPathsFromFile \/prefix\n.\/pkgs\/development\/libraries\/qt-5\/5.4\/qtbase\/series\n=> [ \"\/prefix\/dlopen-resolv.patch\" \"\/prefix\/tzdir.patch\"\n\"\/prefix\/dlopen-libXcursor.patch\" \"\/prefix\/dlopen-openssl.patch\"\n\"\/prefix\/dlopen-dbus.patch\" \"\/prefix\/xdg-config-dirs.patch\"\n\"\/prefix\/nix-profiles-library-paths.patch\"\n\"\/prefix\/compose-search-path.patch\" ]\n\nlib.strings.fileContentsfileContents :: path -> string\n\nRead the contents of a file removing the trailing \\n\n\nfile\n\nFunction argument\n\nlib.strings.fileContents usage example\n\n$ echo \"1.0\" > .\/version\n\nfileContents .\/version\n=> \"1.0\"\n\nlib.strings.sanitizeDerivationNamesanitizeDerivationName :: String -> String\n\nCreates a valid derivation name from a potentially invalid one.\n\nstring\n\nFunction argument\n\nlib.strings.sanitizeDerivationName usage example\n\nsanitizeDerivationName \"..\/hello.bar # foo\"\n=> \"-hello.bar-foo\"\nsanitizeDerivationName \"\"\n=> \"unknown\"\nsanitizeDerivationName pkgs.hello\n=> \"-nix-store-2g75chlbpxlrqn15zlby2dfh8hr9qwbk-hello-2.10\"\n" }
,{ "url": "functions\/library\/trivial\/", "title": "Miscellaneous functions", "text": "Miscellaneous functionslib.trivial.idid :: a -> a\n\nThe identity function For when you need a function that does “nothing”.\n\nx\n\nThe value to return\n\nlib.trivial.constconst :: a -> b -> a\n\nThe constant function\n\nIgnores the second argument. If called with only one argument, constructs a function that always returns a static value.\n\nx\n\nValue to return\n\ny\n\nValue to ignore\n\nlib.trivial.const usage example\n\nlet f = const 5; in f 10\n=> 5\n\nlib.trivial.pipepipe :: a -> [<functions>] -> <return type of last function>\n\nPipes a value through a list of functions, left to right.\n\nval\n\nFunction argument\n\nfunctions\n\nFunction argument\n\nlib.trivial.pipe usage example\n\npipe 2 [\n(x: x + 2)  # 2 + 2 = 4\n(x: x * 2)  # 4 * 2 = 8\n]\n=> 8\n\n# ideal to do text transformations\npipe [ \"a\/b\" \"a\/c\" ] [\n\n# create the cp command\n(map (file: ''cp \"${src}\/${file}\" $out\\n''))\n\n# concatenate all commands into one string\nlib.concatStrings\n\n# make that string into a nix derivation\n(pkgs.runCommand \"copy-to-out\" {})\n\n]\n=> <drv which copies all files to $out>\n\nThe output type of each function has to be the input type\nof the next function, and the last function returns the\nfinal value.\n\nlib.trivial.concatnote please don’t add a function like compose = flip pipe. This would confuse users, because the order\nof the functions in the list is not clear. With pipe, it’s obvious that it goes first-to-last. With compose, not so\nmuch.\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nlib.trivial.orboolean “or”\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nlib.trivial.andboolean “and”\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nlib.trivial.bitAndbitwise “and”\n\nlib.trivial.bitOrbitwise “or”\n\nlib.trivial.bitXorbitwise “xor”\n\nlib.trivial.bitNotbitwise “not”\n\nlib.trivial.boolToStringboolToString :: bool -> string\n\nConvert a boolean to a string.\n\nThis function uses the strings \"true\" and \"false\" to represent boolean values. Calling toString on a bool instead\nreturns \"1\" and \"\" (sic!).\n\nb\n\nFunction argument\n\nlib.trivial.mergeAttrsMerge two attribute sets shallowly, right side trumps left\n\nmergeAttrs :: attrs -> attrs -> attrs\n\nx\n\nLeft attribute set\n\ny\n\nRight attribute set (higher precedence for equal keys)\n\nlib.trivial.mergeAttrs usage example\n\nmergeAttrs { a = 1; b = 2; } { b = 3; c = 4; }\n=> { a = 1; b = 3; c = 4; }\n\nlib.trivial.flipflip :: (a -> b -> c) -> (b -> a -> c)\n\nFlip the order of the arguments of a binary function.\n\nf\n\nFunction argument\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nlib.trivial.flip usage example\n\nflip concat [1] [2]\n=> [ 2 1 ]\n\nlib.trivial.mapNullableApply function if the supplied argument is non-null.\n\nf\n\nFunction to call\n\na\n\nArgument to check for null before passing it to f\n\nlib.trivial.mapNullable usage example\n\nmapNullable (x: x+1) null\n=> null\nmapNullable (x: x+1) 22\n=> 23\n\nlib.trivial.versionReturns the current full nixpkgs version number.\n\nlib.trivial.releaseReturns the current nixpkgs release number as string.\n\nlib.trivial.codeNameReturns the current nixpkgs release code name.\n\nOn each release the first letter is bumped and a new animal is chosen starting with that new letter.\n\nlib.trivial.versionSuffixReturns the current nixpkgs version suffix as string.\n\nlib.trivial.revisionWithDefaultrevisionWithDefault :: string -> string\n\nAttempts to return the the current revision of nixpkgs and returns the supplied default value otherwise.\n\ndefault\n\nDefault value to return if revision can not be determined\n\nlib.trivial.inNixShellinNixShell :: bool\n\nDetermine whether the function is being called from inside a Nix shell.\n\nlib.trivial.minReturn minimum of two numbers.\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nlib.trivial.maxReturn maximum of two numbers.\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nlib.trivial.modInteger modulus\n\nbase\n\nFunction argument\n\nint\n\nFunction argument\n\nlib.trivial.mod usage example\n\nmod 11 10\n=> 1\nmod 1 10\n=> 1\n\nlib.trivial.compareC-style comparisons\n\na < b, compare a b => -1 a == b, compare a b => 0 a > b, compare a b => 1\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nlib.trivial.splitByAndCompare(a -> bool) -> (a -> a -> int) -> (a -> a -> int) -> (a -> a -> int)\n\nSplit type into two subtypes by predicate p, take all elements of the first subtype to be less than all the elements of\nthe second subtype, compare elements of a single subtype with yes and no respectively.\n\np\n\nPredicate\n\nyes\n\nComparison function if predicate holds for both values\n\nno\n\nComparison function if predicate holds for neither value\n\na\n\nFirst value to compare\n\nb\n\nSecond value to compare\n\nlib.trivial.splitByAndCompare usage example\n\nlet cmp = splitByAndCompare (hasPrefix \"foo\") compare compare; in\n\ncmp \"a\" \"z\" => -1\ncmp \"fooa\" \"fooz\" => -1\n\ncmp \"f\" \"a\" => 1\ncmp \"fooa\" \"a\" => -1\n# while\ncompare \"fooa\" \"a\" => 1\n\nlib.trivial.importJSONReads a JSON file.\n\nType :: path -> any\n\npath\n\nFunction argument\n\nlib.trivial.importTOMLReads a TOML file.\n\nType :: path -> any\n\npath\n\nFunction argument\n\nlib.trivial.setFunctionArgsAdd metadata about expected function arguments to a function. The metadata should match the\nformat given by builtins.functionArgs, i.e. a set from expected argument to a bool representing whether that argument\nhas a default or not. setFunctionArgs : (a → b) → Map String Bool → (a → b)\n\nThis function is necessary because you can't dynamically create a function of the { a, b ? foo, ... }: format, but some\nfacilities like callPackage expect to be able to query expected arguments.\n\nf\n\nFunction argument\n\nargs\n\nFunction argument\n\nlib.trivial.functionArgsExtract the expected function arguments from a function. This works both with nix-native { a, b\n? foo, ... }: style functions and functions with args set with 'setFunctionArgs'. It has the same return type and\nsemantics as builtins.functionArgs. setFunctionArgs : (a → b) → Map String Bool.\n\nf\n\nFunction argument\n\nlib.trivial.isFunctionCheck whether something is a function or something annotated with function args.\n\nf\n\nFunction argument\n\nlib.trivial.toHexStringConvert the given positive integer to a string of its hexadecimal representation. For example:\n\ntoHexString 0 => \"0\"\n\ntoHexString 16 => \"10\"\n\ntoHexString 250 => \"FA\"\n\ni\n\nFunction argument\n\nlib.trivial.toBaseDigitstoBaseDigits base i converts the positive integer i to a list of its digits in the given base.\nFor example:\n\ntoBaseDigits 10 123 => [ 1 2 3 ]\n\ntoBaseDigits 2 6 => [ 1 1 0 ]\n\ntoBaseDigits 16 250 => [ 15 10 ]\n\nbase\n\nFunction argument\n\ni\n\nFunction argument\n" }
,{ "url": "functions\/generators\/", "title": "Generators", "text": "GeneratorsGenerators are functions that create file formats from nix data structures, e. g. for configuration files.\nThere are generators available for: INI, JSON and YAML\n\nAll generators follow a similar call interface: generatorName configFunctions data, where configFunctions is an attrset\nof user-defined functions that format nested parts of the content. They each have common defaults, so often they do not\nneed to be set manually. An example is mkSectionName ? (name: libStr.escape [ \"[\" \"]\" ] name) from the INI generator. It\nreceives the name of a section and sanitizes it. The default mkSectionName escapes [ and ] with a backslash.\n\nGenerators can be fine-tuned to produce exactly the file format required by your application\/service. One example is an\nINI-file format which uses :  as separator, the strings \"yes\"\/\"no\" as boolean values and requires all string values to\nbe quoted:\n\nwith lib;\nlet\n  customToINI = generators.toINI {\n    # specifies how to format a key\/value pair\n    mkKeyValue = generators.mkKeyValueDefault {\n      # specifies the generated string for a subset of nix values\n      mkValueString = v:\n             if v == true then ''\"yes\"''\n        else if v == false then ''\"no\"''\n        else if isString v then ''\"${v}\"''\n        # and delegats all other values to the default generator\n        else generators.mkValueStringDefault {} v;\n    } \":\";\n  };\n\n# the INI file can now be given as plain old nix values\nin customToINI {\n  main = {\n    pushinfo = true;\n    autopush = false;\n    host = \"localhost\";\n    port = 42;\n  };\n  mergetool = {\n    merge = \"diff3\";\n  };\n}\n\nThis will produce the following INI file as nix string:\n\n[main]\nautopush:\"no\"\nhost:\"localhost\"\nport:42\npushinfo:\"yes\"\nstr\\:ange:\"very::strange\"\n\n[mergetool]\nmerge:\"diff3\"\n\nNix store paths can be converted to strings by enclosing a derivation attribute like so: \"${drv}\". \n\nDetailed documentation for each generator can be found in lib\/generators.nix.\n" }
,{ "url": "functions\/debug\/", "title": "Debugging Nix Expressions", "text": "Debugging Nix ExpressionsNix is a unityped, dynamic language, this means every value can potentially appear anywhere.\nSince it is also non-strict, evaluation order and what ultimately is evaluated might surprise you. Therefore it is\nimportant to be able to debug nix expressions.\n\nIn the lib\/debug.nix file you will find a number of functions that help (pretty-)printing values while evaluation is\nrunning. You can even specify how deep these values should be printed recursively, and transform them on the fly. Please\nconsult the docstrings in lib\/debug.nix for usage information.\n" }
,{ "url": "functions\/prefer-remote-fetch\/", "title": "prefer-remote-fetch overlay", "text": "prefer-remote-fetch overlayprefer-remote-fetch is an overlay that download sources on remote builder. This is useful\nwhen the evaluating machine has a slow upload while the builder can fetch faster directly from the source. To use it,\nput the following snippet as a new overlay:\n\nself: super:\n  (super.prefer-remote-fetch self super)\n\nA full configuration example for that sets the overlay up for your own account, could look like this\n\n$ mkdir ~\/.config\/nixpkgs\/overlays\/\n$ cat > ~\/.config\/nixpkgs\/overlays\/prefer-remote-fetch.nix <<EOF\n  self: super: super.prefer-remote-fetch self super\nEOF\n" }
,{ "url": "functions\/nix-gitignore\/", "title": "pkgs.nix-gitignore", "text": "pkgs.nix-gitignorepkgs.nix-gitignore is a function that acts similarly to builtins.filterSource but also allows\nfiltering with the help of the gitignore format.\n\nUsagepkgs.nix-gitignore exports a number of functions, but you'll most likely need either gitignoreSource or\ngitignoreSourcePure. As their first argument, they both accept either 1. a file with gitignore lines or 2. a string with\ngitignore lines, or 3. a list of either of the two. They will be concatenated into a single big string.\n\n{ pkgs ? import <nixpkgs> {} }:\n\n nix-gitignore.gitignoreSource [] .\/source\n     # Simplest version\n\n nix-gitignore.gitignoreSource \"supplemental-ignores\\n\" .\/source\n     # This one reads the .\/source\/.gitignore and concats the auxiliary ignores\n\n nix-gitignore.gitignoreSourcePure \"ignore-this\\nignore-that\\n\" .\/source\n     # Use this string as gitignore, don't read .\/source\/.gitignore.\n\n nix-gitignore.gitignoreSourcePure [\"ignore-this\\nignore-that\\n\", ~\/.gitignore] .\/source\n     # It also accepts a list (of strings and paths) that will be concatenated\n     # once the paths are turned to strings via readFile.\n\nThese functions are derived from the Filter functions by setting the first filter argument to (_: _: true):\n\ngitignoreSourcePure = gitignoreFilterSourcePure (_: _: true);\ngitignoreSource = gitignoreFilterSource (_: _: true);\n\nThose filter functions accept the same arguments the builtins.filterSource function would pass to its filters, thus fn:\ngitignoreFilterSourcePure fn \"\" should be extensionally equivalent to filterSource. The file is blacklisted if it's\nblacklisted by either your filter or the gitignoreFilter.\n\nIf you want to make your own filter from scratch, you may use\n\ngitignoreFilter = ign: root: filterPattern (gitignoreToPatterns ign) root;\n\ngitignore files in subdirectoriesIf you wish to use a filter that would search for .gitignore files in subdirectories,\njust like git does by default, use this function:\n\ngitignoreFilterRecursiveSource = filter: patterns: root:\n# OR\ngitignoreRecursiveSource = gitignoreFilterSourcePure (_: _: true);\n" }
,{ "url": "stdenv\/stdenv\/", "title": "The Standard Environment", "text": "The Standard EnvironmentThe standard build environment in the Nix Packages collection provides an environment for\nbuilding Unix packages that does a lot of common build tasks automatically. In fact, for Unix packages that use the\nstandard .\/configure; make; make install build interface, you don’t need to write a build script at all; the standard\nenvironment does everything automatically. If stdenv doesn’t do what you need automatically, you can easily customise or\noverride the various build phases.\n\nUsing stdenvTo build a package with the standard environment, you use the function stdenv.mkDerivation, instead of the\nprimitive built-in function derivation, e.g.\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  src = fetchurl {\n    url = \"http:\/\/example.org\/libfoo-1.2.3.tar.bz2\";\n    sha256 = \"0x2g1jqygyr5wiwg4ma1nd7w4ydpy82z9gkcv8vh2v8dn3y58v5m\";\n  };\n}\n\n(stdenv needs to be in scope, so if you write this in a separate Nix expression from pkgs\/all-packages.nix, you need to\npass it as a function argument.) Specifying a name and a src is the absolute minimum Nix requires. For convenience, you\ncan also use pname and version attributes and mkDerivation will automatically set name to \"${pname}-${version}\" by\ndefault. Since RFC 0035, this is preferred for packages in Nixpkgs, as it allows us to reuse the version easily:\n\nstdenv.mkDerivation rec {\n  pname = \"libfoo\";\n  version = \"1.2.3\";\n  src = fetchurl {\n    url = \"http:\/\/example.org\/libfoo-source-${version}.tar.bz2\";\n    sha256 = \"0x2g1jqygyr5wiwg4ma1nd7w4ydpy82z9gkcv8vh2v8dn3y58v5m\";\n  };\n}\n\nMany packages have dependencies that are not provided in the standard environment. It’s usually sufficient to specify\nthose dependencies in the buildInputs attribute:\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  ...\n  buildInputs = [libbar perl ncurses];\n}\n\nThis attribute ensures that the bin subdirectories of these packages appear in the PATH environment variable during the\nbuild, that their include subdirectories are searched by the C compiler, and so on. (See The Standard Environment for\ndetails.)\n\nOften it is necessary to override or modify some aspect of the build. To make this easier, the standard environment\nbreaks the package build into a number of phases, all of which can be overridden or modified individually: unpacking the\nsources, applying patches, configuring, building, and installing. (There are some others; see The Standard Environment.)\nFor instance, a package that doesn’t supply a makefile but instead has to be compiled \"manually\" could be handled like\nthis:\n\nstdenv.mkDerivation {\n  name = \"fnord-4.5\";\n  ...\n  buildPhase = ''\n    gcc foo.c -o foo\n  '';\n  installPhase = ''\n    mkdir -p $out\/bin\n    cp foo $out\/bin\n  '';\n}\n\n(Note the use of ''-style string literals, which are very convenient for large multi-line script fragments because they\ndon’t need escaping of \" and \\, and because indentation is intelligently removed.)\n\nThere are many other attributes to customise the build. These are listed in The Standard Environment.\n\nWhile the standard environment provides a generic builder, you can still supply your own build script:\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  ...\n  builder = .\/builder.sh;\n}\n\nwhere the builder can do anything it wants, but typically starts with\n\nsource $stdenv\/setup\n\nto let stdenv set up the environment (e.g., process the buildInputs). If you want, you can still use stdenv’s generic\nbuilder:\n\nsource $stdenv\/setup\n\nbuildPhase() {\n  echo \"... this is my custom build phase ...\"\n  gcc foo.c -o foo\n}\n\ninstallPhase() {\n  mkdir -p $out\/bin\n  cp foo $out\/bin\n}\n\ngenericBuild\n\nTools provided by stdenvThe standard environment provides the following packages:\n\n  - The GNU C Compiler, configured with C and C++ support.\n  - GNU coreutils (contains a few dozen standard Unix commands).\n  - GNU findutils (contains find).\n  - GNU diffutils (contains diff, cmp).\n  - GNU sed.\n  - GNU grep.\n  - GNU awk.\n  - GNU tar.\n  - gzip, bzip2 and xz.\n  - GNU Make.\n  - Bash. This is the shell used for all builders in the Nix Packages collection. Not using \/bin\/sh removes a large\n    source of portability problems.\n  - The patch command.\n\nOn Linux, stdenv also includes the patchelf utility.\n\nSpecifying dependenciesAs described in the Nix manual, almost any *.drv store path in a derivation’s attribute set will\ninduce a dependency on that derivation. mkDerivation, however, takes a few attributes intended to, between them, include\nall the dependencies of a package. This is done both for structure and consistency, but also so that certain other setup\ncan take place. For example, certain dependencies need their bin directories added to the PATH. That is built-in, but\nother setup is done via a pluggable mechanism that works in conjunction with these dependency attributes. See The\nStandard Environment for details.\n\nDependencies can be broken down along three axes: their host and target platforms relative to the new derivation’s, and\nwhether they are propagated. The platform distinctions are motivated by cross compilation; see Cross-compilation for\nexactly what each platform means. [^footnote-stdenv-ignored-build-platform] But even if one is not cross compiling, the\nplatforms imply whether or not the dependency is needed at run-time or build-time, a concept that makes perfect sense\noutside of cross compilation. By default, the run-time\/build-time distinction is just a hint for mental clarity, but\nwith strictDeps set it is mostly enforced even in the native case.\n\nThe extension of PATH with dependencies, alluded to above, proceeds according to the relative platforms alone. The\nprocess is carried out only for dependencies whose host platform matches the new derivation’s build platform i.e.\ndependencies which run on the platform where the new derivation will be built.\n[^footnote-stdenv-native-dependencies-in-path] For each dependency <dep> of those dependencies, dep\/bin, if present, is\nadded to the PATH environment variable.\n\nThe dependency is propagated when it forces some of its other-transitive (non-immediate) downstream dependencies to also\ntake it on as an immediate dependency. Nix itself already takes a package’s transitive dependencies into account, but\nthis propagation ensures nixpkgs-specific infrastructure like setup hooks (mentioned above) also are run as if the\npropagated dependency.\n\nIt is important to note that dependencies are not necessarily propagated as the same sort of dependency that they were\nbefore, but rather as the corresponding sort so that the platform rules still line up. The exact rules for dependency\npropagation can be given by assigning to each dependency two integers based one how its host and target platforms are\noffset from the depending derivation’s platforms. Those offsets are given below in the descriptions of each dependency\nlist attribute. Algorithmically, we traverse propagated inputs, accumulating every propagated dependency’s propagated\ndependencies and adjusting them to account for the “shift in perspective” described by the current dependency’s platform\noffsets. This results in sort a transitive closure of the dependency relation, with the offsets being approximately\nsummed when two dependency links are combined. We also prune transitive dependencies whose combined offsets go\nout-of-bounds, which can be viewed as a filter over that transitive closure removing dependencies that are blatantly\nabsurd.\n\nWe can define the process precisely with Natural Deduction using the inference rules. This probably seems a bit obtuse,\nbut so is the bash code that actually implements it! [^footnote-stdenv-find-inputs-location] They’re confusing in very\ndifferent ways so… hopefully if something doesn’t make sense in one presentation, it will in the other!\n\nlet mapOffset(h, t, i) = i + (if i <= 0 then h else t - 1)\n\npropagated-dep(h0, t0, A, B)\npropagated-dep(h1, t1, B, C)\nh0 + h1 in {-1, 0, 1}\nh0 + t1 in {-1, 0, 1}\n-------------------------------------- Transitive property\npropagated-dep(mapOffset(h0, t0, h1),\n               mapOffset(h0, t0, t1),\n               A, C)\n\nlet mapOffset(h, t, i) = i + (if i <= 0 then h else t - 1)\n\ndep(h0, _, A, B)\npropagated-dep(h1, t1, B, C)\nh0 + h1 in {-1, 0, 1}\nh0 + t1 in {-1, 0, -1}\n----------------------------- Take immediate dependencies' propagated dependencies\npropagated-dep(mapOffset(h0, t0, h1),\n               mapOffset(h0, t0, t1),\n               A, C)\n\npropagated-dep(h, t, A, B)\n----------------------------- Propagated dependencies count as dependencies\ndep(h, t, A, B)\n\nSome explanation of this monstrosity is in order. In the common case, the target offset of a dependency is the successor\nto the target offset: t = h + 1. That means that:\n\nlet f(h, t, i) = i + (if i <= 0 then h else t - 1)\nlet f(h, h + 1, i) = i + (if i <= 0 then h else (h + 1) - 1)\nlet f(h, h + 1, i) = i + (if i <= 0 then h else h)\nlet f(h, h + 1, i) = i + h\n\nThis is where “sum-like” comes in from above: We can just sum all of the host offsets to get the host offset of the\ntransitive dependency. The target offset is the transitive dependency is simply the host offset + 1, just as it was with\nthe dependencies composed to make this transitive one; it can be ignored as it doesn’t add any new information.\n\nBecause of the bounds checks, the uncommon cases are h = t and h + 2 = t. In the former case, the motivation for\nmapOffset is that since its host and target platforms are the same, no transitive dependency of it should be able to\n“discover” an offset greater than its reduced target offsets. mapOffset effectively “squashes” all its transitive\ndependencies’ offsets so that none will ever be greater than the target offset of the original h = t package. In the\nother case, h + 1 is skipped over between the host and target offsets. Instead of squashing the offsets, we need to\n“rip” them apart so no transitive dependencies’ offset is that one.\n\nOverall, the unifying theme here is that propagation shouldn’t be introducing transitive dependencies involving\nplatforms the depending package is unaware of. [One can imagine the dependending package asking for dependencies with\nthe platforms it knows about; other platforms it doesn’t know how to ask for. The platform description in that scenario\nis a kind of unforagable capability.] The offset bounds checking and definition of mapOffset together ensure that this\nis the case. Discovering a new offset is discovering a new platform, and since those platforms weren’t in the derivation\n“spec” of the needing package, they cannot be relevant. From a capability perspective, we can imagine that the host and\ntarget platforms of a package are the capabilities a package requires, and the depending package must provide the\ncapability to the dependency.\n\nVariables specifying dependenciesdepsBuildBuildA list of dependencies whose host and target platforms are the new\nderivation’s build platform. This means a -1 host and -1 target offset from the new derivation’s platforms. These are\nprograms and libraries used at build time that produce programs and libraries also used at build time. If the dependency\ndoesn’t care about the target platform (i.e. isn’t a compiler or similar tool), put it in nativeBuildInputs instead. The\nmost common use of this buildPackages.stdenv.cc, the default C compiler for this role. That example crops up more than\none might think in old commonly used C libraries.\n\nSince these packages are able to be run at build-time, they are always added to the PATH, as described above. But since\nthese packages are only guaranteed to be able to run then, they shouldn’t persist as run-time dependencies. This isn’t\ncurrently enforced, but could be in the future.\n\nnativeBuildInputsA list of dependencies whose host platform is the new derivation’s build platform, and target platform\nis the new derivation’s host platform. This means a -1 host offset and 0 target offset from the new derivation’s\nplatforms. These are programs and libraries used at build-time that, if they are a compiler or similar tool, produce\ncode to run at run-time—i.e. tools used to build the new derivation. If the dependency doesn’t care about the target\nplatform (i.e. isn’t a compiler or similar tool), put it here, rather than in depsBuildBuild or depsBuildTarget. This\ncould be called depsBuildHost but nativeBuildInputs is used for historical continuity.\n\nSince these packages are able to be run at build-time, they are added to the PATH, as described above. But since these\npackages are only guaranteed to be able to run then, they shouldn’t persist as run-time dependencies. This isn’t\ncurrently enforced, but could be in the future.\n\ndepsBuildTargetA list of dependencies whose host platform is the new derivation’s build platform, and target platform is\nthe new derivation’s target platform. This means a -1 host offset and 1 target offset from the new derivation’s\nplatforms. These are programs used at build time that produce code to run with code produced by the depending package.\nMost commonly, these are tools used to build the runtime or standard library that the currently-being-built compiler\nwill inject into any code it compiles. In many cases, the currently-being-built-compiler is itself employed for that\ntask, but when that compiler won’t run (i.e. its build and host platform differ) this is not possible. Other times, the\ncompiler relies on some other tool, like binutils, that is always built separately so that the dependency is\nunconditional.\n\nThis is a somewhat confusing concept to wrap one’s head around, and for good reason. As the only dependency type where\nthe platform offsets are not adjacent integers, it requires thinking of a bootstrapping stage two away from the current\none. It and its use-case go hand in hand and are both considered poor form: try to not need this sort of dependency, and\ntry to avoid building standard libraries and runtimes in the same derivation as the compiler produces code using them.\nInstead strive to build those like a normal library, using the newly-built compiler just as a normal library would. In\nshort, do not use this attribute unless you are packaging a compiler and are sure it is needed.\n\nSince these packages are able to run at build time, they are added to the PATH, as described above. But since these\npackages are only guaranteed to be able to run then, they shouldn’t persist as run-time dependencies. This isn’t\ncurrently enforced, but could be in the future.\n\ndepsHostHostA list of dependencies whose host and target platforms match the new derivation’s host platform. This means\na 0 host offset and 0 target offset from the new derivation’s host platform. These are packages used at run-time to\ngenerate code also used at run-time. In practice, this would usually be tools used by compilers for macros or a\nmetaprogramming system, or libraries used by the macros or metaprogramming code itself. It’s always preferable to use a\ndepsBuildBuild dependency in the derivation being built over a depsHostHost on the tool doing the building for this\npurpose.\n\nbuildInputsA list of dependencies whose host platform and target platform match the new derivation’s. This means a 0\nhost offset and a 1 target offset from the new derivation’s host platform. This would be called depsHostTarget but for\nhistorical continuity. If the dependency doesn’t care about the target platform (i.e. isn’t a compiler or similar tool),\nput it here, rather than in depsBuildBuild.\n\nThese are often programs and libraries used by the new derivation at run-time, but that isn’t always the case. For\nexample, the machine code in a statically-linked library is only used at run-time, but the derivation containing the\nlibrary is only needed at build-time. Even in the dynamic case, the library may also be needed at build-time to appease\nthe linker.\n\ndepsTargetTargetA list of dependencies whose host platform matches the new derivation’s target platform. This means a 1\noffset from the new derivation’s platforms. These are packages that run on the target platform, e.g. the standard\nlibrary or run-time deps of standard library that a compiler insists on knowing about. It’s poor form in almost all\ncases for a package to depend on another from a future stage [future stage corresponding to positive offset]. Do not use\nthis attribute unless you are packaging a compiler and are sure it is needed.\n\ndepsBuildBuildPropagatedThe propagated equivalent of depsBuildBuild. This perhaps never ought to be used, but it is\nincluded for consistency [see below for the others].\n\npropagatedNativeBuildInputsThe propagated equivalent of nativeBuildInputs. This would be called depsBuildHostPropagated\nbut for historical continuity. For example, if package Y has propagatedNativeBuildInputs = [X], and package Z has\nbuildInputs = [Y], then package Z will be built as if it included package X in its nativeBuildInputs. If instead,\npackage Z has nativeBuildInputs = [Y], then Z will be built as if it included X in the depsBuildBuild of package Z,\nbecause of the sum of the two -1 host offsets.\n\ndepsBuildTargetPropagatedThe propagated equivalent of depsBuildTarget. This is prefixed for the same reason of alerting\npotential users.\n\ndepsHostHostPropagatedThe propagated equivalent of depsHostHost.\n\npropagatedBuildInputsThe propagated equivalent of buildInputs. This would be called depsHostTargetPropagated but for\nhistorical continuity.\n\ndepsTargetTargetPropagatedThe propagated equivalent of depsTargetTarget. This is prefixed for the same reason of\nalerting potential users.\n\nAttributesVariables affecting stdenv initialisationNIX_DEBUGA natural number indicating how much information to log. If\nset to 1 or higher, stdenv will print moderate debugging information during the build. In particular, the gcc and ld\nwrapper scripts will print out the complete command line passed to the wrapped tools. If set to 6 or higher, the stdenv\nsetup script will be run with set -x tracing. If set to 7 or higher, the gcc and ld wrapper scripts will also be run\nwith set -x tracing.\n\nAttributes affecting build propertiesenableParallelBuildingIf set to true, stdenv will pass specific flags to make and\nother build tools to enable parallel building with up to build-cores workers.\n\nUnless set to false, some build systems with good support for parallel building including cmake, meson, and qmake will\nset it to true.\n\nSpecial variablespassthruThis is an attribute set which can be filled with arbitrary values. For example:\n\npassthru = {\n  foo = \"bar\";\n  baz = {\n    value1 = 4;\n    value2 = 5;\n  };\n}\n\nValues inside it are not passed to the builder, so you can change them without triggering a rebuild. However, they can\nbe accessed outside of a derivation directly, as if they were set inside a derivation itself, e.g. hello.baz.value1. We\ndon’t specify any usage or schema of passthru - it is meant for values that would be useful outside the derivation in\nother parts of a Nix expression (e.g. in other derivations). An example would be to convey some specific dependency of\nyour derivation which contains a program with plugins support. Later, others who make derivations with plugins can use\npassed-through dependency to ensure that their plugin would be binary-compatible with built program.\n\npassthru.updateScriptA script to be run by maintainers\/scripts\/update.nix when the package is matched. It needs to be an\nexecutable file, either on the file system:\n\npassthru.updateScript = .\/update.sh;\n\nor inside the expression itself:\n\npassthru.updateScript = writeScript \"update-zoom-us\" ''\n  #!\/usr\/bin\/env nix-shell\n  #!nix-shell -i bash -p curl pcre common-updater-scripts\n\n  set -eu -o pipefail\n\n  version=\"$(curl -sI https:\/\/zoom.us\/client\/latest\/zoom_x86_64.tar.xz | grep -Fi 'Location:' | pcregrep -o1 '\/(([0-9]\\.?)+)\/')\"\n  update-source-version zoom-us \"$version\"\n'';\n\nThe attribute can also contain a list, a script followed by arguments to be passed to it:\n\npassthru.updateScript = [ ..\/..\/update.sh pname \"--requested-release=unstable\" ];\n\nThe script will be run with UPDATE_NIX_ATTR_PATH environment variable set to the attribute path it is supposed to\nupdate.\n\nThe script will be usually run from the root of the Nixpkgs repository but you should not rely on that. Also note that\nthe update scripts will be run in parallel by default; you should avoid running git commit or any other commands that\ncannot handle that. \n\nFor information about how to run the updates, execute nix-shell maintainers\/scripts\/update.nix.\n\nPhasesThe generic builder has a number of phases. Package builds are split into phases to make it easier to override\nspecific parts of the build (e.g., unpacking the sources or installing the binaries). Furthermore, it allows a nicer\npresentation of build logs in the Nix build farm.\n\nEach phase can be overridden in its entirety either by setting the environment variable namePhase to a string containing\nsome shell commands to be executed, or by redefining the shell function namePhase. The former is convenient to override\na phase from the derivation, while the latter is convenient from a build script. However, typically one only wants to\nadd some commands to a phase, e.g. by defining postInstall or preFixup, as skipping some of the default actions may have\nunexpected consequences. The default script for each phase is defined in the file pkgs\/stdenv\/generic\/setup.sh.\n\nControlling phasesThere are a number of variables that control what phases are executed and in what order:\n\nVariables affecting phase controlphasesSpecifies the phases. You can change the order in which phases are executed, or\nadd new phases, by setting this variable. If it’s not set, the default value is used, which is $prePhases unpackPhase\npatchPhase $preConfigurePhases configurePhase $preBuildPhases buildPhase checkPhase $preInstallPhases installPhase\nfixupPhase installCheckPhase $preDistPhases distPhase $postPhases.\n\nUsually, if you just want to add a few phases, it’s more convenient to set one of the variables below (such as\npreInstallPhases), as you then don’t specify all the normal phases.\n\nprePhasesAdditional phases executed before any of the default phases.\n\npreConfigurePhasesAdditional phases executed just before the configure phase.\n\npreBuildPhasesAdditional phases executed just before the build phase.\n\npreInstallPhasesAdditional phases executed just before the install phase.\n\npreFixupPhasesAdditional phases executed just before the fixup phase.\n\npreDistPhasesAdditional phases executed just before the distribution phase.\n\npostPhasesAdditional phases executed after any of the default phases.\n\nThe unpack phaseThe unpack phase is responsible for unpacking the source code of the package. The default implementation\nof unpackPhase unpacks the source files listed in the src environment variable to the current directory. It supports the\nfollowing files by default:\n\nTar filesThese can optionally be compressed using gzip (.tar.gz, .tgz or .tar.Z), bzip2 (.tar.bz2, .tbz2 or .tbz) or xz\n(.tar.xz, .tar.lzma or .txz).\n\nZip filesZip files are unpacked using unzip. However, unzip is not in the standard environment, so you should add it to\nnativeBuildInputs yourself.\n\nDirectories in the Nix storeThese are simply copied to the current directory. The hash part of the file name is\nstripped, e.g. \/nix\/store\/1wydxgby13cz...-my-sources would be copied to my-sources.\n\nAdditional file types can be supported by setting the unpackCmd variable (see below).\n\nVariables controlling the unpack phasesrcs \/ srcThe list of source files or directories to be unpacked or copied. One of\nthese must be set.\n\nsourceRootAfter running unpackPhase, the generic builder changes the current directory to the directory created by\nunpacking the sources. If there are multiple source directories, you should set sourceRoot to the name of the intended\ndirectory.\n\nsetSourceRootAlternatively to setting sourceRoot, you can set setSourceRoot to a shell command to be evaluated by the\nunpack phase after the sources have been unpacked. This command must set sourceRoot.\n\npreUnpackHook executed at the start of the unpack phase.\n\npostUnpackHook executed at the end of the unpack phase.\n\ndontUnpackSet to true to skip the unpack phase.\n\ndontMakeSourcesWritableIf set to 1, the unpacked sources are not made writable. By default, they are made writable to\nprevent problems with read-only sources. For example, copied store directories would be read-only without this.\n\nunpackCmdThe unpack phase evaluates the string $unpackCmd for any unrecognised file. The path to the current source file\nis contained in the curSrc variable.\n\nThe patch phaseThe patch phase applies the list of patches defined in the patches variable.\n\nVariables controlling the patch phasedontPatchSet to true to skip the patch phase.\n\npatchesThe list of patches. They must be in the format accepted by the patch command, and may optionally be compressed\nusing gzip (.gz), bzip2 (.bz2) or xz (.xz).\n\npatchFlagsFlags to be passed to patch. If not set, the argument -p1 is used, which causes the leading directory\ncomponent to be stripped from the file names in each patch.\n\nprePatchHook executed at the start of the patch phase.\n\npostPatchHook executed at the end of the patch phase.\n\nThe configure phaseThe configure phase prepares the source tree for building. The default configurePhase runs\n.\/configure (typically an Autoconf-generated script) if it exists.\n\nVariables controlling the configure phaseconfigureScriptThe name of the configure script. It defaults to .\/configure if\nit exists; otherwise, the configure phase is skipped. This can actually be a command (like perl .\/Configure.pl).\n\nconfigureFlagsA list of strings passed as additional arguments to the configure script.\n\ndontConfigureSet to true to skip the configure phase.\n\nconfigureFlagsArrayA shell array containing additional arguments passed to the configure script. You must use this\ninstead of configureFlags if the arguments contain spaces.\n\ndontAddPrefixBy default, the flag --prefix=$prefix is added to the configure flags. If this is undesirable, set this\nvariable to true.\n\nprefixThe prefix under which the package must be installed, passed via the --prefix option to the configure script. It\ndefaults to $out.\n\nprefixKeyThe key to use when specifying the prefix. By default, this is set to --prefix= as that is used by the majority\nof packages.\n\ndontAddDisableDepTrackBy default, the flag --disable-dependency-tracking is added to the configure flags to speed up\nAutomake-based builds. If this is undesirable, set this variable to true.\n\ndontFixLibtoolBy default, the configure phase applies some special hackery to all files called ltmain.sh before running\nthe configure script in order to improve the purity of Libtool-based packages [^footnote-stdenv-sys-lib-search-path] .\nIf this is undesirable, set this variable to true.\n\ndontDisableStaticBy default, when the configure script has --enable-static, the option --disable-static is added to the\nconfigure flags.\n\nIf this is undesirable, set this variable to true.\n\nconfigurePlatformsBy default, when cross compiling, the configure script has --build=... and --host=... passed. Packages\ncan instead pass [ \"build\" \"host\" \"target\" ] or a subset to control exactly which platform flags are passed. Compilers\nand other tools can use this to also pass the target platform. [^footnote-stdenv-build-time-guessing-impurity]\n\npreConfigureHook executed at the start of the configure phase.\n\npostConfigureHook executed at the end of the configure phase.\n\nThe build phaseThe build phase is responsible for actually building the package (e.g. compiling it). The default\nbuildPhase simply calls make if a file named Makefile, makefile or GNUmakefile exists in the current directory (or the\nmakefile is explicitly set); otherwise it does nothing.\n\nVariables controlling the build phasedontBuildSet to true to skip the build phase.\n\nmakefileThe file name of the Makefile.\n\nmakeFlagsA list of strings passed as additional flags to make. These flags are also used by the default install and\ncheck phase. For setting make flags specific to the build phase, use buildFlags (see below).\n\nmakeFlags = [ \"PREFIX=$(out)\" ];\n\nThe flags are quoted in bash, but environment variables can be specified by using the make syntax. \n\nmakeFlagsArrayA shell array containing additional arguments passed to make. You must use this instead of makeFlags if\nthe arguments contain spaces, e.g.\n\npreBuild = ''\n  makeFlagsArray+=(CFLAGS=\"-O0 -g\" LDFLAGS=\"-lfoo -lbar\")\n'';\n\nNote that shell arrays cannot be passed through environment variables, so you cannot set makeFlagsArray in a derivation\nattribute (because those are passed through environment variables): you have to define them in shell code.\n\nbuildFlags \/ buildFlagsArrayA list of strings passed as additional flags to make. Like makeFlags and makeFlagsArray, but\nonly used by the build phase.\n\npreBuildHook executed at the start of the build phase.\n\npostBuildHook executed at the end of the build phase.\n\nYou can set flags for make through the makeFlags variable.\n\nBefore and after running make, the hooks preBuild and postBuild are called, respectively.\n\nThe check phaseThe check phase checks whether the package was built correctly by running its test suite. The default\ncheckPhase calls make check, but only if the doCheck variable is enabled.\n\nVariables controlling the check phasedoCheckControls whether the check phase is executed. By default it is skipped, but\nif doCheck is set to true, the check phase is usually executed. Thus you should set\n\ndoCheck = true;\n\nin the derivation to enable checks. The exception is cross compilation. Cross compiled builds never run tests, no matter\nhow doCheck is set, as the newly-built program won’t run on the platform used to build it.\n\nmakeFlags \/ makeFlagsArray \/ makefileSee the build phase for details.\n\ncheckTargetThe make target that runs the tests. Defaults to check.\n\ncheckFlags \/ checkFlagsArrayA list of strings passed as additional flags to make. Like makeFlags and makeFlagsArray, but\nonly used by the check phase.\n\ncheckInputsA list of dependencies used by the phase. This gets included in nativeBuildInputs when doCheck is set.\n\npreCheckHook executed at the start of the check phase.\n\npostCheckHook executed at the end of the check phase.\n\nThe install phaseThe install phase is responsible for installing the package in the Nix store under out. The default\ninstallPhase creates the directory $out and calls make install.\n\nVariables controlling the install phasedontInstallSet to true to skip the install phase.\n\nmakeFlags \/ makeFlagsArray \/ makefileSee the build phase for details.\n\ninstallTargetsThe make targets that perform the installation. Defaults to install. Example:\n\ninstallTargets = \"install-bin install-doc\";\n\ninstallFlags \/ installFlagsArrayA list of strings passed as additional flags to make. Like makeFlags and makeFlagsArray,\nbut only used by the install phase.\n\npreInstallHook executed at the start of the install phase.\n\npostInstallHook executed at the end of the install phase.\n\nThe fixup phaseThe fixup phase performs some (Nix-specific) post-processing actions on the files installed under $out by\nthe install phase. The default fixupPhase does the following:\n\n  - It moves the man\/, doc\/ and info\/ subdirectories of $out to share\/.\n  - It strips libraries and executables of debug information.\n  - On Linux, it applies the patchelf command to ELF executables and libraries to remove unused directories from the\n    RPATH in order to prevent unnecessary runtime dependencies.\n  - It rewrites the interpreter paths of shell scripts to paths found in PATH. E.g., \/usr\/bin\/perl will be rewritten to\n    \/nix\/store\/some-perl\/bin\/perl found in PATH.\n\nVariables controlling the fixup phasedontFixupSet to true to skip the fixup phase.\n\ndontStripIf set, libraries and executables are not stripped. By default, they are.\n\ndontStripHostLike dontStrip, but only affects the strip command targetting the package’s host platform. Useful when\nsupporting cross compilation, but otherwise feel free to ignore.\n\ndontStripTargetLike dontStrip, but only affects the strip command targetting the packages’ target platform. Useful when\nsupporting cross compilation, but otherwise feel free to ignore.\n\ndontMoveSbinIf set, files in $out\/sbin are not moved to $out\/bin. By default, they are.\n\nstripAllListList of directories to search for libraries and executables from which all symbols should be stripped. By\ndefault, it’s empty. Stripping all symbols is risky, since it may remove not just debug symbols but also ELF information\nnecessary for normal execution.\n\nstripAllFlagsFlags passed to the strip command applied to the files in the directories listed in stripAllList. Defaults\nto -s (i.e. --strip-all).\n\nstripDebugListList of directories to search for libraries and executables from which only debugging-related symbols\nshould be stripped. It defaults to lib lib32 lib64 libexec bin sbin.\n\nstripDebugFlagsFlags passed to the strip command applied to the files in the directories listed in stripDebugList.\nDefaults to -S (i.e. --strip-debug).\n\ndontPatchELFIf set, the patchelf command is not used to remove unnecessary RPATH entries. Only applies to Linux.\n\ndontPatchShebangsIf set, scripts starting with #! do not have their interpreter paths rewritten to paths in the Nix\nstore.\n\ndontPruneLibtoolFilesIf set, libtool .la files associated with shared libraries won’t have their dependency_libs field\ncleared.\n\nforceShareThe list of directories that must be moved from $out to $out\/share. Defaults to man doc info.\n\nsetupHookA package can export a setup hook by setting this variable. The setup hook, if defined, is copied to\n$out\/nix-support\/setup-hook. Environment variables are then substituted in it using substituteAll.\n\npreFixupHook executed at the start of the fixup phase.\n\npostFixupHook executed at the end of the fixup phase.\n\nseparateDebugInfoIf set to true, the standard environment will enable debug information in C\/C++ builds. After\ninstallation, the debug information will be separated from the executables and stored in the output named debug. (This\noutput is enabled automatically; you don’t need to set the outputs attribute explicitly.) To be precise, the debug\ninformation is stored in debug\/lib\/debug\/.build-id\/XX\/YYYY…, where <XXYYYY…> is the <build ID> of the binary — a SHA-1\nhash of the contents of the binary. Debuggers like GDB use the build ID to look up the separated debug information.\n\nFor example, with GDB, you can add\n\nset debug-file-directory ~\/.nix-profile\/lib\/debug\n\nto ~\/.gdbinit. GDB will then be able to find debug information installed via nix-env -i.\n\nThe installCheck phaseThe installCheck phase checks whether the package was installed correctly by running its test\nsuite against the installed directories. The default installCheck calls make installcheck.\n\nVariables controlling the installCheck phasedoInstallCheckControls whether the installCheck phase is executed. By\ndefault it is skipped, but if doInstallCheck is set to true, the installCheck phase is usually executed. Thus you should\nset\n\ndoInstallCheck = true;\n\nin the derivation to enable install checks. The exception is cross compilation. Cross compiled builds never run tests,\nno matter how doInstallCheck is set, as the newly-built program won’t run on the platform used to build it.\n\ninstallCheckTargetThe make target that runs the install tests. Defaults to installcheck.\n\ninstallCheckFlags \/ installCheckFlagsArrayA list of strings passed as additional flags to make. Like makeFlags and\nmakeFlagsArray, but only used by the installCheck phase.\n\ninstallCheckInputsA list of dependencies used by the phase. This gets included in nativeBuildInputs when doInstallCheck\nis set.\n\npreInstallCheckHook executed at the start of the installCheck phase.\n\npostInstallCheckHook executed at the end of the installCheck phase.\n\nThe distribution phaseThe distribution phase is intended to produce a source distribution of the package. The default\ndistPhase first calls make dist, then it copies the resulting source tarballs to $out\/tarballs\/. This phase is only\nexecuted if the attribute doDist is set.\n\nVariables controlling the distribution phasedistTargetThe make target that produces the distribution. Defaults to dist.\n\ndistFlags \/ distFlagsArrayAdditional flags passed to make.\n\ntarballsThe names of the source distribution files to be copied to $out\/tarballs\/. It can contain shell wildcards. The\ndefault is *.tar.gz.\n\ndontCopyDistIf set, no files are copied to $out\/tarballs\/.\n\npreDistHook executed at the start of the distribution phase.\n\npostDistHook executed at the end of the distribution phase.\n\nShell functionsThe standard environment provides a number of useful functions.\n\nmakeWrapper <executable> <wrapperfile> <args>Constructs a wrapper for a program with various possible arguments. For\nexample:\n\n# adds `FOOBAR=baz` to `$out\/bin\/foo`’s environment\nmakeWrapper $out\/bin\/foo $wrapperfile --set FOOBAR baz\n\n# prefixes the binary paths of `hello` and `git`\n# Be advised that paths often should be patched in directly\n# (via string replacements or in `configurePhase`).\nmakeWrapper $out\/bin\/foo $wrapperfile --prefix PATH : ${lib.makeBinPath [ hello git ]}\n\nThere’s many more kinds of arguments, they are documented in nixpkgs\/pkgs\/build-support\/setup-hooks\/make-wrapper.sh.\n\nwrapProgram is a convenience function you probably want to use most of the time.\n\nsubstitute <infile> <outfile> <subs>Performs string substitution on the contents of <infile>, writing the result to\n<outfile>. The substitutions in <subs> are of the following form:\n\n--replace <s1> <s2>\n\nReplace every occurrence of the string <s1> by <s2>.\n\n--subst-var <varName>\n\nReplace every occurrence of @varName@ by the contents of the environment variable <varName>. This is useful for\ngenerating files from templates, using @...@ in the template as placeholders.\n\n--subst-var-by <varName> <s>\n\nReplace every occurrence of @varName@ by the string <s>.\n\nExample:\n\nsubstitute .\/foo.in .\/foo.out \\\n    --replace \/usr\/bin\/bar $bar\/bin\/bar \\\n    --replace \"a string containing spaces\" \"some other text\" \\\n    --subst-var someVar\n\nsubstituteInPlace <file> <subs>Like substitute, but performs the substitutions in place on the file <file>.\n\nsubstituteAll <infile> <outfile>Replaces every occurrence of @varName@, where <varName> is any environment variable, in\n<infile>, writing the result to <outfile>. For instance, if <infile> has the contents\n\n#! @bash@\/bin\/sh\nPATH=@coreutils@\/bin\necho @foo@\n\nand the environment contains bash=\/nix\/store\/bmwp0q28cf21...-bash-3.2-p39 and\ncoreutils=\/nix\/store\/68afga4khv0w...-coreutils-6.12, but does not contain the variable foo, then the output will be\n\n#! \/nix\/store\/bmwp0q28cf21...-bash-3.2-p39\/bin\/sh\nPATH=\/nix\/store\/68afga4khv0w...-coreutils-6.12\/bin\necho @foo@\n\nThat is, no substitution is performed for undefined variables.\n\nEnvironment variables that start with an uppercase letter or an underscore are filtered out, to prevent global variables\n(like HOME) or private variables (like __ETC_PROFILE_DONE) from accidentally getting substituted. The variables also\nhave to be valid bash \"names\", as defined in the bash manpage (alphanumeric or _, must not start with a number).\n\nsubstituteAllInPlace <file>Like substituteAll, but performs the substitutions in place on the file <file>.\n\nstripHash <path>Strips the directory and hash part of a store path, outputting the name part to stdout. For example:\n\n# prints coreutils-8.24\nstripHash \"\/nix\/store\/9s9r019176g7cvn2nvcw41gsp862y6b4-coreutils-8.24\"\n\nIf you wish to store the result in another variable, then the following idiom may be useful:\n\nname=\"\/nix\/store\/9s9r019176g7cvn2nvcw41gsp862y6b4-coreutils-8.24\"\nsomeVar=$(stripHash $name)\n\nwrapProgram <executable> <makeWrapperArgs>Convenience function for makeWrapper that automatically creates a sane wrapper\nfile. It takes all the same arguments as makeWrapper, except for --argv0.\n\nIt cannot be applied multiple times, since it will overwrite the wrapper file.\n\nPackage setup hooksNix itself considers a build-time dependency as merely something that should previously be built and\naccessible at build time—packages themselves are on their own to perform any additional setup. In most cases, that is\nfine, and the downstream derivation can deal with its own dependencies. But for a few common tasks, that would result in\nalmost every package doing the same sort of setup work—depending not on the package itself, but entirely on which\ndependencies were used.\n\nIn order to alleviate this burden, the setup hook mechanism was written, where any package can include a shell script\nthat [by convention rather than enforcement by Nix], any downstream reverse-dependency will source as part of its build\nprocess. That allows the downstream dependency to merely specify its dependencies, and lets those dependencies\neffectively initialize themselves. No boilerplate mirroring the list of dependencies is needed.\n\nThe setup hook mechanism is a bit of a sledgehammer though: a powerful feature with a broad and indiscriminate area of\neffect. The combination of its power and implicit use may be expedient, but isn’t without costs. Nix itself is\nunchanged, but the spirit of added dependencies being effect-free is violated even if the letter isn’t. For example, if\na derivation path is mentioned more than once, Nix itself doesn’t care and simply makes sure the dependency derivation\nis already built just the same—depending is just needing something to exist, and needing is idempotent. However, a\ndependency specified twice will have its setup hook run twice, and that could easily change the build environment\n(though a well-written setup hook will therefore strive to be idempotent so this is in fact not observable). More\nbroadly, setup hooks are anti-modular in that multiple dependencies, whether the same or different, should not interfere\nand yet their setup hooks may well do so.\n\nThe most typical use of the setup hook is actually to add other hooks which are then run (i.e. after all the setup\nhooks) on each dependency. For example, the C compiler wrapper’s setup hook feeds itself flags for each dependency that\ncontains relevant libraries and headers. This is done by defining a bash function, and appending its name to one of\nenvBuildBuildHooks, envBuildHostHooks, envBuildTargetHooks, envHostHostHooks, envHostTargetHooks, or\nenvTargetTargetHooks. These 6 bash variables correspond to the 6 sorts of dependencies by platform (there’s 12 total but\nwe ignore the propagated\/non-propagated axis).\n\nPackages adding a hook should not hard code a specific hook, but rather choose a variable relative to how they are\nincluded. Returning to the C compiler wrapper example, if the wrapper itself is an n dependency, then it only wants to\naccumulate flags from n + 1 dependencies, as only those ones match the compiler’s target platform. The hostOffset\nvariable is defined with the current dependency’s host offset targetOffset with its target offset, before its setup hook\nis sourced. Additionally, since most environment hooks don’t care about the target platform, that means the setup hook\ncan append to the right bash array by doing something like\n\naddEnvHooks \"$hostOffset\" myBashFunction\n\nThe existence of setups hooks has long been documented and packages inside Nixpkgs are free to use this mechanism. Other\npackages, however, should not rely on these mechanisms not changing between Nixpkgs versions. Because of the existing\nissues with this system, there’s little benefit from mandating it be stable for any period of time.\n\nFirst, let’s cover some setup hooks that are part of Nixpkgs default stdenv. This means that they are run for every\npackage built using stdenv.mkDerivation. Some of these are platform specific, so they may run on Linux but not Darwin or\nvice-versa.\n\nmove-docs.shThis setup hook moves any installed documentation to the \/share subdirectory directory. This includes the\nman, doc and info directories. This is needed for legacy programs that do not know how to use the share subdirectory.\n\ncompress-man-pages.shThis setup hook compresses any man pages that have been installed. The compression is done using\nthe gzip program. This helps to reduce the installed size of packages.\n\nstrip.shThis runs the strip command on installed binaries and libraries. This removes unnecessary information like debug\nsymbols when they are not needed. This also helps to reduce the installed size of packages.\n\npatch-shebangs.shThis setup hook patches installed scripts to use the full path to the shebang interpreter. A shebang\ninterpreter is the first commented line of a script telling the operating system which program will run the script (e.g\n#!\/bin\/bash). In Nix, we want an exact path to that interpreter to be used. This often replaces \/bin\/sh with a path in\nthe Nix store.\n\naudit-tmpdir.shThis verifies that no references are left from the install binaries to the directory used to build those\nbinaries. This ensures that the binaries do not need things outside the Nix store. This is currently supported in Linux\nonly.\n\nmultiple-outputs.shThis setup hook adds configure flags that tell packages to install files into any one of the proper\noutputs listed in outputs. This behavior can be turned off by setting setOutputFlags to false in the derivation\nenvironment. See Multiple-output packages for more information.\n\nmove-sbin.shThis setup hook moves any binaries installed in the sbin\/ subdirectory into bin\/. In addition, a link is\nprovided from sbin\/ to bin\/ for compatibility.\n\nmove-lib64.shThis setup hook moves any libraries installed in the lib64\/ subdirectory into lib\/. In addition, a link is\nprovided from lib64\/ to lib\/ for compatibility.\n\nmove-systemd-user-units.shThis setup hook moves any systemd user units installed in the lib\/ subdirectory into share\/.\nIn addition, a link is provided from share\/ to lib\/ for compatibility. This is needed for systemd to find user services\nwhen installed into the user profile.\n\nset-source-date-epoch-to-latest.shThis sets SOURCE_DATE_EPOCH to the modification time of the most recent file.\n\nBintools WrapperThe Bintools Wrapper wraps the binary utilities for a bunch of miscellaneous purposes. These are GNU\nBinutils when targetting Linux, and a mix of cctools and GNU binutils for Darwin. [The “Bintools” name is supposed to be\na compromise between “Binutils” and “cctools” not denoting any specific implementation.] Specifically, the underlying\nbintools package, and a C standard library (glibc or Darwin’s libSystem, just for the dynamic loader) are all fed in,\nand dependency finding, hardening (see below), and purity checks for each are handled by the Bintools Wrapper. Packages\ntypically depend on CC Wrapper, which in turn (at run time) depends on the Bintools Wrapper.\n\nThe Bintools Wrapper was only just recently split off from CC Wrapper, so the division of labor is still being worked\nout. For example, it shouldn’t care about the C standard library, but just take a derivation with the dynamic loader\n(which happens to be the glibc on linux). Dependency finding however is a task both wrappers will continue to need to\nshare, and probably the most important to understand. It is currently accomplished by collecting directories of\nhost-platform dependencies (i.e. buildInputs and nativeBuildInputs) in environment variables. The Bintools Wrapper’s\nsetup hook causes any lib and lib64 subdirectories to be added to NIX_LDFLAGS. Since the CC Wrapper and the Bintools\nWrapper use the same strategy, most of the Bintools Wrapper code is sparsely commented and refers to the CC Wrapper. But\nthe CC Wrapper’s code, by contrast, has quite lengthy comments. The Bintools Wrapper merely cites those, rather than\nrepeating them, to avoid falling out of sync.\n\nA final task of the setup hook is defining a number of standard environment variables to tell build systems which\nexecutables fulfill which purpose. They are defined to just be the base name of the tools, under the assumption that the\nBintools Wrapper’s binaries will be on the path. Firstly, this helps poorly-written packages, e.g. ones that look for\njust gcc when CC isn’t defined yet clang is to be used. Secondly, this helps packages not get confused when\ncross-compiling, in which case multiple Bintools Wrappers may simultaneously be in use.\n[^footnote-stdenv-per-platform-wrapper] BUILD_- and TARGET_-prefixed versions of the normal environment variable are\ndefined for additional Bintools Wrappers, properly disambiguating them.\n\nA problem with this final task is that the Bintools Wrapper is honest and defines LD as ld. Most packages, however,\nfirstly use the C compiler for linking, secondly use LD anyways, defining it as the C compiler, and thirdly, only so\ndefine LD when it is undefined as a fallback. This triple-threat means Bintools Wrapper will break those packages, as LD\nis already defined as the actual linker which the package won’t override yet doesn’t want to use. The workaround is to\ndefine, just for the problematic package, LD as the C compiler. A good way to do this would be preConfigure = \"LD=$CC\".\n\nCC WrapperThe CC Wrapper wraps a C toolchain for a bunch of miscellaneous purposes. Specifically, a C compiler (GCC or\nClang), wrapped binary tools, and a C standard library (glibc or Darwin’s libSystem, just for the dynamic loader) are\nall fed in, and dependency finding, hardening (see below), and purity checks for each are handled by the CC Wrapper.\nPackages typically depend on the CC Wrapper, which in turn (at run-time) depends on the Bintools Wrapper.\n\nDependency finding is undoubtedly the main task of the CC Wrapper. This works just like the Bintools Wrapper, except\nthat any include subdirectory of any relevant dependency is added to NIX_CFLAGS_COMPILE. The setup hook itself contains\nsome lengthy comments describing the exact convoluted mechanism by which this is accomplished.\n\nSimilarly, the CC Wrapper follows the Bintools Wrapper in defining standard environment variables with the names of the\ntools it wraps, for the same reasons described above. Importantly, while it includes a cc symlink to the c compiler for\nportability, the CC will be defined using the compiler’s “real name” (i.e. gcc or clang). This helps lousy build systems\nthat inspect on the name of the compiler rather than run it.\n\nHere are some more packages that provide a setup hook. Since the list of hooks is extensible, this is not an exhaustive\nlist. The mechanism is only to be used as a last resort, so it might cover most uses.\n\nPerlAdds the lib\/site_perl subdirectory of each build input to the PERL5LIB environment variable. For instance, if\nbuildInputs contains Perl, then the lib\/site_perl subdirectory of each input is added to the PERL5LIB environment\nvariable.\n\nPythonAdds the lib\/${python.libPrefix}\/site-packages subdirectory of each build input to the PYTHONPATH environment\nvariable.\n\npkg-configAdds the lib\/pkgconfig and share\/pkgconfig subdirectories of each build input to the PKG_CONFIG_PATH\nenvironment variable.\n\nAutomakeAdds the share\/aclocal subdirectory of each build input to the ACLOCAL_PATH environment variable.\n\nAutoconfThe autoreconfHook derivation adds autoreconfPhase, which runs autoreconf, libtoolize and automake, essentially\npreparing the configure script in autotools-based builds. Most autotools-based packages come with the configure script\npre-generated, but this hook is necessary for a few packages and when you need to patch the package’s configure scripts.\n\nlibxml2Adds every file named catalog.xml found under the xml\/dtd and xml\/xsl subdirectories of each build input to the\nXML_CATALOG_FILES environment variable.\n\nteTeX \/ TeX LiveAdds the share\/texmf-nix subdirectory of each build input to the TEXINPUTS environment variable.\n\nQt 4Sets the QTDIR environment variable to Qt’s path.\n\ngdk-pixbufExports GDK_PIXBUF_MODULE_FILE environment variable to the builder. Add librsvg package to buildInputs to get\nsvg support. See also the setup hook description in GNOME platform docs.\n\nGHCCreates a temporary package database and registers every Haskell build input in it (TODO: how?).\n\nGNOME platformHooks related to GNOME platform and related libraries like GLib, GTK and GStreamer are described in GNOME.\n\nautoPatchelfHookThis is a special setup hook which helps in packaging proprietary software in that it automatically\ntries to find missing shared library dependencies of ELF files based on the given buildInputs and nativeBuildInputs.\n\nYou can also specify a runtimeDependencies variable which lists dependencies to be unconditionally added to rpath of all\nexecutables. This is useful for programs that use dlopen 3 to load libraries at runtime.\n\nIn certain situations you may want to run the main command (autoPatchelf) of the setup hook on a file or a set of\ndirectories instead of unconditionally patching all outputs. This can be done by setting the dontAutoPatchelf\nenvironment variable to a non-empty value.\n\nBy default autoPatchelf will fail as soon as any ELF file requires a dependency which cannot be resolved via the given\nbuild inputs. In some situations you might prefer to just leave missing dependencies unpatched and continue to patch the\nrest. This can be achieved by setting the autoPatchelfIgnoreMissingDeps environment variable to a non-empty value.\n\nThe autoPatchelf command also recognizes a --no-recurse command line flag, which prevents it from recursing into\nsubdirectories.\n\nbreakpointHookThis hook will make a build pause instead of stopping when a failure happens. It prevents nix from\ncleaning up the build environment immediately and allows the user to attach to a build environment using the cntr\ncommand. Upon build error it will print instructions on how to use cntr, which can be used to enter the environment for\ndebugging. Installing cntr and running the command will provide shell access to the build sandbox of failed build. At\n\/var\/lib\/cntr the sandboxed filesystem is mounted. All commands and files of the system are still accessible within the\nshell. To execute commands from the sandbox use the cntr exec subcommand. cntr is only supported on Linux-based\nplatforms. To use it first add cntr to your environment.systemPackages on NixOS or alternatively to the root user on\nnon-NixOS systems. Then in the package that is supposed to be inspected, add breakpointHook to nativeBuildInputs.\n\nnativeBuildInputs = [ breakpointHook ];\n\nWhen a build failure happens there will be an instruction printed that shows how to attach with cntr to the build\nsandbox.\n\nCaution with remote builds \n\nThis won’t work with remote builds as the build environment is on a different machine and can’t be accessed by cntr.\nRemote builds can be turned off by setting --option builders '' for nix-build or --builders '' for nix build. \n\ninstallShellFilesThis hook helps with installing manpages and shell completion files. It exposes 2 shell functions\ninstallManPage and installShellCompletion that can be used from your postInstall hook.\n\nThe installManPage function takes one or more paths to manpages to install. The manpages must have a section suffix, and\nmay optionally be compressed (with .gz suffix). This function will place them into the correct directory.\n\nThe installShellCompletion function takes one or more paths to shell completion files. By default it will autodetect the\nshell type from the completion file extension, but you may also specify it by passing one of --bash, --fish, or --zsh.\nThese flags apply to all paths listed after them (up until another shell flag is given). Each path may also have a\ncustom installation name provided by providing a flag --name NAME before the path. If this flag is not provided, zsh\ncompletions will be renamed automatically such that foobar.zsh becomes _foobar. A root name may be provided for all\npaths using the flag --cmd NAME; this synthesizes the appropriate name depending on the shell (e.g. --cmd foo will\nsynthesize the name foo.bash for bash and _foo for zsh). The path may also be a fifo or named fd (such as produced by\n<(cmd)), in which case the shell and name must be provided.\n\nnativeBuildInputs = [ installShellFiles ];\npostInstall = ''\n  installManPage doc\/foobar.1 doc\/barfoo.3\n  # explicit behavior\n  installShellCompletion --bash --name foobar.bash share\/completions.bash\n  installShellCompletion --fish --name foobar.fish share\/completions.fish\n  installShellCompletion --zsh --name _foobar share\/completions.zsh\n  # implicit behavior\n  installShellCompletion share\/completions\/foobar.{bash,fish,zsh}\n  # using named fd\n  installShellCompletion --cmd foobar \\\n    --bash <($out\/bin\/foobar --bash-completion) \\\n    --fish <($out\/bin\/foobar --fish-completion) \\\n    --zsh <($out\/bin\/foobar --zsh-completion)\n'';\n\nlibiconv, libintlA few libraries automatically add to NIX_LDFLAGS their library, making their symbols automatically\navailable to the linker. This includes libiconv and libintl (gettext). This is done to provide compatibility between GNU\nLinux, where libiconv and libintl are bundled in, and other systems where that might not be the case. Sometimes, this\nbehavior is not desired. To disable this behavior, set dontAddExtraLibs.\n\nvalidatePkgConfigThe validatePkgConfig hook validates all pkg-config (.pc) files in a package. This helps catching some\ncommon errors in pkg-config files, such as undefined variables.\n\ncmakeOverrides the default configure phase to run the CMake command. By default, we use the Make generator of CMake. In\naddition, dependencies are added automatically to CMAKE_PREFIX_PATH so that packages are correctly detected by CMake.\nSome additional flags are passed in to give similar behavior to configure-based packages. You can disable this hook’s\nbehavior by setting configurePhase to a custom value, or by setting dontUseCmakeConfigure. cmakeFlags controls flags\npassed only to CMake. By default, parallel building is enabled as CMake supports parallel building almost everywhere.\nWhen Ninja is also in use, CMake will detect that and use the ninja generator.\n\nxcbuildHookOverrides the build and install phases to run the \"xcbuild\" command. This hook is needed when a project only\ncomes with build files for the XCode build system. You can disable this behavior by setting buildPhase and\nconfigurePhase to a custom value. xcbuildFlags controls flags passed only to xcbuild.\n\nMesonOverrides the configure phase to run meson to generate Ninja files. To run these files, you should accompany Meson\nwith ninja. By default, enableParallelBuilding is enabled as Meson supports parallel building almost everywhere.\n\nVariables controlling MesonmesonFlagsControls the flags passed to meson.\n\nmesonBuildTypeWhich --buildtype to pass to Meson. We default to plain.\n\nmesonAutoFeaturesWhat value to set -Dauto_features= to. We default to enabled.\n\nmesonWrapModeWhat value to set -Dwrap_mode= to. We default to nodownload as we disallow network access.\n\ndontUseMesonConfigureDisables using Meson’s configurePhase.\n\nninjaOverrides the build, install, and check phase to run ninja instead of make. You can disable this behavior with the\ndontUseNinjaBuild, dontUseNinjaInstall, and dontUseNinjaCheck, respectively. Parallel building is enabled by default in\nNinja.\n\nunzipThis setup hook will allow you to unzip .zip files specified in $src. There are many similar packages like unrar,\nundmg, etc.\n\nwafHookOverrides the configure, build, and install phases. This will run the “waf” script used by many projects. If\nwafPath (default .\/waf) doesn’t exist, it will copy the version of waf available in Nixpkgs. wafFlags can be used to\npass flags to the waf script.\n\nsconsOverrides the build, install, and check phases. This uses the scons build system as a replacement for make. scons\ndoes not provide a configure phase, so everything is managed at build and install time.\n\nPurity in NixpkgsMeasures taken to prevent dependencies on packages outside the store, and what you can do to prevent\nthem.\n\nGCC doesn’t search in locations such as \/usr\/include. In fact, attempts to add such directories through the -I flag are\nfiltered out. Likewise, the linker (from GNU binutils) doesn’t search in standard locations such as \/usr\/lib. Programs\nbuilt on Linux are linked against a GNU C Library that likewise doesn’t search in the default system locations.\n\nHardening in NixpkgsThere are flags available to harden packages at compile or link-time. These can be toggled using the\nstdenv.mkDerivation parameters hardeningDisable and hardeningEnable.\n\nBoth parameters take a list of flags as strings. The special \"all\" flag can be passed to hardeningDisable to turn off\nall hardening. These flags can also be used as environment variables for testing or development purposes.\n\nThe following flags are enabled by default and might require disabling with hardeningDisable if the program to package\nis incompatible.\n\nformatAdds the -Wformat -Wformat-security -Werror=format-security compiler options. At present, this warns about calls\nto printf and scanf functions where the format string is not a string literal and there are no format arguments, as in\nprintf(foo);. This may be a security hole if the format string came from untrusted input and contains %n.\n\nThis needs to be turned off or fixed for errors similar to:\n\n\/tmp\/nix-build-zynaddsubfx-2.5.2.drv-0\/zynaddsubfx-2.5.2\/src\/UI\/guimain.cpp:571:28: error: format not a string literal and no format arguments [-Werror=format-security]\n         printf(help_message);\n                            ^\ncc1plus: some warnings being treated as errors\n\nstackprotectorAdds the -fstack-protector-strong --param ssp-buffer-size=4 compiler options. This adds safety checks\nagainst stack overwrites rendering many potential code injection attacks into aborting situations. In the best case this\nturns code injection vulnerabilities into denial of service or into non-issues (depending on the application).\n\nThis needs to be turned off or fixed for errors similar to:\n\nbin\/blib.a(bios_console.o): In function `bios_handle_cup':\n\/tmp\/nix-build-ipxe-20141124-5cbdc41.drv-0\/ipxe-5cbdc41\/src\/arch\/i386\/firmware\/pcbios\/bios_console.c:86: undefined reference to `__stack_chk_fail'\n\nfortifyAdds the -O2 -D_FORTIFY_SOURCE=2 compiler options. During code generation the compiler knows a great deal of\ninformation about buffer sizes (where possible), and attempts to replace insecure unlimited length buffer function calls\nwith length-limited ones. This is especially useful for old, crufty code. Additionally, format strings in writable\nmemory that contain %n are blocked. If an application depends on such a format string, it will need to be worked around.\n\nAdditionally, some warnings are enabled which might trigger build failures if compiler warnings are treated as errors in\nthe package build. In this case, set NIX_CFLAGS_COMPILE to -Wno-error=warning-type.\n\nThis needs to be turned off or fixed for errors similar to:\n\nmalloc.c:404:15: error: return type is an incomplete type\nmalloc.c:410:19: error: storage size of 'ms' isn't known\n\nstrdup.h:22:1: error: expected identifier or '(' before '__extension__'\n\nstrsep.c:65:23: error: register name not specified for 'delim'\n\ninstallwatch.c:3751:5: error: conflicting types for '__open_2'\n\nfcntl2.h:50:4: error: call to '__open_missing_mode' declared with attribute error: open with O_CREAT or O_TMPFILE in second argument needs 3 arguments\n\npicAdds the -fPIC compiler options. This options adds support for position independent code in shared libraries and thus\nmaking ASLR possible.\n\nMost notably, the Linux kernel, kernel modules and other code not running in an operating system environment like boot\nloaders won’t build with PIC enabled. The compiler will is most cases complain that PIC is not supported for a specific\nbuild.\n\nThis needs to be turned off or fixed for assembler errors similar to:\n\nccbLfRgg.s: Assembler messages:\nccbLfRgg.s:33: Error: missing or invalid displacement expression `private_key_len@GOTOFF'\n\nstrictoverflowSigned integer overflow is undefined behaviour according to the C standard. If it happens, it is an error\nin the program as it should check for overflow before it can happen, not afterwards. GCC provides built-in functions to\nperform arithmetic with overflow checking, which are correct and faster than any custom implementation. As a workaround,\nthe option -fno-strict-overflow makes gcc behave as if signed integer overflows were defined.\n\nThis flag should not trigger any build or runtime errors.\n\nrelroAdds the -z relro linker option. During program load, several ELF memory sections need to be written to by the\nlinker, but can be turned read-only before turning over control to the program. This prevents some GOT (and .dtors)\noverwrite attacks, but at least the part of the GOT used by the dynamic linker (.got.plt) is still vulnerable.\n\nThis flag can break dynamic shared object loading. For instance, the module systems of Xorg and OpenCV are incompatible\nwith this flag. In almost all cases the bindnow flag must also be disabled and incompatible programs typically fail with\nsimilar errors at runtime.\n\nbindnowAdds the -z bindnow linker option. During program load, all dynamic symbols are resolved, allowing for the\ncomplete GOT to be marked read-only (due to relro). This prevents GOT overwrite attacks. For very large applications,\nthis can incur some performance loss during initial load while symbols are resolved, but this shouldn’t be an issue for\ndaemons.\n\nThis flag can break dynamic shared object loading. For instance, the module systems of Xorg and PHP are incompatible\nwith this flag. Programs incompatible with this flag often fail at runtime due to missing symbols, like:\n\nintel_drv.so: undefined symbol: vgaHWFreeHWRec\n\nThe following flags are disabled by default and should be enabled with hardeningEnable for packages that take untrusted\ninput like network services.\n\npieAdds the -fPIE compiler and -pie linker options. Position Independent Executables are needed to take advantage of\nAddress Space Layout Randomization, supported by modern kernel versions. While ASLR can already be enforced for data\nareas in the stack and heap (brk and mmap), the code areas must be compiled as position-independent. Shared libraries\nalready do this with the pic flag, so they gain ASLR automatically, but binary .text regions need to be build with pie\nto gain ASLR. When this happens, ROP attacks are much harder since there are no static locations to bounce off of during\na memory corruption attack.\n\nFor more in-depth information on these hardening flags and hardening in general, refer to the Debian Wiki, Ubuntu Wiki,\nGentoo Wiki, and the Arch Wiki.\n\n[^footnote-stdenv-ignored-build-platform]: The build platform is ignored because it is a mere implementation detail of\nthe package satisfying the dependency: As a general programming principle, dependencies are always specified as\ninterfaces, not concrete implementation. [^footnote-stdenv-native-dependencies-in-path]: Currently, this means for\nnative builds all dependencies are put on the PATH. But in the future that may not be the case for sake of matching\ncross: the platforms would be assumed to be unique for native and cross builds alike, so only the depsBuild* and\nnativeBuildInputs would be added to the PATH. [^footnote-stdenv-find-inputs-location]: The findInputs function,\ncurrently residing in pkgs\/stdenv\/generic\/setup.sh, implements the propagation logic.\n[^footnote-stdenv-sys-lib-search-path]: It clears the sys_lib_*search_path variables in the Libtool script to prevent\nLibtool from using libraries in \/usr\/lib and such. [^footnote-stdenv-build-time-guessing-impurity]: Eventually these\nwill be passed building natively as well, to improve determinism: build-time guessing, as is done today, is a risk of\nimpurity. [^footnote-stdenv-per-platform-wrapper]: Each wrapper targets a single platform, so if binaries for multiple\nplatforms are needed, the underlying binaries must be wrapped multiple times. As this is a property of the wrapper\nitself, the multiple wrappings are needed whether or not the same underlying binaries can target multiple platforms.\n" }
,{ "url": "stdenv\/meta\/", "title": "Meta-attributes", "text": "Meta-attributesNix packages can declare meta-attributes that contain information about a package such as a description,\nits homepage, its license, and so on. For instance, the GNU Hello package has a meta declaration like this:\n\nmeta = with lib; {\n  description = \"A program that produces a familiar, friendly greeting\";\n  longDescription = ''\n    GNU Hello is a program that prints \"Hello, world!\" when you run it.\n    It is fully customizable.\n  '';\n  homepage = \"https:\/\/www.gnu.org\/software\/hello\/manual\/\";\n  license = licenses.gpl3Plus;\n  maintainers = [ maintainers.eelco ];\n  platforms = platforms.all;\n};\n\nMeta-attributes are not passed to the builder of the package. Thus, a change to a meta-attribute doesn’t trigger a\nrecompilation of the package. The value of a meta-attribute must be a string.\n\nThe meta-attributes of a package can be queried from the command-line using nix-env:\n\n$ nix-env -qa hello --json\n{\n    \"hello\": {\n        \"meta\": {\n            \"description\": \"A program that produces a familiar, friendly greeting\",\n            \"homepage\": \"https:\/\/www.gnu.org\/software\/hello\/manual\/\",\n            \"license\": {\n                \"fullName\": \"GNU General Public License version 3 or later\",\n                \"shortName\": \"GPLv3+\",\n                \"url\": \"http:\/\/www.fsf.org\/licensing\/licenses\/gpl.html\"\n            },\n            \"longDescription\": \"GNU Hello is a program that prints \\\"Hello, world!\\\" when you run it.\\nIt is fully customizable.\\n\",\n            \"maintainers\": [\n                \"Ludovic Court\\u00e8s <ludo@gnu.org>\"\n            ],\n            \"platforms\": [\n                \"i686-linux\",\n                \"x86_64-linux\",\n                \"armv5tel-linux\",\n                \"armv7l-linux\",\n                \"mips32-linux\",\n                \"x86_64-darwin\",\n                \"i686-cygwin\",\n                \"i686-freebsd\",\n                \"x86_64-freebsd\",\n                \"i686-openbsd\",\n                \"x86_64-openbsd\"\n            ],\n            \"position\": \"\/home\/user\/dev\/nixpkgs\/pkgs\/applications\/misc\/hello\/default.nix:14\"\n        },\n        \"name\": \"hello-2.9\",\n        \"system\": \"x86_64-linux\"\n    }\n}\n\nnix-env knows about the description field specifically:\n\n$ nix-env -qa hello --description\nhello-2.3  A program that produces a familiar, friendly greeting\n\nStandard meta-attributesIt is expected that each meta-attribute is one of the following:\n\ndescriptionA short (one-line) description of the package. This is shown by nix-env -q --description and also on the\nNixpkgs release pages.\n\nDon’t include a period at the end. Don’t include newline characters. Capitalise the first character. For brevity, don’t\nrepeat the name of package --- just describe what it does.\n\nWrong: \"libpng is a library that allows you to decode PNG images.\"\n\nRight: \"A library for decoding PNG images\"\n\nlongDescriptionAn arbitrarily long description of the package.\n\nbranchRelease branch. Used to specify that a package is not going to receive updates that are not in this branch; for\nexample, Linux kernel 3.0 is supposed to be updated to 3.0.X, not 3.1.\n\nhomepageThe package’s homepage. Example: https:\/\/www.gnu.org\/software\/hello\/manual\/\n\ndownloadPageThe page where a link to the current version can be found. Example: https:\/\/ftp.gnu.org\/gnu\/hello\/\n\nchangelogA link or a list of links to the location of Changelog for a package. A link may use expansion to refer to the\ncorrect changelog version. Example: \"https:\/\/git.savannah.gnu.org\/cgit\/hello.git\/plain\/NEWS?h=v${version}\"\n\nlicenseThe license, or licenses, for the package. One from the attribute set defined in nixpkgs\/lib\/licenses.nix. At\nthis moment using both a list of licenses and a single license is valid. If the license field is in the form of a list\nrepresentation, then it means that parts of the package are licensed differently. Each license should preferably be\nreferenced by their attribute. The non-list attribute value can also be a space delimited string representation of the\ncontained attribute shortNames or spdxIds. The following are all valid examples:\n\n  - Single license referenced by attribute (preferred) lib.licenses.gpl3Only.\n  - Single license referenced by its attribute shortName (frowned upon) \"gpl3Only\".\n  - Single license referenced by its attribute spdxId (frowned upon) \"GPL-3.0-only\".\n  - Multiple licenses referenced by attribute (preferred) with lib.licenses; [ asl20 free ofl ].\n  - Multiple licenses referenced as a space delimited string of attribute shortNames (frowned upon) \"asl20 free ofl\".\n\nFor details, see Licenses.\n\nmaintainersA list of the maintainers of this Nix expression. Maintainers are defined in\nnixpkgs\/maintainers\/maintainer-list.nix. There is no restriction to becoming a maintainer, just add yourself to that\nlist in a separate commit titled “maintainers: add alice”, and reference maintainers with maintainers = with\nlib.maintainers; [ alice bob ].\n\npriorityThe priority of the package, used by nix-env to resolve file name conflicts between packages. See the Nix manual\npage for nix-env for details. Example: \"10\" (a low-priority package).\n\nplatformsThe list of Nix platform types on which the package is supported. Hydra builds packages according to the\nplatform specified. If no platform is specified, the package does not have prebuilt binaries. An example is:\n\nmeta.platforms = lib.platforms.linux;\n\nAttribute Set lib.platforms defines various common lists of platforms types.\n\ntests This attribute is special in that it is not actually under the meta attribute set but rather under the passthru\nattribute set. This is due to how meta attributes work, and the fact that they are supposed to contain only metadata,\nnot derivations. \n\nAn attribute set with as values tests. A test is a derivation, which builds successfully when the test passes, and fails\nto build otherwise. A derivation that is a test needs to have meta.timeout defined.\n\nThe NixOS tests are available as nixosTests in parameters of derivations. For instance, the OpenSMTPD derivation\nincludes lines similar to:\n\n{ \/* ... *\/, nixosTests }:\n{\n  # ...\n  passthru.tests = {\n    basic-functionality-and-dovecot-integration = nixosTests.opensmtpd;\n  };\n}\n\ntimeoutA timeout (in seconds) for building the derivation. If the derivation takes longer than this time to build, it\ncan fail due to breaking the timeout. However, all computers do not have the same computing power, hence some builders\nmay decide to apply a multiplicative factor to this value. When filling this value in, try to keep it approximately\nconsistent with other values already present in nixpkgs.\n\nhydraPlatformsThe list of Nix platform types for which the Hydra instance at hydra.nixos.org will build the package.\n(Hydra is the Nix-based continuous build system.) It defaults to the value of meta.platforms. Thus, the only reason to\nset meta.hydraPlatforms is if you want hydra.nixos.org to build the package on a subset of meta.platforms, or not at\nall, e.g.\n\nmeta.platforms = lib.platforms.linux;\nmeta.hydraPlatforms = [];\n\nbrokenIf set to true, the package is marked as \"broken\", meaning that it won’t show up in nix-env -qa, and cannot be\nbuilt or installed. Such packages should be removed from Nixpkgs eventually unless they are fixed.\n\nupdateWalkerIf set to true, the package is tested to be updated correctly by the update-walker.sh script without\nadditional settings. Such packages have meta.version set and their homepage (or the page specified by meta.downloadPage)\ncontains a direct link to the package tarball.\n\nLicensesThe meta.license attribute should preferably contain a value from lib.licenses defined in\nnixpkgs\/lib\/licenses.nix, or in-place license description of the same format if the license is unlikely to be useful in\nanother expression.\n\nAlthough it’s typically better to indicate the specific license, a few generic options are available:\n\nlib.licenses.free, \"free\"Catch-all for free software licenses not listed above.\n\nlib.licenses.unfreeRedistributable, \"unfree-redistributable\"Unfree package that can be redistributed in binary form.\nThat is, it’s legal to redistribute the output of the derivation. This means that the package can be included in the\nNixpkgs channel.\n\nSometimes proprietary software can only be redistributed unmodified. Make sure the builder doesn’t actually modify the\noriginal binaries; otherwise we’re breaking the license. For instance, the NVIDIA X11 drivers can be redistributed\nunmodified, but our builder applies patchelf to make them work. Thus, its license is \"unfree\" and it cannot be included\nin the Nixpkgs channel.\n\nlib.licenses.unfree, \"unfree\"Unfree package that cannot be redistributed. You can build it yourself, but you cannot\nredistribute the output of the derivation. Thus it cannot be included in the Nixpkgs channel.\n\nlib.licenses.unfreeRedistributableFirmware, \"unfree-redistributable-firmware\"This package supplies unfree,\nredistributable firmware. This is a separate value from unfree-redistributable because not everybody cares whether\nfirmware is free.\n" }
,{ "url": "stdenv\/cross-compilation\/", "title": "Cross-compilation", "text": "Cross-compilationIntroduction\"Cross-compilation\" means compiling a program on one machine for another type of machine.\nFor example, a typical use of cross-compilation is to compile programs for embedded devices. These devices often don't\nhave the computing power and memory to compile their own programs. One might think that cross-compilation is a fairly\nniche concern. However, there are significant advantages to rigorously distinguishing between build-time and run-time\nenvironments! Significant, because the benefits apply even when one is developing and deploying on the same machine.\nNixpkgs is increasingly adopting the opinion that packages should be written with cross-compilation in mind, and Nixpkgs\nshould evaluate in a similar way (by minimizing cross-compilation-specific special cases) whether or not one is\ncross-compiling.\n\nThis chapter will be organized in three parts. First, it will describe the basics of how to package software in a way\nthat supports cross-compilation. Second, it will describe how to use Nixpkgs when cross-compiling. Third, it will\ndescribe the internal infrastructure supporting cross-compilation.\n\nPackaging in a cross-friendly mannerPlatform parametersNixpkgs follows the conventions of GNU autoconf. We distinguish\nbetween 3 types of platforms when building a derivation: build, host, and target. In summary, build is the platform on\nwhich a package is being built, host is the platform on which it will run. The third attribute, target, is relevant only\nfor certain specific compilers and build tools.\n\nIn Nixpkgs, these three platforms are defined as attribute sets under the names buildPlatform, hostPlatform, and\ntargetPlatform. They are always defined as attributes in the standard environment. That means one can access them like:\n\n{ stdenv, fooDep, barDep, ... }: ...stdenv.buildPlatform...\n\nbuildPlatform\n\nThe \"build platform\" is the platform on which a package is built. Once someone has a built package, or pre-built binary\npackage, the build platform should not matter and can be ignored.\n\nhostPlatform\n\nThe \"host platform\" is the platform on which a package will be run. This is the simplest platform to understand, but\nalso the one with the worst name.\n\ntargetPlatform\n\nThe \"target platform\" attribute is, unlike the other two attributes, not actually fundamental to the process of building\nsoftware. Instead, it is only relevant for compatibility with building certain specific compilers and build tools. It\ncan be safely ignored for all other packages.\n\nThe build process of certain compilers is written in such a way that the compiler resulting from a single build can\nitself only produce binaries for a single platform. The task of specifying this single \"target platform\" is thus pushed\nto build time of the compiler. The root cause of this is that the compiler (which will be run on the host) and the\nstandard library\/runtime (which will be run on the target) are built by a single build process.\n\nThere is no fundamental need to think about a single target ahead of time like this. If the tool supports modular or\npluggable backends, both the need to specify the target at build time and the constraint of having only a single target\ndisappear. An example of such a tool is LLVM.\n\nAlthough the existence of a \"target platform\" is arguably a historical mistake, it is a common one: examples of tools\nthat suffer from it are GCC, Binutils, GHC and Autoconf. Nixpkgs tries to avoid sharing in the mistake where possible.\nStill, because the concept of a target platform is so ingrained, it is best to support it as is.\n\nThe exact schema these fields follow is a bit ill-defined due to a long and convoluted evolution, but this is slowly\nbeing cleaned up. You can see examples of ones used in practice in lib.systems.examples; note how they are not all very\nconsistent. For now, here are few fields can count on them containing:\n\nsystem\n\nThis is a two-component shorthand for the platform. Examples of this would be \"x86_64-darwin\" and \"i686-linux\"; see\nlib.systems.doubles for more. The first component corresponds to the CPU architecture of the platform and the second to\nthe operating system of the platform ([cpu]-[os]). This format has built-in support in Nix, such as the\nbuiltins.currentSystem impure string.\n\nconfig\n\nThis is a 3- or 4- component shorthand for the platform. Examples of this would be x86_64-unknown-linux-gnu and\naarch64-apple-darwin14. This is a standard format called the \"LLVM target triple\", as they are pioneered by LLVM. In\nthe 4-part form, this corresponds to [cpu]-[vendor]-[os]-[abi]. This format is strictly more informative than the \"Nix\nhost double\", as the previous format could analogously be termed. This needs a better name than config!\n\nparsed\n\nThis is a Nix representation of a parsed LLVM target triple with white-listed components. This can be specified\ndirectly, or actually parsed from the config. See lib.systems.parse for the exact representation.\n\nlibc\n\nThis is a string identifying the standard C library used. Valid identifiers include \"glibc\" for GNU libc, \"libSystem\"\nfor Darwin's Libsystem, and \"uclibc\" for µClibc. It should probably be refactored to use the module system, like parse.\n\nis*\n\nThese predicates are defined in lib.systems.inspect, and slapped onto every platform. They are superior to the ones in\nstdenv as they force the user to be explicit about which platform they are inspecting. Please use these instead of\nthose.\n\nplatform\n\nThis is, quite frankly, a dumping ground of ad-hoc settings (it's an attribute set). See lib.systems.platforms for\nexamples—there's hopefully one in there that will work verbatim for each platform that is working. Please help us triage\nthese flags and give them better homes!\n\nTheory of dependency categorization This is a rather philosophical description that isn't very Nixpkgs-specific. For an\noverview of all the relevant attributes given to mkDerivation, see The Standard Environment. For a description of how\neverything is implemented, see Cross-compilation. \n\nIn this section we explore the relationship between both runtime and build-time dependencies and the 3 Autoconf\nplatforms.\n\nA run time dependency between two packages requires that their host platforms match. This is directly implied by the\nmeaning of \"host platform\" and \"runtime dependency\": The package dependency exists while both packages are running on a\nsingle host platform.\n\nA build time dependency, however, has a shift in platforms between the depending package and the depended-on package.\n\"build time dependency\" means that to build the depending package we need to be able to run the depended-on's package.\nThe depending package's build platform is therefore equal to the depended-on package's host platform.\n\nIf both the dependency and depending packages aren't compilers or other machine-code-producing tools, we're done. And\nindeed buildInputs and nativeBuildInputs have covered these simpler cases for many years. But if the dependency does\nproduce machine code, we might need to worry about its target platform too. In principle, that target platform might be\nany of the depending package's build, host, or target platforms, but we prohibit dependencies from a \"later\" platform to\nan earlier platform to limit confusion because we've never seen a legitimate use for them.\n\nFinally, if the depending package is a compiler or other machine-code-producing tool, it might need dependencies that\nrun at \"emit time\". This is for compilers that (regrettably) insist on being built together with their source languages'\nstandard libraries. Assuming build != host != target, a run-time dependency of the standard library cannot be run at the\ncompiler's build time or run time, but only at the run time of code emitted by the compiler.\n\nPutting this all together, that means we have dependencies in the form \"host → target\", in at most the following six\ncombinations:\n\nPossible dependency types\n\n| Dependency’s host platform | Dependency’s target platform |\n| --- | --- |\n| build | build |\n| build | host |\n| build | target |\n| host | host |\n| host | target |\n| target | target |\n\nSome examples will make this table clearer. Suppose there's some package that is being built with a (build, host,\ntarget) platform triple of (foo, bar, baz). If it has a build-time library dependency, that would be a \"host → build\"\ndependency with a triple of (foo, foo, *) (the target platform is irrelevant). If it needs a compiler to be built, that\nwould be a \"build → host\" dependency with a triple of (foo, foo, *) (the target platform is irrelevant). That compiler,\nwould be built with another compiler, also \"build → host\" dependency, with a triple of (foo, foo, foo).\n\nCross packaging cookbookSome frequently encountered problems when packaging for cross-compilation should be answered\nhere. Ideally, the information above is exhaustive, so this section cannot provide any new information, but it is\nludicrous and cruel to expect everyone to spend effort working through the interaction of many features just to figure\nout the same answer to the same common problem. Feel free to add to this list!\n\nMy package fails to find a binutils command (cc\/ar\/ld etc.)Many packages assume that an unprefixed binutils (cc\/ar\/ld\netc.) is available, but Nix doesn't provide one. It only provides a prefixed one, just as it only does for all the other\nbinutils programs. It may be necessary to patch the package to fix the build system to use a prefix. For instance,\ninstead of cc, use ${stdenv.cc.targetPrefix}cc.\n\nmakeFlags = [ \"CC=${stdenv.cc.targetPrefix}cc\" ];\n\nHow do I avoid compiling a GCC cross-compiler from source?On less powerful machines, it can be inconvenient to\ncross-compile a package only to find out that GCC has to be compiled from source, which could take up to several hours.\nNixpkgs maintains a limited cross-related jobset on Hydra, which tests cross-compilation to various platforms from build\nplatforms \"x86_64-darwin\", \"x86_64-linux\", and \"aarch64-linux\". See pkgs\/top-level\/release-cross.nix for the full list\nof target platforms and packages. For instance, the following invocation fetches the pre-built cross-compiled GCC for\narmv6l-unknown-linux-gnueabihf and builds GNU Hello from source.\n\n$ nix-build '<nixpkgs>' -A pkgsCross.raspberryPi.hello\n\nWhat if my package’s build system needs to build a C program to be run under the build environment?Add the following to\nyour mkDerivation invocation.\n\ndepsBuildBuild = [ buildPackages.stdenv.cc ];\n\nMy package’s testsuite needs to run host platform code.Add the following to your mkDerivation invocation.\n\ndoCheck = stdenv.hostPlatform == stdenv.buildPlatform;\n\nCross-building packagesNixpkgs can be instantiated with localSystem alone, in which case there is no cross-compiling and\neverything is built by and for that system, or also with crossSystem, in which case packages run on the latter, but all\nbuilding happens on the former. Both parameters take the same schema as the 3 (build, host, and target) platforms\ndefined in the previous section. As mentioned above, lib.systems.examples has some platforms which are used as arguments\nfor these parameters in practice. You can use them programmatically, or on the command line:\n\n$ nix-build '<nixpkgs>' --arg crossSystem '(import <nixpkgs\/lib>).systems.examples.fooBarBaz' -A whatever\n\nEventually we would like to make these platform examples an unnecessary convenience so that\n\n$ nix-build '<nixpkgs>' --arg crossSystem '{ config = \"<arch>-<os>-<vendor>-<abi>\"; }' -A whatever\n\nworks in the vast majority of cases. The problem today is dependencies on other sorts of configuration which aren't\ngiven proper defaults. We rely on the examples to crudely to set those configuration parameters in some vaguely sane\nmanner on the users behalf. Issue #34274 tracks this inconvenience along with its root cause in crufty configuration\noptions. \n\nWhile one is free to pass both parameters in full, there's a lot of logic to fill in missing fields. As discussed in the\nprevious section, only one of system, config, and parsed is needed to infer the other two. Additionally, libc will be\ninferred from parse. Finally, localSystem.system is also impurely inferred based on the platform evaluation occurs. This\nmeans it is often not necessary to pass localSystem at all, as in the command-line example in the previous paragraph.\n\nMany sources (manual, wiki, etc) probably mention passing system, platform, along with the optional crossSystem to\nNixpkgs: import <nixpkgs> { system = ..; platform = ..; crossSystem = ..; }. Passing those two instead of localSystem is\nstill supported for compatibility, but is discouraged. Indeed, much of the inference we do for these parameters is\nmotivated by compatibility as much as convenience. \n\nOne would think that localSystem and crossSystem overlap horribly with the three *Platforms (buildPlatform,\nhostPlatform, and targetPlatform; see stage.nix or the manual). Actually, those identifiers are purposefully not used\nhere to draw a subtle but important distinction: While the granularity of having 3 platforms is necessary to properly\nbuild packages, it is overkill for specifying the user's intent when making a build plan or package set. A simple \"build\nvs deploy\" dichotomy is adequate: the sliding window principle described in the previous section shows how to\ninterpolate between the these two \"end points\" to get the 3 platform triple for each bootstrapping stage. That means for\nany package a given package set, even those not bound on the top level but only reachable via dependencies or\nbuildPackages, the three platforms will be defined as one of localSystem or crossSystem, with the former replacing the\nlatter as one traverses build-time dependencies. A last simple difference is that crossSystem should be null when one\ndoesn't want to cross-compile, while the *Platforms are always non-null. localSystem is always non-null.\n\nCross-compilation infrastructureImplementation of dependenciesThe categories of dependencies developed in\nCross-compilation are specified as lists of derivations given to mkDerivation, as documented in The Standard\nEnvironment. In short, each list of dependencies for \"host → target\" of \"foo → bar\" is called depsFooBar, with\nexceptions for backwards compatibility that depsBuildHost is instead called nativeBuildInputs and depsHostTarget is\ninstead called buildInputs. Nixpkgs is now structured so that each depsFooBar is automatically taken from pkgsFooBar.\n(These pkgsFooBars are quite new, so there is no special case for nativeBuildInputs and buildInputs.) For example,\npkgsBuildHost.gcc should be used at build-time, while pkgsHostTarget.gcc should be used at run-time.\n\nNow, for most of Nixpkgs's history, there were no pkgsFooBar attributes, and most packages have not been refactored to\nuse it explicitly. Prior to those, there were just buildPackages, pkgs, and targetPackages. Those are now redefined as\naliases to pkgsBuildHost, pkgsHostTarget, and pkgsTargetTarget. It is acceptable, even recommended, to use them for\nlibraries to show that the host platform is irrelevant.\n\nBut before that, there was just pkgs, even though both buildInputs and nativeBuildInputs existed. [Cross barely worked,\nand those were implemented with some hacks on mkDerivation to override dependencies.] What this means is the vast\nmajority of packages do not use any explicit package set to populate their dependencies, just using whatever callPackage\ngives them even if they do correctly sort their dependencies into the multiple lists described above. And indeed, asking\nthat users both sort their dependencies, and take them from the right attribute set, is both too onerous and redundant,\nso the recommended approach (for now) is to continue just categorizing by list and not using an explicit package set.\n\nTo make this work, we \"splice\" together the six pkgsFooBar package sets and have callPackage actually take its arguments\nfrom that. This is currently implemented in pkgs\/top-level\/splice.nix. mkDerivation then, for each dependency attribute,\npulls the right derivation out from the splice. This splicing can be skipped when not cross-compiling as the package\nsets are the same, but still is a bit slow for cross-compiling. We'd like to do something better, but haven't come up\nwith anything yet.\n\nBootstrappingEach of the package sets described above come from a single bootstrapping stage. While\npkgs\/top-level\/default.nix, coordinates the composition of stages at a high level, pkgs\/top-level\/stage.nix \"ties the\nknot\" (creates the fixed point) of each stage. The package sets are defined per-stage however, so they can be thought of\nas edges between stages (the nodes) in a graph. Compositions like pkgsBuildTarget.targetPackages can be thought of as\npaths to this graph.\n\nWhile there are many package sets, and thus many edges, the stages can also be arranged in a linear chain. In other\nwords, many of the edges are redundant as far as connectivity is concerned. This hinges on the type of bootstrapping we\ndo. Currently for cross it is:\n\n1.  (native, native, native)\n\n2.  (native, native, foreign)\n\n3.  (native, foreign, foreign)\n\nIn each stage, pkgsBuildHost refers to the previous stage, pkgsBuildBuild refers to the one before that, and\npkgsHostTarget refers to the current one, and pkgsTargetTarget refers to the next one. When there is no previous or next\nstage, they instead refer to the current stage. Note how all the invariants regarding the mapping between dependency and\ndepending packages' build host and target platforms are preserved. pkgsBuildTarget and pkgsHostHost are more complex in\nthat the stage fitting the requirements isn't always a fixed chain of \"prevs\" and \"nexts\" away (modulo the \"saturating\"\nself-references at the ends). We just special case each instead. All the primary edges are implemented is in\npkgs\/stdenv\/booter.nix, and secondarily aliases in pkgs\/top-level\/stage.nix.\n\nThe native stages are bootstrapped in legacy ways that predate the current cross implementation. This is why the\nbootstrapping stages leading up to the final stages are ignored in the previous paragraph. \n\nIf one looks at the 3 platform triples, one can see that they overlap such that one could put them together into a chain\nlike:\n\n(native, native, native, foreign, foreign)\n\nIf one imagines the saturating self references at the end being replaced with infinite stages, and then overlays those\nplatform triples, one ends up with the infinite tuple:\n\n(native..., native, native, native, foreign, foreign, foreign...)\n\nOne can then imagine any sequence of platforms such that there are bootstrap stages with their 3 platforms determined by\n\"sliding a window\" that is the 3 tuple through the sequence. This was the original model for bootstrapping. Without a\ntarget platform (assume a better world where all compilers are multi-target and all standard libraries are built in\ntheir own derivation), this is sufficient. Conversely if one wishes to cross compile \"faster\", with a \"Canadian Cross\"\nbootstrapping stage where build != host != target, more bootstrapping stages are needed since no sliding window provides\nthe pesky pkgsBuildTarget package set since it skips the Canadian cross stage's \"host\".\n\nIt is much better to refer to buildPackages than targetPackages, or more broadly package sets that do not mention\n“target”. There are three reasons for this.\n\nFirst, it is because bootstrapping stages do not have a unique targetPackages. For example a (x86-linux, x86-linux,\narm-linux) and (x86-linux, x86-linux, x86-windows) package set both have a (x86-linux, x86-linux, x86-linux) package\nset. Because there is no canonical targetPackages for such a native (build == host == target) package set, we set their\ntargetPackages\n\nSecond, it is because this is a frequent source of hard-to-follow \"infinite recursions\" \/ cycles. When only package sets\nthat don't mention target are used, the package set forms a directed acyclic graph. This means that all cycles that\nexist are confined to one stage. This means they are a lot smaller, and easier to follow in the code or a backtrace. It\nalso means they are present in native and cross builds alike, and so more likely to be caught by CI and other users.\n\nThirdly, it is because everything target-mentioning only exists to accommodate compilers with lousy build systems that\ninsist on the compiler itself and standard library being built together. Of course that is bad because bigger\nderivations means longer rebuilds. It is also problematic because it tends to make the standard libraries less like\nother libraries than they could be, complicating code and build systems alike. Because of the other problems, and\nbecause of these innate disadvantages, compilers ought to be packaged another way where possible. \n\nIf one explores Nixpkgs, they will see derivations with names like gccCross. Such *Cross derivations is a holdover from\nbefore we properly distinguished between the host and target platforms—the derivation with “Cross” in the name covered\nthe build = host != target case, while the other covered the host = target, with build platform the same or not based on\nwhether one was using its .nativeDrv or .crossDrv. This ugliness will disappear soon. \n" }
,{ "url": "stdenv\/platform-notes\/", "title": "Platform Notes", "text": "Platform NotesDarwin (macOS)Some common issues when packaging software for Darwin:\n\n  - The Darwin stdenv uses clang instead of gcc. When referring to the compiler $CC or cc will work in both cases. Some\n    builds hardcode gcc\/g++ in their build scripts, that can usually be fixed with using something like makeFlags = [\n    \"CC=cc\" ]; or by patching the build scripts.\n    \n    stdenv.mkDerivation {\n      name = \"libfoo-1.2.3\";\n      # ...\n      buildPhase = ''\n        $CC -o hello hello.c\n      '';\n    }\n\n  - On Darwin, libraries are linked using absolute paths, libraries are resolved by their install_name at link time.\n    Sometimes packages won’t set this correctly causing the library lookups to fail at runtime. This can be fixed by\n    adding extra linker flags or by running install_name_tool -id during the fixupPhase.\n    \n    stdenv.mkDerivation {\n      name = \"libfoo-1.2.3\";\n      # ...\n      makeFlags = lib.optional stdenv.isDarwin \"LDFLAGS=-Wl,-install_name,$(out)\/lib\/libfoo.dylib\";\n    }\n\n  - Even if the libraries are linked using absolute paths and resolved via their install_name correctly, tests can\n    sometimes fail to run binaries. This happens because the checkPhase runs before the libraries are installed.\n    \n    This can usually be solved by running the tests after the installPhase or alternatively by using DYLD_LIBRARY_PATH.\n    More information about this variable can be found in the dyld(1) manpage.\n    \n    dyld: Library not loaded: \/nix\/store\/7hnmbscpayxzxrixrgxvvlifzlxdsdir-jq-1.5-lib\/lib\/libjq.1.dylib\n    Referenced from: \/private\/tmp\/nix-build-jq-1.5.drv-0\/jq-1.5\/tests\/..\/jq\n    Reason: image not found\n    .\/tests\/jqtest: line 5: 75779 Abort trap: 6\n\n    stdenv.mkDerivation {\n      name = \"libfoo-1.2.3\";\n      # ...\n      doInstallCheck = true;\n      installCheckTarget = \"check\";\n    }\n\n  - Some packages assume xcode is available and use xcrun to resolve build tools like clang, etc. This causes errors\n    like xcode-select: error: no developer tools were found at '\/Applications\/Xcode.app' while the build doesn’t\n    actually depend on xcode.\n    \n    stdenv.mkDerivation {\n      name = \"libfoo-1.2.3\";\n      # ...\n      prePatch = ''\n        substituteInPlace Makefile \\\n            --replace '\/usr\/bin\/xcrun clang' clang\n      '';\n    }\n\n    The package xcbuild can be used to build projects that really depend on Xcode. However, this replacement is not 100%\n    compatible with Xcode and can occasionally cause issues.\n" }
,{ "url": "builders\/fetchers\/", "title": "Fetchers", "text": "FetchersWhen using Nix, you will frequently need to download source code and other files from the internet. Nixpkgs\ncomes with a few helper functions that allow you to fetch fixed-output derivations in a structured way.\n\nThe two fetcher primitives are fetchurl and fetchzip. Both of these have two required arguments, a URL and a hash. The\nhash is typically sha256, although many more hash algorithms are supported. Nixpkgs contributors are currently\nrecommended to use sha256. This hash will be used by Nix to identify your source. A typical usage of fetchurl is\nprovided below.\n\n{ stdenv, fetchurl }:\n\nstdenv.mkDerivation {\n  name = \"hello\";\n  src = fetchurl {\n    url = \"http:\/\/www.example.org\/hello.tar.gz\";\n    sha256 = \"1111111111111111111111111111111111111111111111111111\";\n  };\n}\n\nThe main difference between fetchurl and fetchzip is in how they store the contents. fetchurl will store the unaltered\ncontents of the URL within the Nix store. fetchzip on the other hand will decompress the archive for you, making files\nand directories directly accessible in the future. fetchzip can only be used with archives. Despite the name, fetchzip\nis not limited to .zip files and can also be used with any tarball.\n\nfetchpatch works very similarly to fetchurl with the same arguments expected. It expects patch files as a source and\nperforms normalization on them before computing the checksum. For example it will remove comments or other unstable\nparts that are sometimes added by version control systems and can change over time.\n\nOther fetcher functions allow you to add source code directly from a VCS such as subversion or git. These are mostly\nstraightforward nambes based on the name of the command used with the VCS system. Because they give you a working\nrepository, they act most like fetchzip.\n\nfetchsvnUsed with Subversion. Expects url to a Subversion directory, rev, and sha256.\n\nfetchgitUsed with Git. Expects url to a Git repo, rev, and sha256. rev in this case can be full the git commit id (SHA1\nhash) or a tag name like refs\/tags\/v1.0.\n\nAdditionally the following optional arguments can be given: fetchSubmodules = true makes fetchgit also fetch the\nsubmodules of a repository. If deepClone is set to true, the entire repository is cloned as opposing to just creating a\nshallow clone. deepClone = true also implies leaveDotGit = true which means that the .git directory of the clone won't\nbe removed after checkout.\n\nfetchfossilUsed with Fossil. Expects url to a Fossil archive, rev, and sha256.\n\nfetchcvsUsed with CVS. Expects cvsRoot, tag, and sha256.\n\nfetchhgUsed with Mercurial. Expects url, rev, and sha256.\n\nA number of fetcher functions wrap part of fetchurl and fetchzip. They are mainly convenience functions intended for\ncommonly used destinations of source code in Nixpkgs. These wrapper fetchers are listed below.\n\nfetchFromGitHubfetchFromGitHub expects four arguments. owner is a string corresponding to the GitHub user or\norganization that controls this repository. repo corresponds to the name of the software repository. These are located\nat the top of every GitHub HTML page as owner\/repo. rev corresponds to the Git commit hash or tag (e.g v1.0) that will\nbe downloaded from Git. Finally, sha256 corresponds to the hash of the extracted directory. Again, other hash algorithms\nare also available but sha256 is currently preferred.\n\nfetchFromGitHub uses fetchzip to download the source archive generated by GitHub for the specified revision. If\nleaveDotGit, deepClone or fetchSubmodules are set to true, fetchFromGitHub will use fetchgit instead. Refer to its\nsection for documentation of these options.\n\nfetchFromGitLabThis is used with GitLab repositories. The arguments expected are very similar to fetchFromGitHub above.\n\nfetchFromGitilesThis is used with Gitiles repositories. The arguments expected are similar to fetchgit.\n\nfetchFromBitbucketThis is used with BitBucket repositories. The arguments expected are very similar to fetchFromGitHub\nabove.\n\nfetchFromSavannahThis is used with Savannah repositories. The arguments expected are very similar to fetchFromGitHub\nabove.\n\nfetchFromRepoOrCzThis is used with repo.or.cz repositories. The arguments expected are very similar to fetchFromGitHub\nabove.\n\nfetchFromSourcehutThis is used with sourcehut repositories. The arguments expected are very similar to fetchFromGitHub\nabove. Don't forget the tilde (~) in front of the user name!\n" }
,{ "url": "builders\/trivial-builders\/", "title": "Trivial builders", "text": "Trivial buildersNixpkgs provides a couple of functions that help with building derivations. The most important one,\nstdenv.mkDerivation, has already been documented above. The following functions wrap stdenv.mkDerivation, making it\neasier to use in certain cases.\n\nrunCommandThis takes three arguments, name, env, and buildCommand. name is just the name that Nix will append to the\nstore path in the same way that stdenv.mkDerivation uses its name attribute. env is an attribute set specifying\nenvironment variables that will be set for this derivation. These attributes are then passed to the wrapped\nstdenv.mkDerivation. buildCommand specifies the commands that will be run to create this derivation. Note that you will\nneed to create $out for Nix to register the command as successful.\n\nAn example of using runCommand is provided below.\n\n(import <nixpkgs> {}).runCommand \"my-example\" {} ''\n  echo My example command is running\n\n  mkdir $out\n\n  echo I can write data to the Nix store > $out\/message\n\n  echo I can also run basic commands like:\n\n  echo ls\n  ls\n\n  echo whoami\n  whoami\n\n  echo date\n  date\n''\n\nrunCommandCCThis works just like runCommand. The only difference is that it also provides a C compiler in buildCommand's\nenvironment. To minimize your dependencies, you should only use this if you are sure you will need a C compiler as part\nof running your command.\n\nrunCommandLocalVariant of runCommand that forces the derivation to be built locally, it is not substituted. This is\nintended for very cheap commands (<1s execution time). It saves on the network roundrip and can speed up a build.\n\nThis sets allowSubstitutes to false, so only use runCommandLocal if you are certain the user will always have a builder\nfor the system of the derivation. This should be true for most trivial use cases (e.g. just copying some files to a\ndifferent location or adding symlinks), because there the system is usually the same as builtins.currentSystem. \n\nwriteTextFile, writeText, writeTextDir, writeScript, writeScriptBinThese functions write text to the Nix store. This is\nuseful for creating scripts from Nix expressions. writeTextFile takes an attribute set and expects two arguments, name\nand text. name corresponds to the name used in the Nix store path. text will be the contents of the file. You can also\nset executable to true to make this file have the executable bit set.\n\nMany more commands wrap writeTextFile including writeText, writeTextDir, writeScript, and writeScriptBin. These are\nconvenience functions over writeTextFile.\n\nsymlinkJoinThis can be used to put many derivations into the same directory structure. It works by creating a new\nderivation and adding symlinks to each of the paths listed. It expects two arguments, name, and paths. name is the name\nused in the Nix store path for the created derivation. paths is a list of paths that will be symlinked. These paths can\nbe to Nix store derivations or any other subdirectory contained within.\n\nwriteReferencesToFileWrites the closure of transitive dependencies to a file.\n\nThis produces the equivalent of nix-store -q --requisites.\n\nFor example,\n\nwriteReferencesToFile (writeScriptBin \"hi\" ''${hello}\/bin\/hello'')\n\nproduces an output path \/nix\/store\/<hash>-runtime-deps containing\n\n\/nix\/store\/<hash>-hello-2.10\n\/nix\/store\/<hash>-hi\n\/nix\/store\/<hash>-libidn2-2.3.0\n\/nix\/store\/<hash>-libunistring-0.9.10\n\/nix\/store\/<hash>-glibc-2.32-40\n\nYou can see that this includes hi, the original input path, hello, which is a direct reference, but also the other paths\nthat are indirectly required to run hello.\n\nwriteDirectReferencesToFileWrites the set of references to the output file, that is, their immediate dependencies.\n\nThis produces the equivalent of nix-store -q --references.\n\nFor example,\n\nwriteDirectReferencesToFile (writeScriptBin \"hi\" ''${hello}\/bin\/hello'')\n\nproduces an output path \/nix\/store\/<hash>-runtime-references containing\n\n\/nix\/store\/<hash>-hello-2.10\n\nbut none of hello's dependencies, because those are not referenced directly by hi's output.\n" }
,{ "url": "builders\/special\/", "title": "Special builders", "text": "Special buildersThis chapter describes several special builders.\n" }
,{ "url": "builders\/special\/fhs-environments\/", "title": "buildFHSUserEnv", "text": "buildFHSUserEnvbuildFHSUserEnv provides a way to build and run FHS-compatible lightweight sandboxes. It creates an\nisolated root with bound \/nix\/store, so its footprint in terms of disk space needed is quite small. This allows one to\nrun software which is hard or unfeasible to patch for NixOS -- 3rd-party source trees with FHS assumptions, games\ndistributed as tarballs, software with integrity checking and\/or external self-updated binaries. It uses Linux\nnamespaces feature to create temporary lightweight environments which are destroyed after all child processes exit,\nwithout root user rights requirement. Accepted arguments are:\n\n  - name Environment name.\n  - targetPkgs Packages to be installed for the main host's architecture (i.e. x86_64 on x86_64 installations). Along\n    with libraries binaries are also installed.\n  - multiPkgs Packages to be installed for all architectures supported by a host (i.e. i686 and x86_64 on x86_64\n    installations). Only libraries are installed by default.\n  - extraBuildCommands Additional commands to be executed for finalizing the directory structure.\n  - extraBuildCommandsMulti Like extraBuildCommands, but executed only on multilib architectures.\n  - extraOutputsToInstall Additional derivation outputs to be linked for both target and multi-architecture packages.\n  - extraInstallCommands Additional commands to be executed for finalizing the derivation with runner script.\n  - runScript A command that would be executed inside the sandbox and passed all the command line arguments. It defaults\n    to bash.\n\nOne can create a simple environment using a shell.nix like that:\n\n{ pkgs ? import <nixpkgs> {} }:\n\n(pkgs.buildFHSUserEnv {\n  name = \"simple-x11-env\";\n  targetPkgs = pkgs: (with pkgs;\n    [ udev\n      alsa-lib\n    ]) ++ (with pkgs.xorg;\n    [ libX11\n      libXcursor\n      libXrandr\n    ]);\n  multiPkgs = pkgs: (with pkgs;\n    [ udev\n      alsa-lib\n    ]);\n  runScript = \"bash\";\n}).env\n\nRunning nix-shell would then drop you into a shell with these libraries and binaries available. You can use this to run\nclosed-source applications which expect FHS structure without hassles: simply change runScript to the application path,\ne.g. .\/bin\/start.sh -- relative paths are supported.\n" }
,{ "url": "builders\/special\/mkshell\/", "title": "pkgs.mkShell", "text": "pkgs.mkShellpkgs.mkShell is a special kind of derivation that is only useful when using it combined with nix-shell. It\nwill in fact fail to instantiate when invoked with nix-build.\n\nUsage\n\n{ pkgs ? import <nixpkgs> {} }:\npkgs.mkShell {\n  # specify which packages to add to the shell environment\n  packages = [ pkgs.gnumake ];\n  # add all the dependencies, of the given packages, to the shell environment\n  inputsFrom = with pkgs; [ hello gnutar ];\n}\n" }
,{ "url": "builders\/images\/", "title": "Images", "text": "ImagesThis chapter describes tools for creating various types of images.\n" }
,{ "url": "builders\/images\/appimagetools\/", "title": "pkgs.appimageTools", "text": "pkgs.appimageToolspkgs.appimageTools is a set of functions for extracting and wrapping AppImage files. They are meant to\nbe used if traditional packaging from source is infeasible, or it would take too long. To quickly run an AppImage file,\npkgs.appimage-run can be used as well.\n\nThe appimageTools API is unstable and may be subject to backwards-incompatible changes in the future. \n\nAppImage formatsThere are different formats for AppImages, see the specification for details.\n\n  - Type 1 images are ISO 9660 files that are also ELF executables.\n  - Type 2 images are ELF executables with an appended filesystem.\n\nThey can be told apart with file -k:\n\n$ file -k type1.AppImage\ntype1.AppImage: ELF 64-bit LSB executable, x86-64, version 1 (SYSV) ISO 9660 CD-ROM filesystem data 'AppImage' (Lepton 3.x), scale 0-0,\nspot sensor temperature 0.000000, unit celsius, color scheme 0, calibration: offset 0.000000, slope 0.000000, dynamically linked, interpreter \/lib64\/ld-linux-x86-64.so.2, for GNU\/Linux 2.6.18, BuildID[sha1]=d629f6099d2344ad82818172add1d38c5e11bc6d, stripped\\012- data\n\n$ file -k type2.AppImage\ntype2.AppImage: ELF 64-bit LSB executable, x86-64, version 1 (SYSV) (Lepton 3.x), scale 232-60668, spot sensor temperature -4.187500, color scheme 15, show scale bar, calibration: offset -0.000000, slope 0.000000 (Lepton 2.x), scale 4111-45000, spot sensor temperature 412442.250000, color scheme 3, minimum point enabled, calibration: offset -75402534979642766821519867692934234112.000000, slope 5815371847733706829839455140374904832.000000, dynamically linked, interpreter \/lib64\/ld-linux-x86-64.so.2, for GNU\/Linux 2.6.18, BuildID[sha1]=79dcc4e55a61c293c5e19edbd8d65b202842579f, stripped\\012- data\n\nNote how the type 1 AppImage is described as an ISO 9660 CD-ROM filesystem, and the type 2 AppImage is not.\n\nWrappingDepending on the type of AppImage you're wrapping, you'll have to use wrapType1 or wrapType2.\n\nappimageTools.wrapType2 { # or wrapType1\n  name = \"patchwork\";\n  src = fetchurl {\n    url = \"https:\/\/github.com\/ssbc\/patchwork\/releases\/download\/v3.11.4\/Patchwork-3.11.4-linux-x86_64.AppImage\";\n    sha256 = \"1blsprpkvm0ws9b96gb36f0rbf8f5jgmw4x6dsb1kswr4ysf591s\";\n  };\n  extraPkgs = pkgs: with pkgs; [ ];\n}\n\n  - name specifies the name of the resulting image.\n  - src specifies the AppImage file to extract.\n  - extraPkgs allows you to pass a function to include additional packages inside the FHS environment your AppImage is\n    going to run in. There are a few ways to learn which dependencies an application needs:\n      - Looking through the extracted AppImage files, reading its scripts and running patchelf and ldd on its\n        executables. This can also be done in appimage-run, by setting APPIMAGE_DEBUG_EXEC=bash.\n      - Running strace -vfefile on the wrapped executable, looking for libraries that can't be found.\n" }
,{ "url": "builders\/images\/dockertools\/", "title": "pkgs.dockerTools", "text": "pkgs.dockerToolspkgs.dockerTools is a set of functions for creating and manipulating Docker images according to the\nDocker Image Specification v1.2.0. Docker itself is not used to perform any of the operations done by these functions.\n\nbuildImageThis function is analogous to the docker build command, in that it can be used to build a Docker-compatible\nrepository tarball containing a single image with one or multiple layers. As such, the result is suitable for being\nloaded in Docker with docker load.\n\nThe parameters of buildImage with relative example values are described below:\n\nbuildImage {\n  name = \"redis\";\n  tag = \"latest\";\n\n  fromImage = someBaseImage;\n  fromImageName = null;\n  fromImageTag = \"latest\";\n\n  contents = pkgs.redis;\n  runAsRoot = ''\n    #!${pkgs.runtimeShell}\n    mkdir -p \/data\n  '';\n\n  config = {\n    Cmd = [ \"\/bin\/redis-server\" ];\n    WorkingDir = \"\/data\";\n    Volumes = { \"\/data\" = { }; };\n  };\n}\n\nThe above example will build a Docker image redis\/latest from the given base image. Loading and running this image in\nDocker results in redis-server being started automatically.\n\n  - name specifies the name of the resulting image. This is the only required argument for buildImage.\n\n  - tag specifies the tag of the resulting image. By default it's null, which indicates that the nix output hash will be\n    used as tag.\n\n  - fromImage is the repository tarball containing the base image. It must be a valid Docker image, such as exported by\n    docker save. By default it's null, which can be seen as equivalent to FROM scratch of a Dockerfile.\n\n  - fromImageName can be used to further specify the base image within the repository, in case it contains multiple\n    images. By default it's null, in which case buildImage will peek the first image available in the repository.\n\n  - fromImageTag can be used to further specify the tag of the base image within the repository, in case an image\n    contains multiple tags. By default it's null, in which case buildImage will peek the first tag available for the\n    base image.\n\n  - contents is a derivation that will be copied in the new layer of the resulting image. This can be similarly seen as\n    ADD contents\/ \/ in a Dockerfile. By default it's null.\n\n  - runAsRoot is a bash script that will run as root in an environment that overlays the existing layers of the base\n    image with the new resulting layer, including the previously copied contents derivation. This can be similarly seen\n    as RUN ... in a Dockerfile.\n\nNOTE: Using this parameter requires the kvm device to be available.\n\n  - config is used to specify the configuration of the containers that will be started off the built image in Docker.\n    The available options are listed in the Docker Image Specification v1.2.0.\n\nAfter the new layer has been created, its closure (to which contents, config and runAsRoot contribute) will be copied in\nthe layer itself. Only new dependencies that are not already in the existing layers will be copied.\n\nAt the end of the process, only one new single layer will be produced and added to the resulting image.\n\nThe resulting repository will only list the single image image\/tag. In the case of the buildImage example it would be\nredis\/latest.\n\nIt is possible to inspect the arguments with which an image was built using its buildArgs attribute.\n\nNOTE: If you see errors similar to getProtocolByName: does not exist (no such protocol name: tcp) you may need to add\npkgs.iana-etc to contents.\n\nNOTE: If you see errors similar to Error_Protocol (\"certificate has unknown CA\",True,UnknownCa) you may need to add\npkgs.cacert to contents.\n\nBy default buildImage will use a static date of one second past the UNIX Epoch. This allows buildImage to produce binary\nreproducible images. When listing images with docker images, the newly created images will be listed like this:\n\n$ docker images\nREPOSITORY   TAG      IMAGE ID       CREATED        SIZE\nhello        latest   08c791c7846e   48 years ago   25.2MB\n\nYou can break binary reproducibility but have a sorted, meaningful CREATED column by setting created to now.\n\npkgs.dockerTools.buildImage {\n  name = \"hello\";\n  tag = \"latest\";\n  created = \"now\";\n  contents = pkgs.hello;\n\n  config.Cmd = [ \"\/bin\/hello\" ];\n}\n\nand now the Docker CLI will display a reasonable date and sort the images as expected:\n\n$ docker images\nREPOSITORY   TAG      IMAGE ID       CREATED              SIZE\nhello        latest   de2bf4786de6   About a minute ago   25.2MB\n\nhowever, the produced images will not be binary reproducible.\n\nbuildLayeredImageCreate a Docker image with many of the store paths being on their own layer to improve sharing between\nimages. The image is realized into the Nix store as a gzipped tarball. Depending on the intended usage, many users might\nprefer to use streamLayeredImage instead, which this function uses internally.\n\nname\n\nThe name of the resulting image.\n\ntag optional\n\nTag of the generated image.\n\n*Default:* the output path's hash\n\nfromImage optional\n\nThe repository tarball containing the base image. It must be a valid Docker image, such as one exported by docker save.\n\n*Default:* `null`, which can be seen as equivalent to `FROM scratch` of a `Dockerfile`.\n\ncontents optional\n\nTop level paths in the container. Either a single derivation, or a list of derivations.\n\n*Default:* `[]`\n\nconfig optional\n\nRun-time configuration of the container. A full list of the options are available at in the  Docker Image Specification\nv1.2.0 .\n\n*Default:* `{}`\n\ncreated optional\n\nDate and time the layers were created. Follows the same now exception supported by buildImage.\n\n*Default:* `1970-01-01T00:00:01Z`\n\nmaxLayers optional\n\nMaximum number of layers to create.\n\n*Default:* `100`\n\n*Maximum:* `125`\n\nextraCommands optional\n\nShell commands to run while building the final layer, without access to most of the layer contents. Changes to this\nlayer are \"on top\" of all the other layers, so can create additional directories and files.\n\nfakeRootCommands optional\n\nShell commands to run while creating the archive for the final layer in a fakeroot environment. Unlike extraCommands,\nyou can run chown to change the owners of the files in the archive, changing fakeroot's state instead of the real\nfilesystem. The latter would require privileges that the build user does not have. Static binaries do not interact with\nthe fakeroot environment. By default all files in the archive will be owned by root.\n\nBehavior of contents in the final imageEach path directly listed in contents will have a symlink in the root of the\nimage.\n\nFor example:\n\npkgs.dockerTools.buildLayeredImage {\n  name = \"hello\";\n  contents = [ pkgs.hello ];\n}\n\nwill create symlinks for all the paths in the hello package:\n\n\/bin\/hello -> \/nix\/store\/h1zb1padqbbb7jicsvkmrym3r6snphxg-hello-2.10\/bin\/hello\n\/share\/info\/hello.info -> \/nix\/store\/h1zb1padqbbb7jicsvkmrym3r6snphxg-hello-2.10\/share\/info\/hello.info\n\/share\/locale\/bg\/LC_MESSAGES\/hello.mo -> \/nix\/store\/h1zb1padqbbb7jicsvkmrym3r6snphxg-hello-2.10\/share\/locale\/bg\/LC_MESSAGES\/hello.mo\n\nAutomatic inclusion of config referencesThe closure of config is automatically included in the closure of the final\nimage.\n\nThis allows you to make very simple Docker images with very little code. This container will start up and run hello:\n\npkgs.dockerTools.buildLayeredImage {\n  name = \"hello\";\n  config.Cmd = [ \"${pkgs.hello}\/bin\/hello\" ];\n}\n\nAdjusting maxLayersIncreasing the maxLayers increases the number of layers which have a chance to be shared between\ndifferent images.\n\nModern Docker installations support up to 128 layers, however older versions support as few as 42.\n\nIf the produced image will not be extended by other Docker builds, it is safe to set maxLayers to 128. However it will\nbe impossible to extend the image further.\n\nThe first (maxLayers-2) most \"popular\" paths will have their own individual layers, then layer #maxLayers-1 will contain\nall the remaining \"unpopular\" paths, and finally layer #maxLayers will contain the Image configuration.\n\nDocker's Layers are not inherently ordered, they are content-addressable and are not explicitly layered until they are\ncomposed in to an Image.\n\nstreamLayeredImageBuilds a script which, when run, will stream an uncompressed tarball of a Docker image to stdout. The\narguments to this function are as for buildLayeredImage. This method of constructing an image does not realize the image\ninto the Nix store, so it saves on IO and disk\/cache space, particularly with large images.\n\nThe image produced by running the output script can be piped directly into docker load, to load it into the local docker\ndaemon:\n\n$(nix-build) | docker load\n\nAlternatively, the image be piped via gzip into skopeo, e.g. to copy it into a registry:\n\n$(nix-build) | gzip --fast | skopeo copy docker-archive:\/dev\/stdin docker:\/\/some_docker_registry\/myimage:tag\n\npullImageThis function is analogous to the docker pull command, in that it can be used to pull a Docker image from a\nDocker registry. By default Docker Hub is used to pull images.\n\nIts parameters are described in the example below:\n\npullImage {\n  imageName = \"nixos\/nix\";\n  imageDigest =\n    \"sha256:20d9485b25ecfd89204e843a962c1bd70e9cc6858d65d7f5fadc340246e2116b\";\n  finalImageName = \"nix\";\n  finalImageTag = \"1.11\";\n  sha256 = \"0mqjy3zq2v6rrhizgb9nvhczl87lcfphq9601wcprdika2jz7qh8\";\n  os = \"linux\";\n  arch = \"x86_64\";\n}\n\n  - imageName specifies the name of the image to be downloaded, which can also include the registry namespace (e.g.\n    nixos). This argument is required.\n\n  - imageDigest specifies the digest of the image to be downloaded. This argument is required.\n\n  - finalImageName, if specified, this is the name of the image to be created. Note it is never used to fetch the image\n    since we prefer to rely on the immutable digest ID. By default it's equal to imageName.\n\n  - finalImageTag, if specified, this is the tag of the image to be created. Note it is never used to fetch the image\n    since we prefer to rely on the immutable digest ID. By default it's latest.\n\n  - sha256 is the checksum of the whole fetched image. This argument is required.\n\n  - os, if specified, is the operating system of the fetched image. By default it's linux.\n\n  - arch, if specified, is the cpu architecture of the fetched image. By default it's x86_64.\n\nnix-prefetch-docker command can be used to get required image parameters:\n\n$ nix run nixpkgs.nix-prefetch-docker -c nix-prefetch-docker --image-name mysql --image-tag 5\n\nSince a given imageName may transparently refer to a manifest list of images which support multiple architectures and\/or\noperating systems, you can supply the --os and --arch arguments to specify exactly which image you want. By default it\nwill match the OS and architecture of the host the command is run on.\n\n$ nix-prefetch-docker --image-name mysql --image-tag 5 --arch x86_64 --os linux\n\nDesired image name and tag can be set using --final-image-name and --final-image-tag arguments:\n\n$ nix-prefetch-docker --image-name mysql --image-tag 5 --final-image-name eu.gcr.io\/my-project\/mysql --final-image-tag prod\n\nexportImageThis function is analogous to the docker export command, in that it can be used to flatten a Docker image\nthat contains multiple layers. It is in fact the result of the merge of all the layers of the image. As such, the result\nis suitable for being imported in Docker with docker import.\n\nNOTE: Using this function requires the kvm device to be available.\n\nThe parameters of exportImage are the following:\n\nexportImage {\n  fromImage = someLayeredImage;\n  fromImageName = null;\n  fromImageTag = null;\n\n  name = someLayeredImage.name;\n}\n\nThe parameters relative to the base image have the same synopsis as described in buildImage, except that fromImage is\nthe only required argument in this case.\n\nThe name argument is the name of the derivation output, which defaults to fromImage.name.\n\nshadowSetupThis constant string is a helper for setting up the base files for managing users and groups, only if such\nfiles don't exist already. It is suitable for being used in a buildImage runAsRoot script for cases like in the example\nbelow:\n\nbuildImage {\n  name = \"shadow-basic\";\n\n  runAsRoot = ''\n    #!${pkgs.runtimeShell}\n    ${shadowSetup}\n    groupadd -r redis\n    useradd -r -g redis redis\n    mkdir \/data\n    chown redis:redis \/data\n  '';\n}\n\nCreating base files like \/etc\/passwd or \/etc\/login.defs is necessary for shadow-utils to manipulate users and groups.\n" }
,{ "url": "builders\/images\/ocitools\/", "title": "pkgs.ociTools", "text": "pkgs.ociToolspkgs.ociTools is a set of functions for creating containers according to the OCI container specification\nv1.0.0. Beyond that it makes no assumptions about the container runner you choose to use to run the created container.\n\nbuildContainerThis function creates a simple OCI container that runs a single command inside of it. An OCI container\nconsists of a config.json and a rootfs directory.The nix store of the container will contain all referenced dependencies\nof the given command.\n\nThe parameters of buildContainer with an example value are described below:\n\nbuildContainer {\n  args = [\n    (with pkgs;\n      writeScript \"run.sh\" ''\n        #!${bash}\/bin\/bash\n        exec ${bash}\/bin\/bash\n      '').outPath\n  ];\n\n  mounts = {\n    \"\/data\" = {\n      type = \"none\";\n      source = \"\/var\/lib\/mydata\";\n      options = [ \"bind\" ];\n    };\n  };\n\n  readonly = false;\n}\n\n  - args specifies a set of arguments to run inside the container. This is the only required argument for\n    buildContainer. All referenced packages inside the derivation will be made available inside the container\n\n  - mounts specifies additional mount points chosen by the user. By default only a minimal set of necessary filesystems\n    are mounted into the container (e.g procfs, cgroupfs)\n\n  - readonly makes the container's rootfs read-only if it is set to true. The default value is false false.\n" }
,{ "url": "builders\/images\/snaptools\/", "title": "pkgs.snapTools", "text": "pkgs.snapToolspkgs.snapTools is a set of functions for creating Snapcraft images. Snap and Snapcraft is not used to\nperform these operations.\n\nThe makeSnap FunctionmakeSnap takes a single named argument, meta. This argument mirrors the upstream snap.yaml format\nexactly.\n\nThe base should not be specified, as makeSnap will force set it.\n\nCurrently, makeSnap does not support creating GUI stubs.\n\nBuild a Hello World SnapThe following expression packages GNU Hello as a Snapcraft snap.\n\nlet\n  inherit (import <nixpkgs> { }) snapTools hello;\nin snapTools.makeSnap {\n  meta = {\n    name = \"hello\";\n    summary = hello.meta.description;\n    description = hello.meta.longDescription;\n    architectures = [ \"amd64\" ];\n    confinement = \"strict\";\n    apps.hello.command = \"${hello}\/bin\/hello\";\n  };\n}\n\nnix-build this expression and install it with snap install .\/result --dangerous. hello will now be the Snapcraft version\nof the package.\n\nBuild a Graphical SnapGraphical programs require many more integrations with the host. This example uses Firefox as an\nexample, because it is one of the most complicated programs we could package.\n\nlet\n  inherit (import <nixpkgs> { }) snapTools firefox;\nin snapTools.makeSnap {\n  meta = {\n    name = \"nix-example-firefox\";\n    summary = firefox.meta.description;\n    architectures = [ \"amd64\" ];\n    apps.nix-example-firefox = {\n      command = \"${firefox}\/bin\/firefox\";\n      plugs = [\n        \"pulseaudio\"\n        \"camera\"\n        \"browser-support\"\n        \"avahi-observe\"\n        \"cups-control\"\n        \"desktop\"\n        \"desktop-legacy\"\n        \"gsettings\"\n        \"home\"\n        \"network\"\n        \"mount-observe\"\n        \"removable-media\"\n        \"x11\"\n      ];\n    };\n    confinement = \"strict\";\n  };\n}\n\nnix-build this expression and install it with snap install .\/result --dangerous. nix-example-firefox will now be the\nSnapcraft version of the Firefox package.\n\nThe specific meaning behind plugs can be looked up in the Snapcraft interface documentation.\n" }
,{ "url": "languages-frameworks\/index\/", "title": "Languages and frameworks", "text": "Languages and frameworksThe standard build environment makes it easy to build typical Autotools-based packages with very\nlittle code. Any other kind of package can be accomodated by overriding the appropriate phases of stdenv. However, there\nare specialised functions in Nixpkgs to easily build packages for other programming languages, such as Perl or Haskell.\nThese are described in this chapter.\n" }
,{ "url": "languages-frameworks\/agda\/", "title": "Agda", "text": "AgdaHow to use AgdaAgda is available as the agda package.\n\nThe agda package installs an Agda-wrapper, which calls agda with --library-file set to a generated library-file within\nthe nix store, this means your library-file in $HOME\/.agda\/libraries will be ignored. By default the agda package\ninstalls Agda with no libraries, i.e. the generated library-file is empty. To use Agda with libraries, the\nagda.withPackages function can be used. This function either takes:\n\n  - A list of packages,\n  - or a function which returns a list of packages when given the agdaPackages attribute set,\n  - or an attribute set containing a list of packages and a GHC derivation for compilation (see below).\n  - or an attribute set containing a function which returns a list of packages when given the agdaPackages attribute set\n    and a GHC derivation for compilation (see below).\n\nFor example, suppose we wanted a version of Agda which has access to the standard library. This can be obtained with the\nexpressions:\n\nagda.withPackages [ agdaPackages.standard-library ]\n\nor\n\nagda.withPackages (p: [ p.standard-library ])\n\nor can be called as in the Compiling Agda section.\n\nIf you want to use a different version of a library (for instance a development version) override the src attribute of\nthe package to point to your local repository\n\nagda.withPackages (p: [\n  (p.standard-library.overrideAttrs (oldAttrs: {\n    version = \"local version\";\n    src = \/path\/to\/local\/repo\/agda-stdlib;\n  }))\n])\n\nYou can also reference a GitHub repository\n\nagda.withPackages (p: [\n  (p.standard-library.overrideAttrs (oldAttrs: {\n    version = \"1.5\";\n    src =  fetchFromGitHub {\n      repo = \"agda-stdlib\";\n      owner = \"agda\";\n      rev = \"v1.5\";\n      sha256 = \"16fcb7ssj6kj687a042afaa2gq48rc8abihpm14k684ncihb2k4w\";\n    };\n  }))\n])\n\nIf you want to use a library not added to Nixpkgs, you can add a dependency to a local library by calling\nagdaPackages.mkDerivation.\n\nagda.withPackages (p: [\n  (p.mkDerivation {\n    pname = \"your-agda-lib\";\n    version = \"1.0.0\";\n    src = \/path\/to\/your-agda-lib;\n  })\n])\n\nAgain you can reference GitHub\n\nagda.withPackages (p: [\n  (p.mkDerivation {\n    pname = \"your-agda-lib\";\n    version = \"1.0.0\";\n    src = fetchFromGitHub {\n      repo = \"repo\";\n      owner = \"owner\";\n      version = \"...\";\n      rev = \"...\";\n      sha256 = \"...\";\n    };\n  })\n])\n\nSee Building Agda Packages for more information on mkDerivation.\n\nAgda will not by default use these libraries. To tell Agda to use a library we have some options:\n\n  - Call agda with the library flag:\n    $ agda -l standard-library -i . MyFile.agda\n  - Write a my-library.agda-lib file for the project you are working on which may look like:\n    name: my-library\n    include: .\n    depend: standard-library\n  - Create the file ~\/.agda\/defaults and add any libraries you want to use by default.\n\nMore information can be found in the official Agda documentation on library management.\n\nCompiling AgdaAgda modules can be compiled using the GHC backend with the --compile flag. A version of ghc with ieee754\nis made available to the Agda program via the --with-compiler flag. This can be overridden by a different version of ghc\nas follows:\n\nagda.withPackages {\n  pkgs = [ ... ];\n  ghc = haskell.compiler.ghcHEAD;\n}\n\nWriting Agda packagesTo write a nix derivation for an Agda library, first check that the library has a *.agda-lib file.\n\nA derivation can then be written using agdaPackages.mkDerivation. This has similar arguments to stdenv.mkDerivation with\nthe following additions:\n\n  - everythingFile can be used to specify the location of the Everything.agda file, defaulting to .\/Everything.agda. If\n    this file does not exist then either it should be patched in or the buildPhase should be overridden (see below).\n  - libraryName should be the name that appears in the *.agda-lib file, defaulting to pname.\n  - libraryFile should be the file name of the *.agda-lib file, defaulting to ${libraryName}.agda-lib.\n\nHere is an example default.nix\n\n{ nixpkgs ?  <nixpkgs> }:\nwith (import nixpkgs {});\nagdaPackages.mkDerivation {\n  version = \"1.0\";\n  pname = \"my-agda-lib\";\n  src = .\/.;\n  buildInputs = [\n    agdaPackages.standard-library\n  ];\n}\n\nBuilding Agda packagesThe default build phase for agdaPackages.mkDerivation simply runs agda on the Everything.agda\nfile. If something else is needed to build the package (e.g. make) then the buildPhase should be overridden.\nAdditionally, a preBuild or configurePhase can be used if there are steps that need to be done prior to checking the\nEverything.agda file. agda and the Agda libraries contained in buildInputs are made available during the build phase.\n\nInstalling Agda packagesThe default install phase copies Agda source files, Agda interface files (*.agdai) and\n*.agda-lib files to the output directory. This can be overridden.\n\nBy default, Agda sources are files ending on .agda, or literate Agda files ending on .lagda, .lagda.tex, .lagda.org,\n.lagda.md, .lagda.rst. The list of recognised Agda source extensions can be extended by setting the extraExtensions\nconfig variable.\n\nAdding Agda packages to NixpkgsTo add an Agda package to nixpkgs, the derivation should be written to\npkgs\/development\/libraries\/agda\/${library-name}\/ and an entry should be added to pkgs\/top-level\/agda-packages.nix. Here\nit is called in a scope with access to all other Agda libraries, so the top line of the default.nix can look like:\n\n{ mkDerivation, standard-library, fetchFromGitHub }:\n\nNote that the derivation function is called with mkDerivation set to agdaPackages.mkDerivation, therefore you could use\na similar set as in your default.nix from Writing Agda Packages with agdaPackages.mkDerivation replaced with\nmkDerivation.\n\nHere is an example skeleton derivation for iowa-stdlib:\n\nmkDerivation {\n  version = \"1.5.0\";\n  pname = \"iowa-stdlib\";\n\n  src = ...\n\n  libraryFile = \"\";\n  libraryName = \"IAL-1.3\";\n\n  buildPhase = ''\n    patchShebangs find-deps.sh\n    make\n  '';\n}\n\nThis library has a file called .agda-lib, and so we give an empty string to libraryFile as nothing precedes .agda-lib in\nthe filename. This file contains name: IAL-1.3, and so we let libraryName = \"IAL-1.3\". This library does not use an\nEverything.agda file and instead has a Makefile, so there is no need to set everythingFile and we set a custom\nbuildPhase.\n\nWhen writing an Agda package it is essential to make sure that no .agda-lib file gets added to the store as a single\nfile (for example by using writeText). This causes Agda to think that the nix store is a Agda library and it will\nattempt to write to it whenever it typechecks something. See https:\/\/github.com\/agda\/agda\/issues\/4613.\n" }
,{ "url": "languages-frameworks\/android\/", "title": "Android", "text": "AndroidThe Android build environment provides three major features and a number of supporting features.\n\nDeploying an Android SDK installation with pluginsThe first use case is deploying the SDK with a desired set of plugins\nor subsets of an SDK.\n\nwith import <nixpkgs> {};\n\nlet\n  androidComposition = androidenv.composeAndroidPackages {\n    toolsVersion = \"26.1.1\";\n    platformToolsVersion = \"30.0.5\";\n    buildToolsVersions = [ \"30.0.3\" ];\n    includeEmulator = false;\n    emulatorVersion = \"30.3.4\";\n    platformVersions = [ \"28\" \"29\" \"30\" ];\n    includeSources = false;\n    includeSystemImages = false;\n    systemImageTypes = [ \"google_apis_playstore\" ];\n    abiVersions = [ \"armeabi-v7a\" \"arm64-v8a\" ];\n    cmakeVersions = [ \"3.10.2\" ];\n    includeNDK = true;\n    ndkVersions = [\"22.0.7026061\"];\n    useGoogleAPIs = false;\n    useGoogleTVAddOns = false;\n    includeExtras = [\n      \"extras;google;gcm\"\n    ];\n  };\nin\nandroidComposition.androidsdk\n\nThe above function invocation states that we want an Android SDK with the above specified plugin versions. By default,\nmost plugins are disabled. Notable exceptions are the tools, platform-tools and build-tools sub packages.\n\nThe following parameters are supported:\n\n  - toolsVersion, specifies the version of the tools package to use\n  - platformsToolsVersion specifies the version of the platform-tools plugin\n  - buildToolsVersions specifies the versions of the build-tools plugins to use.\n  - includeEmulator specifies whether to deploy the emulator package (false by default). When enabled, the version of\n    the emulator to deploy can be specified by setting the emulatorVersion parameter.\n  - cmakeVersions specifies which CMake versions should be deployed.\n  - includeNDK specifies that the Android NDK bundle should be included. Defaults to: false.\n  - ndkVersions specifies the NDK versions that we want to use. These are linked under the ndk directory of the SDK\n    root, and the first is linked under the ndk-bundle directory.\n  - ndkVersion is equivalent to specifying one entry in ndkVersions, and ndkVersions overrides this parameter if\n    provided.\n  - includeExtras is an array of identifier strings referring to arbitrary add-on packages that should be installed.\n  - platformVersions specifies which platform SDK versions should be included.\n\nFor each platform version that has been specified, we can apply the following options:\n\n  - includeSystemImages specifies whether a system image for each platform SDK should be included.\n  - includeSources specifies whether the sources for each SDK version should be included.\n  - useGoogleAPIs specifies that for each selected platform version the Google API should be included.\n  - useGoogleTVAddOns specifies that for each selected platform version the Google TV add-on should be included.\n\nFor each requested system image we can specify the following options:\n\n  - systemImageTypes specifies what kind of system images should be included. Defaults to: default.\n  - abiVersions specifies what kind of ABI version of each system image should be included. Defaults to: armeabi-v7a.\n\nMost of the function arguments have reasonable default settings.\n\nYou can specify license names:\n\n  - extraLicenses is a list of license names. You can get these names from repo.json or querypackages.sh licenses. The\n    SDK license (android-sdk-license) is accepted for you if you set accept_license to true. If you are doing something\n    like working with preview SDKs, you will want to add android-sdk-preview-license or whichever license applies here.\n\nAdditionally, you can override the repositories that composeAndroidPackages will pull from:\n\n  - repoJson specifies a path to a generated repo.json file. You can generate this by running generate.sh, which in turn\n    will call into mkrepo.rb.\n  - repoXmls is an attribute set containing paths to repo XML files. If specified, it takes priority over repoJson, and\n    will trigger a local build writing out a repo.json to the Nix store based on the given repository XMLs.\n\nrepoXmls = {\n  packages = [ .\/xml\/repository2-1.xml ];\n  images = [\n    .\/xml\/android-sys-img2-1.xml\n    .\/xml\/android-tv-sys-img2-1.xml\n    .\/xml\/android-wear-sys-img2-1.xml\n    .\/xml\/android-wear-cn-sys-img2-1.xml\n    .\/xml\/google_apis-sys-img2-1.xml\n    .\/xml\/google_apis_playstore-sys-img2-1.xml\n  ];\n  addons = [ .\/xml\/addon2-1.xml ];\n};\n\nWhen building the above expression with:\n\n$ nix-build\n\nThe Android SDK gets deployed with all desired plugin versions.\n\nWe can also deploy subsets of the Android SDK. For example, to only the platform-tools package, you can evaluate the\nfollowing expression:\n\nwith import <nixpkgs> {};\n\nlet\n  androidComposition = androidenv.composeAndroidPackages {\n    # ...\n  };\nin\nandroidComposition.platform-tools\n\nUsing predefined Android package compositionsIn addition to composing an Android package set manually, it is also\npossible to use a predefined composition that contains all basic packages for a specific Android version, such as\nversion 9.0 (API-level 28).\n\nThe following Nix expression can be used to deploy the entire SDK with all basic plugins:\n\nwith import <nixpkgs> {};\n\nandroidenv.androidPkgs_9_0.androidsdk\n\nIt is also possible to use one plugin only:\n\nwith import <nixpkgs> {};\n\nandroidenv.androidPkgs_9_0.platform-tools\n\nBuilding an Android applicationIn addition to the SDK, it is also possible to build an Ant-based Android project and\nautomatically deploy all the Android plugins that a project requires.\n\nwith import <nixpkgs> {};\n\nandroidenv.buildApp {\n  name = \"MyAndroidApp\";\n  src = .\/myappsources;\n  release = true;\n\n  # If release is set to true, you need to specify the following parameters\n  keyStore = .\/keystore;\n  keyAlias = \"myfirstapp\";\n  keyStorePassword = \"mykeystore\";\n  keyAliasPassword = \"myfirstapp\";\n\n  # Any Android SDK parameters that install all the relevant plugins that a\n  # build requires\n  platformVersions = [ \"24\" ];\n\n  # When we include the NDK, then ndk-build is invoked before Ant gets invoked\n  includeNDK = true;\n}\n\nAside from the app-specific build parameters (name, src, release and keystore parameters), the buildApp {} function\nsupports all the function parameters that the SDK composition function (the function shown in the previous section)\nsupports.\n\nThis build function is particularly useful when it is desired to use Hydra: the Nix-based continuous integration\nsolution to build Android apps. An Android APK gets exposed as a build product and can be installed on any Android\ndevice with a web browser by navigating to the build result page.\n\nSpawning emulator instancesFor testing purposes, it can also be quite convenient to automatically generate scripts that\nspawn emulator instances with all desired configuration settings.\n\nAn emulator spawn script can be configured by invoking the emulateApp {} function:\n\nwith import <nixpkgs> {};\n\nandroidenv.emulateApp {\n  name = \"emulate-MyAndroidApp\";\n  platformVersion = \"28\";\n  abiVersion = \"x86\"; # armeabi-v7a, mips, x86_64\n  systemImageType = \"google_apis_playstore\";\n}\n\nAdditional flags may be applied to the Android SDK's emulator through the runtime environment variable\n$NIX_ANDROID_EMULATOR_FLAGS.\n\nIt is also possible to specify an APK to deploy inside the emulator and the package and activity names to launch it:\n\nwith import <nixpkgs> {};\n\nandroidenv.emulateApp {\n  name = \"emulate-MyAndroidApp\";\n  platformVersion = \"24\";\n  abiVersion = \"armeabi-v7a\"; # mips, x86, x86_64\n  systemImageType = \"default\";\n  useGoogleAPIs = false;\n  app = .\/MyApp.apk;\n  package = \"MyApp\";\n  activity = \"MainActivity\";\n}\n\nIn addition to prebuilt APKs, you can also bind the APK parameter to a buildApp {} function invocation shown in the\nprevious example.\n\nNotes on environment variables in Android projects  - ANDROID_SDK_ROOT should point to the Android SDK. In your Nix\n    expressions, this should be ${androidComposition.androidsdk}\/libexec\/android-sdk. Note that ANDROID_HOME is\n    deprecated, but if you rely on tools that need it, you can export it too.\n  - ANDROID_NDK_ROOT should point to the Android NDK, if you're doing NDK development. In your Nix expressions, this\n    should be ${ANDROID_SDK_ROOT}\/ndk-bundle.\n\nIf you are running the Android Gradle plugin, you need to export GRADLE_OPTS to override aapt2 to point to the aapt2\nbinary in the Nix store as well, or use a FHS environment so the packaged aapt2 can run. If you don't want to use a FHS\nenvironment, something like this should work:\n\nlet\n  buildToolsVersion = \"30.0.3\";\n\n  # Use buildToolsVersion when you define androidComposition\n  androidComposition = <...>;\nin\npkgs.mkShell rec {\n  ANDROID_SDK_ROOT = \"${androidComposition.androidsdk}\/libexec\/android-sdk\";\n  ANDROID_NDK_ROOT = \"${ANDROID_SDK_ROOT}\/ndk-bundle\";\n\n  # Use the same buildToolsVersion here\n  GRADLE_OPTS = \"-Dorg.gradle.project.android.aapt2FromMavenOverride=${ANDROID_SDK_ROOT}\/build-tools\/${buildToolsVersion}\/aapt2\";\n}\n\nIf you are using cmake, you need to add it to PATH in a shell hook or FHS env profile. The path is suffixed with a build\nnumber, but properly prefixed with the version. So, something like this should suffice:\n\nlet\n  cmakeVersion = \"3.10.2\";\n\n  # Use cmakeVersion when you define androidComposition\n  androidComposition = <...>;\nin\npkgs.mkShell rec {\n  ANDROID_SDK_ROOT = \"${androidComposition.androidsdk}\/libexec\/android-sdk\";\n  ANDROID_NDK_ROOT = \"${ANDROID_SDK_ROOT}\/ndk-bundle\";\n\n  # Use the same cmakeVersion here\n  shellHook = ''\n    export PATH=\"$(echo \"$ANDROID_SDK_ROOT\/cmake\/${cmakeVersion}\".*\/bin):$PATH\"\n  '';\n}\n\nNote that running Android Studio with ANDROID_SDK_ROOT set will automatically write a local.properties file with sdk.dir\nset to $ANDROID_SDK_ROOT if one does not already exist. If you are using the NDK as well, you may have to add ndk.dir to\nthis file.\n\nAn example shell.nix that does all this for you is provided in examples\/shell.nix. This shell.nix includes a shell hook\nthat overwrites local.properties with the correct sdk.dir and ndk.dir values. This will ensure that the SDK and NDK\ndirectories will both be correct when you run Android Studio inside nix-shell.\n\nNotes on improving build.gradle compatibilityEnsure that your buildToolsVersion and ndkVersion match what is declared in\nandroidenv. If you are using cmake, make sure its declared version is correct too.\n\nOtherwise, you may get cryptic errors from aapt2 and the Android Gradle plugin warning that it cannot install the build\ntools because the SDK directory is not writeable.\n\nandroid {\n    buildToolsVersion \"30.0.3\"\n    ndkVersion = \"22.0.7026061\"\n    externalNativeBuild {\n        cmake {\n            version \"3.10.2\"\n        }\n    }\n}\n\nQuerying the available versions of each pluginrepo.json provides all the options in one file now.\n\nA shell script in the pkgs\/development\/mobile\/androidenv\/ subdirectory can be used to retrieve all possible options:\n\n.\/querypackages.sh packages\n\nThe above command-line instruction queries all package versions in repo.json.\n\nUpdating the generated expressionsrepo.json is generated from XML files that the Android Studio package manager uses. To\nupdate the expressions run the generate.sh script that is stored in the pkgs\/development\/mobile\/androidenv\/\nsubdirectory:\n\n.\/generate.sh\n" }
,{ "url": "languages-frameworks\/beam\/", "title": "BEAM Languages (Erlang, Elixir & LFE)", "text": "BEAM Languages (Erlang, Elixir & LFE)IntroductionIn this document and related Nix expressions, we use the term, BEAM, to\ndescribe the environment. BEAM is the name of the Erlang Virtual Machine and, as far as we're concerned, from a\npackaging perspective, all languages that run on the BEAM are interchangeable. That which varies, like the build system,\nis transparent to users of any given BEAM package, so we make no distinction.\n\nAvailable versions and deprecations scheduleElixirnixpkgs follows the official elixir deprecation schedule and keeps the\nlast 5 released versions of Elixir available.\n\nStructureAll BEAM-related expressions are available via the top-level beam attribute, which includes:\n\n  - interpreters: a set of compilers running on the BEAM, including multiple Erlang\/OTP versions\n    (beam.interpreters.erlangR22, etc), Elixir (beam.interpreters.elixir) and LFE (Lisp Flavoured Erlang)\n    (beam.interpreters.lfe).\n\n  - packages: a set of package builders (Mix and rebar3), each compiled with a specific Erlang\/OTP version, e.g.\n    beam.packages.erlang22.\n\nThe default Erlang compiler, defined by beam.interpreters.erlang, is aliased as erlang. The default BEAM package set is\ndefined by beam.packages.erlang and aliased at the top level as beamPackages.\n\nTo create a package builder built with a custom Erlang version, use the lambda, beam.packagesWith, which accepts an\nErlang\/OTP derivation and produces a package builder similar to beam.packages.erlang.\n\nMany Erlang\/OTP distributions available in beam.interpreters have versions with ODBC and\/or Java enabled or without wx\n(no observer support). For example, there's beam.interpreters.erlangR22_odbc_javac, which corresponds to\nbeam.interpreters.erlangR22 and beam.interpreters.erlangR22_nox, which corresponds to beam.interpreters.erlangR22.\n\nBuild ToolsRebar3We provide a version of Rebar3, under rebar3. We also provide a helper to fetch Rebar3 dependencies\nfrom a lockfile under fetchRebar3Deps.\n\nWe also provide a version on Rebar3 with plugins included, under rebar3WithPlugins. This package is a function which\ntakes two arguments: plugins, a list of nix derivations to include as plugins (loaded only when specified in\nrebar.config), and globalPlugins, which should always be loaded by rebar3. Example: rebar3WithPlugins { globalPlugins =\n[beamPackages.pc]; }.\n\nWhen adding a new plugin it is important that the packageName attribute is the same as the atom used by rebar3 to refer\nto the plugin.\n\nMix & Erlang.mkErlang.mk works exactly as expected. There is a bootstrap process that needs to be run, which is\nsupported by the buildErlangMk derivation.\n\nFor Elixir applications use mixRelease to make a release. See examples for more details.\n\nThere is also a buildMix helper, whose behavior is closer to that of buildErlangMk and buildRebar3. The primary\ndifference is that mixRelease makes a release, while buildMix only builds the package, making it useful for libraries\nand other dependencies.\n\nHow to Install BEAM PackagesBEAM builders are not registered at the top level, simply because they are not relevant to\nthe vast majority of Nix users. To install any of those builders into your profile, refer to them by their attribute\npath beamPackages.rebar3:\n\n$ nix-env -f \"<nixpkgs>\" -iA beamPackages.rebar3\n\nPackaging BEAM ApplicationsErlang ApplicationsRebar3 PackagesThe Nix function, buildRebar3, defined in\nbeam.packages.erlang.buildRebar3 and aliased at the top level, can be used to build a derivation that understands how to\nbuild a Rebar3 project.\n\nIf a package needs to compile native code via Rebar3's port compilation mechanism, add compilePort = true; to the\nderivation.\n\nErlang.mk PackagesErlang.mk functions similarly to Rebar3, except we use buildErlangMk instead of buildRebar3.\n\nMix PackagesmixRelease is used to make a release in the mix sense. Dependencies will need to be fetched with\nfetchMixDeps and passed to it.\n\nmixRelease - Elixir Phoenix exampleHere is how your default.nix file would look.\n\nwith import <nixpkgs> { };\n\nlet\n  packages = beam.packagesWith beam.interpreters.erlang;\n  src = builtins.fetchgit {\n    url = \"ssh:\/\/git@github.com\/your_id\/your_repo\";\n    rev = \"replace_with_your_commit\";\n  };\n\n  pname = \"your_project\";\n  version = \"0.0.1\";\n  mixEnv = \"prod\";\n\n  mixFodDeps = packages.fetchMixDeps {\n    pname = \"mix-deps-${pname}\";\n    inherit src mixEnv version;\n    # nix will complain and tell you the right value to replace this with\n    sha256 = lib.fakeSha256;\n    # if you have build time environment variables add them here\n    MY_ENV_VAR=\"my_value\";\n  };\n\n  nodeDependencies = (pkgs.callPackage .\/assets\/default.nix { }).shell.nodeDependencies;\n\n  frontEndFiles = stdenvNoCC.mkDerivation {\n    pname = \"frontend-${pname}\";\n\n    nativeBuildInputs = [ nodejs ];\n\n    inherit version src;\n\n    buildPhase = ''\n      cp -r .\/assets $TEMPDIR\n\n      mkdir -p $TEMPDIR\/assets\/node_modules\/.cache\n      cp -r ${nodeDependencies}\/lib\/node_modules $TEMPDIR\/assets\n      export PATH=\"${nodeDependencies}\/bin:$PATH\"\n\n      cd $TEMPDIR\/assets\n      webpack --config .\/webpack.config.js\n      cd ..\n    '';\n\n    installPhase = ''\n      cp -r .\/priv\/static $out\/\n    '';\n\n    outputHashAlgo = \"sha256\";\n    outputHashMode = \"recursive\";\n    # nix will complain and tell you the right value to replace this with\n    outputHash = lib.fakeSha256;\n\n    impureEnvVars = lib.fetchers.proxyImpureEnvVars;\n  };\n\n\nin packages.mixRelease {\n  inherit src pname version mixEnv mixFodDeps;\n  # if you have build time environment variables add them here\n  MY_ENV_VAR=\"my_value\";\n  preInstall = ''\n    mkdir -p .\/priv\/static\n    cp -r ${frontEndFiles} .\/priv\/static\n  '';\n}\n\nSetup will require the following steps:\n\n  - Move your secrets to runtime environment variables. For more information refer to the runtime.exs docs. On a fresh\n    Phoenix build that would mean that both DATABASE_URL and SECRET_KEY need to be moved to runtime.exs.\n  - cd assets and nix-shell -p node2nix --run node2nix --development will generate a Nix expression containing your\n    frontend dependencies\n  - commit and push those changes\n  - you can now nix-build .\n  - To run the release, set the RELEASE_TMP environment variable to a directory that your program has write access to.\n    It will be used to store the BEAM settings.\n\nExample of creating a service for an Elixir - Phoenix projectIn order to create a service with your release, you could\nadd a service.nix in your project with the following\n\n{config, pkgs, lib, ...}:\n\nlet\n  release = pkgs.callPackage .\/default.nix;\n  release_name = \"app\";\n  working_directory = \"\/home\/app\";\nin\n{\n  systemd.services.${release_name} = {\n    wantedBy = [ \"multi-user.target\" ];\n    after = [ \"network.target\" \"postgresql.service\" ];\n    requires = [ \"network-online.target\" \"postgresql.service\" ];\n    description = \"my app\";\n    environment = {\n      # RELEASE_TMP is used to write the state of the\n      # VM configuration when the system is running\n      # it needs to be a writable directory\n      RELEASE_TMP = working_directory;\n      # can be generated in an elixir console with\n      # Base.encode32(:crypto.strong_rand_bytes(32))\n      RELEASE_COOKIE = \"my_cookie\";\n      MY_VAR = \"my_var\";\n    };\n    serviceConfig = {\n      Type = \"exec\";\n      DynamicUser = true;\n      WorkingDirectory = working_directory;\n      # Implied by DynamicUser, but just to emphasize due to RELEASE_TMP\n      PrivateTmp = true;\n      ExecStart = ''\n        ${release}\/bin\/${release_name} start\n      '';\n      ExecStop = ''\n        ${release}\/bin\/${release_name} stop\n      '';\n      ExecReload = ''\n        ${release}\/bin\/${release_name} restart\n      '';\n      Restart = \"on-failure\";\n      RestartSec = 5;\n      StartLimitBurst = 3;\n      StartLimitInterval = 10;\n    };\n    # disksup requires bash\n    path = [ pkgs.bash ];\n  };\n\n  environment.systemPackages = [ release ];\n}\n\nHow to DevelopCreating a ShellUsually, we need to create a shell.nix file and do our development inside of the\nenvironment specified therein. Just install your version of Erlang and any other interpreters, and then use your normal\nbuild tools. As an example with Elixir:\n\n{ pkgs ? import \"<nixpkgs\"> {} }:\n\nwith pkgs;\n\nlet\n\n  elixir = beam.packages.erlangR22.elixir_1_9;\n\nin\nmkShell {\n  buildInputs = [ elixir ];\n\n  ERL_INCLUDE_PATH=\"${erlang}\/lib\/erlang\/usr\/include\";\n}\n\nElixir - Phoenix projectHere is an example shell.nix.\n\nwith import <nixpkgs> { };\n\nlet\n  # define packages to install\n  basePackages = [\n    git\n    # replace with beam.packages.erlang.elixir_1_11 if you need\n    beam.packages.erlang.elixir\n    nodejs-15_x\n    postgresql_13\n    # only used for frontend dependencies\n    # you are free to use yarn2nix as well\n    nodePackages.node2nix\n    # formatting js file\n    nodePackages.prettier\n  ];\n\n  inputs = basePackages ++ lib.optionals stdenv.isLinux [ inotify-tools ]\n    ++ lib.optionals stdenv.isDarwin\n    (with darwin.apple_sdk.frameworks; [ CoreFoundation CoreServices ]);\n\n  # define shell startup command\n  hooks = ''\n    # this allows mix to work on the local directory\n    mkdir -p .nix-mix .nix-hex\n    export MIX_HOME=$PWD\/.nix-mix\n    export HEX_HOME=$PWD\/.nix-mix\n    export PATH=$MIX_HOME\/bin:$HEX_HOME\/bin:$PATH\n    # TODO: not sure how to make hex available without installing it afterwards.\n    mix local.hex --if-missing\n    export LANG=en_US.UTF-8\n    export ERL_AFLAGS=\"-kernel shell_history enabled\"\n\n    # postges related\n    # keep all your db data in a folder inside the project\n    export PGDATA=\"$PWD\/db\"\n\n    # phoenix related env vars\n    export POOL_SIZE=15\n    export DB_URL=\"postgresql:\/\/postgres:postgres@localhost:5432\/db\"\n    export PORT=4000\n    export MIX_ENV=dev\n    # add your project env vars here, word readable in the nix store.\n    export ENV_VAR=\"your_env_var\"\n  '';\n\nin mkShell {\n  buildInputs = inputs;\n  shellHook = hooks;\n}\n\nInitializing the project will require the following steps:\n\n  - create the db directory initdb .\/db (inside your mix project folder)\n  - create the postgres user createuser postgres -ds\n  - create the db createdb db\n  - start the postgres instance pg_ctl -l \"$PGDATA\/server.log\" start\n  - add the \/db folder to your .gitignore\n  - you can start your phoenix server and get a shell with iex -S mix phx.server\n" }
,{ "url": "languages-frameworks\/bower\/", "title": "Bower", "text": "BowerBower is a package manager for web site front-end components. Bower packages (comprising of build artefacts and\nsometimes sources) are stored in git repositories, typically on Github. The package registry is run by the Bower team\nwith package metadata coming from the bower.json file within each package.\n\nThe end result of running Bower is a bower_components directory which can be included in the web app's build process.\n\nBower can be run interactively, by installing nodePackages.bower. More interestingly, the Bower components can be\ndeclared in a Nix derivation, with the help of nodePackages.bower2nix.\n\nbower2nix usageSuppose you have a bower.json with the following contents:\n\nExample bower.json\n\n  \"name\": \"my-web-app\",\n  \"dependencies\": {\n    \"angular\": \"~1.5.0\",\n    \"bootstrap\": \"~3.3.6\"\n  }\n\nRunning bower2nix will produce something like the following output:\n\n{ fetchbower, buildEnv }:\nbuildEnv { name = \"bower-env\"; ignoreCollisions = true; paths = [\n  (fetchbower \"angular\" \"1.5.3\" \"~1.5.0\" \"1749xb0firxdra4rzadm4q9x90v6pzkbd7xmcyjk6qfza09ykk9y\")\n  (fetchbower \"bootstrap\" \"3.3.6\" \"~3.3.6\" \"1vvqlpbfcy0k5pncfjaiskj3y6scwifxygfqnw393sjfxiviwmbv\")\n  (fetchbower \"jquery\" \"2.2.2\" \"1.9.1 - 2\" \"10sp5h98sqwk90y4k6hbdviwqzvzwqf47r3r51pakch5ii2y7js1\")\n];\n\nUsing the bower2nix command line arguments, the output can be redirected to a file. A name like bower-packages.nix would\nbe fine.\n\nThe resulting derivation is a union of all the downloaded Bower packages (and their dependencies). To use it, they still\nneed to be linked together by Bower, which is where buildBowerComponents is useful.\n\nbuildBowerComponents functionThe function is implemented in pkgs\/development\/bower-modules\/generic\/default.nix.\n\nExample buildBowerComponents\n\n<programlisting language=\"nix\">\nbowerComponents = buildBowerComponents {\n  name = \"my-web-app\";\n  generated = .\/bower-packages.nix; <co xml:id=\"ex-buildBowerComponents-1\" \/>\n  src = myWebApp; <co xml:id=\"ex-buildBowerComponents-2\" \/>\n};\n<\/programlisting>\n\nIn \"buildBowerComponents\" example the following arguments are of special significance to the function:\n\n<calloutlist>\n  <callout arearefs=\"ex-buildBowerComponents-1\">\n    <para>\n      <varname>generated<\/varname> specifies the file which was created by <command>bower2nix<\/command>.\n    <\/para>\n    <\/callout>\n      <callout arearefs=\"ex-buildBowerComponents-2\">\n    <para>\n      <varname>src<\/varname> is your project's sources. It needs to contain a <filename>bower.json<\/filename> file.\n    <\/para>\n  <\/callout>\n<\/calloutlist>\n\nbuildBowerComponents will run Bower to link together the output of bower2nix, resulting in a bower_components directory\nwhich can be used.\n\nHere is an example of a web frontend build process using gulp. You might use grunt, or anything else.\n\nExample build script (gulpfile.js)\n\nvar gulp = require('gulp');\n\ngulp.task('default', [], function () {\n  gulp.start('build');\n});\n\ngulp.task('build', [], function () {\n  console.log(\"Just a dummy gulp build\");\n  gulp\n    .src([\".\/bower_components\/**\/*\"])\n    .pipe(gulp.dest(\".\/gulpdist\/\"));\n});\n\nExample Full example — default.nix\n\n<programlisting language=\"nix\">\n{ myWebApp ? { outPath = .\/.; name = \"myWebApp\"; }\n, pkgs ? import &lt;nixpkgs&gt; {}\n}:\n\npkgs.stdenv.mkDerivation {\n  name = \"my-web-app-frontend\";\n  src = myWebApp;\n\n  buildInputs = [ pkgs.nodePackages.gulp ];\n\n  bowerComponents = pkgs.buildBowerComponents { <co xml:id=\"ex-buildBowerComponentsDefault-1\" \/>\n    name = \"my-web-app\";\n    generated = .\/bower-packages.nix;\n    src = myWebApp;\n  };\n\n  buildPhase = ''\n    cp --reflink=auto --no-preserve=mode -R $bowerComponents\/bower_components . <co xml:id=\"ex-buildBowerComponentsDefault-2\" \/>\n    export HOME=$PWD <co xml:id=\"ex-buildBowerComponentsDefault-3\" \/>\n    ${pkgs.nodePackages.gulp}\/bin\/gulp build <co xml:id=\"ex-buildBowerComponentsDefault-4\" \/>\n  '';\n\n  installPhase = \"mv gulpdist $out\";\n}\n<\/programlisting>\n\nA few notes about Full example — default.nix:\n\n<calloutlist>\n  <callout arearefs=\"ex-buildBowerComponentsDefault-1\">\n    <para>\n      The result of <varname>buildBowerComponents<\/varname> is an input to the frontend build.\n    <\/para>\n  <\/callout>\n  <callout arearefs=\"ex-buildBowerComponentsDefault-2\">\n    <para>\n      Whether to symlink or copy the <filename>bower_components<\/filename> directory depends on the build tool in use. In this case a copy is used to avoid <command>gulp<\/command> silliness with permissions.\n    <\/para>\n  <\/callout>\n  <callout arearefs=\"ex-buildBowerComponentsDefault-3\">\n    <para>\n      <command>gulp<\/command> requires <varname>HOME<\/varname> to refer to a writeable directory.\n    <\/para>\n  <\/callout>\n  <callout arearefs=\"ex-buildBowerComponentsDefault-4\">\n    <para>\n      The actual build command. Other tools could be used.\n    <\/para>\n  <\/callout>\n<\/calloutlist>\n\nTroubleshootingENOCACHE errors from buildBowerComponentsThis means that Bower was looking for a package version which\ndoesn't exist in the generated bower-packages.nix.\n\nIf bower.json has been updated, then run bower2nix again.\n\nIt could also be a bug in bower2nix or fetchbower. If possible, try reformulating the version specification in\nbower.json.\n" }
,{ "url": "languages-frameworks\/coq\/", "title": "Coq and coq packages", "text": "Coq and coq packagesCoq derivation: coqThe Coq derivation is overridable through the coq.override overrides, where\noverrides is an attribute set which contains the arguments to override. We recommend overriding either of the following\n\n  - version (optional, defaults to the latest version of Coq selected for nixpkgs, see pkgs\/top-level\/coq-packages to\n    witness this choice), which follows the conventions explained in the coqPackages section below,\n  - customOCamlPackage (optional, defaults to null, which lets Coq choose a version automatically), which can be set to\n    any of the ocaml packages attribute of ocaml-ng (such as ocaml-ng.ocamlPackages_4_10 which is the default for\n    Coq 8.11 for example).\n  - coq-version (optional, defaults to the short version e.g. \"8.10\"), is a version number of the form \"x.y\" that\n    indicates which Coq's version build behavior to mimic when using a source which is not a release. E.g. coq.override\n    { version = \"d370a9d1328a4e1cdb9d02ee032f605a9d94ec7a\"; coq-version = \"8.10\"; }.\n\nCoq packages attribute sets: coqPackagesThe recommended way of defining a derivation for a Coq library, is to use the\ncoqPackages.mkCoqDerivation function, which is essentially a specialization of mkDerivation taking into account most of\nthe specifics of Coq libraries. The following attributes are supported:\n\n  - pname (required) is the name of the package,\n  - version (optional, defaults to null), is the version to fetch and build, this attribute is interpreted in several\n    ways depending on its type and pattern:\n      - if it is a known released version string, i.e. from the release attribute below, the according release is\n        picked, and the version attribute of the resulting derivation is set to this release string,\n      - if it is a majorMinor \"x.y\" prefix of a known released version (as defined above), then the latest \"x.y.z\" known\n        released version is selected (for the ordering given by versionAtLeast),\n      - if it is a path or a string representing an absolute path (i.e. starting with \"\/\"), the provided path is\n        selected as a source, and the version attribute of the resulting derivation is set to \"dev\",\n      - if it is a string of the form owner:branch then it tries to download the branch of owner owner for a project of\n        the same name using the same vcs, and the version attribute of the resulting derivation is set to \"dev\",\n        additionally if the owner is not provided (i.e. if the owner: prefix is missing), it defaults to the original\n        owner of the package (see below),\n      - if it is a string of the form \"#N\", and the domain is github, then it tries to download the current head of the\n        pull request #N from github,\n  - defaultVersion (optional). Coq libraries may be compatible with some specific versions of Coq only. The\n    defaultVersion attribute is used when no version is provided (or if version = null) to select the version of the\n    library to use by default, depending on the context. This selection will mainly depend on a coq version number but\n    also possibly on other packages versions (e.g. mathcomp). If its value ends up to be null, the package is marked for\n    removal in end-user coqPackages attribute set.\n  - release (optional, defaults to {}), lists all the known releases of the library and for each of them provides an\n    attribute set with at least a sha256 attribute (you may put the empty string \"\" in order to automatically insert a\n    fake sha256, this will trigger an error which will allow you to find the correct sha256), each attribute set of the\n    list of releases also takes optional overloading arguments for the fetcher as below (i.e.domain, owner, repo, rev\n    assuming the default fetcher is used) and optional overrides for the result of the fetcher (i.e. version and src).\n  - fetcher (optional, defaults to a generic fetching mechanism supporting github or gitlab based infrastructures), is a\n    function that takes at least an owner, a repo, a rev, and a sha256 and returns an attribute set with a version and\n    src.\n  - repo (optional, defaults to the value of pname),\n  - owner (optional, defaults to \"coq-community\").\n  - domain (optional, defaults to \"github.com\"), domains including the strings \"github\" or \"gitlab\" in their names are\n    automatically supported, otherwise, one must change the fetcher argument to support them (cf\n    pkgs\/development\/coq-modules\/heq\/default.nix for an example),\n  - releaseRev (optional, defaults to (v: v)), provides a default mapping from release names to revision hashes\/branch\n    names\/tags,\n  - displayVersion (optional), provides a way to alter the computation of name from pname, by explaining how to display\n    version numbers,\n  - namePrefix (optional), provides a way to alter the computation of name from pname, by explaining which dependencies\n    must occur in name,\n  - extraBuildInputs (optional), by default buildInputs just contains coq, this allows to add more build inputs,\n  - mlPlugin (optional, defaults to false). Some extensions (plugins) might require OCaml and sometimes other OCaml\n    packages. Standard dependencies can be added by setting the current option to true. For a finer grain control, the\n    coq.ocamlPackages attribute can be used in extraBuildInputs to depend on the same package set Coq was built against.\n  - useDune2ifVersion (optional, default to (x: false) uses Dune2 to build the package if the provided predicate\n    evaluates to true on the version, e.g. useDune2if = versions.isGe \"1.1\" will use dune if the version of the package\n    is greater or equal to \"1.1\",\n  - useDune2 (optional, defaults to false) uses Dune2 to build the package if set to true, the presence of this\n    attribute overrides the behavior of the previous one.\n  - enableParallelBuilding (optional, defaults to true), since it is activated by default, we provide a way to disable\n    it.\n  - extraInstallFlags (optional), allows to extend installFlags which initializes the variable COQMF_COQLIB so as to\n    install in the proper subdirectory. Indeed Coq libraries should be installed in\n    $(out)\/lib\/coq\/${coq.coq-version}\/user-contrib\/. Such directories are automatically added to the $COQPATH\n    environment variable by the hook defined in the Coq derivation.\n  - setCOQBIN (optional, defaults to true), by default, the environment variable $COQBIN is set to the current Coq's\n    binary, but one can disable this behavior by setting it to false,\n  - useMelquiondRemake (optional, default to null) is an attribute set, which, if given, overloads the\n    preConfigurePhases, configureFlags, buildPhase, and installPhase attributes of the derivation for a specific use in\n    libraries using remake as set up by Guillaume Melquiond for flocq, gappalib, interval, and coquelicot (see the\n    corresponding derivation for concrete examples of use of this option). For backward compatibility, the attribute\n    useMelquiondRemake.logpath must be set to the logical root of the library (otherwise, one can pass\n    useMelquiondRemake = {} to activate this without backward compatibility).\n  - dropAttrs, keepAttrs, dropDerivationAttrs are all optional and allow to tune which attribute is added or removed\n    from the final call to mkDerivation.\n\nIt also takes other standard mkDerivation attributes, they are added as such, except for meta which extends an\nautomatically computed meta (where the platform is the same as coq and the homepage is automatically computed).\n\nHere is a simple package example. It is a pure Coq library, thus it depends on Coq. It builds on the Mathematical\nComponents library, thus it also takes some mathcomp derivations as extraBuildInputs.\n\n{ lib, mkCoqDerivation, version ? null\n, coq, mathcomp, mathcomp-finmap, mathcomp-bigenough }:\nwith lib; mkCoqDerivation {\n  \/* namePrefix leads to e.g. `name = coq8.11-mathcomp1.11-multinomials-1.5.2` *\/\n  namePrefix = [ \"coq\" \"mathcomp\" ];\n  pname = \"multinomials\";\n  owner = \"math-comp\";\n  inherit version;\n  defaultVersion =  with versions; switch [ coq.version mathcomp.version ] [\n      { cases = [ (range \"8.7\" \"8.12\")  \"1.11.0\" ];             out = \"1.5.2\"; }\n      { cases = [ (range \"8.7\" \"8.11\")  (range \"1.8\" \"1.10\") ]; out = \"1.5.0\"; }\n      { cases = [ (range \"8.7\" \"8.10\")  (range \"1.8\" \"1.10\") ]; out = \"1.4\"; }\n      { cases = [ \"8.6\"                 (range \"1.6\" \"1.7\") ];  out = \"1.1\"; }\n    ] null;\n  release = {\n    \"1.5.2\".sha256 = \"15aspf3jfykp1xgsxf8knqkxv8aav2p39c2fyirw7pwsfbsv2c4s\";\n    \"1.5.1\".sha256 = \"13nlfm2wqripaq671gakz5mn4r0xwm0646araxv0nh455p9ndjs3\";\n    \"1.5.0\".sha256 = \"064rvc0x5g7y1a0nip6ic91vzmq52alf6in2bc2dmss6dmzv90hw\";\n    \"1.5.0\".rev    = \"1.5\";\n    \"1.4\".sha256   = \"0vnkirs8iqsv8s59yx1fvg1nkwnzydl42z3scya1xp1b48qkgn0p\";\n    \"1.3\".sha256   = \"0l3vi5n094nx3qmy66hsv867fnqm196r8v605kpk24gl0aa57wh4\";\n    \"1.2\".sha256   = \"1mh1w339dslgv4f810xr1b8v2w7rpx6fgk9pz96q0fyq49fw2xcq\";\n    \"1.1\".sha256   = \"1q8alsm89wkc0lhcvxlyn0pd8rbl2nnxg81zyrabpz610qqjqc3s\";\n    \"1.0\".sha256   = \"1qmbxp1h81cy3imh627pznmng0kvv37k4hrwi2faa101s6bcx55m\";\n  };\n\n  propagatedBuildInputs =\n    [ mathcomp.ssreflect mathcomp.algebra mathcomp-finmap mathcomp-bigenough ];\n\n  meta = {\n    description = \"A Coq\/SSReflect Library for Monoidal Rings and Multinomials\";\n    license = licenses.cecill-c;\n  };\n}\n" }
,{ "url": "languages-frameworks\/crystal\/", "title": "Crystal", "text": "CrystalBuilding a Crystal packageThis section uses Mint as an example for how to build a Crystal package.\n\nIf the Crystal project has any dependencies, the first step is to get a shards.nix file encoding those. Get a copy of\nthe project and go to its root directory such that its shard.lock file is in the current directory, then run crystal2nix\nin it\n\n$ git clone https:\/\/github.com\/mint-lang\/mint\n$ cd mint\n$ git checkout 0.5.0\n$ nix-shell -p crystal2nix --run crystal2nix\n\nThis should have generated a shards.nix file.\n\nNext create a Nix file for your derivation and use pkgs.crystal.buildCrystalPackage as follows:\n\nwith import <nixpkgs> {};\ncrystal.buildCrystalPackage rec {\n  pname = \"mint\";\n  version = \"0.5.0\";\n\n  src = fetchFromGitHub {\n    owner = \"mint-lang\";\n    repo = \"mint\";\n    rev = version;\n    sha256 = \"0vxbx38c390rd2ysvbwgh89v2232sh5rbsp3nk9wzb70jybpslvl\";\n  };\n\n  # Insert the path to your shards.nix file here\n  shardsFile = .\/shards.nix;\n\n  ...\n}\n\nThis won't build anything yet, because we haven't told it what files build. We can specify a mapping from binary names\nto source files with the crystalBinaries attribute. The project's compilation instructions should show this. For Mint,\nthe binary is called \"mint\", which is compiled from the source file src\/mint.cr, so we'll specify this as follows:\n\n  crystalBinaries.mint.src = \"src\/mint.cr\";\n\n  # ...\n\nAdditionally you can override the default crystal build options (which are currently --release --progress --no-debug\n--verbose) with\n\n  crystalBinaries.mint.options = [ \"--release\" \"--verbose\" ];\n\nDepending on the project, you might need additional steps to get it to compile successfully. In Mint's case, we need to\nlink against openssl, so in the end the Nix file looks as follows:\n\nwith import <nixpkgs> {};\ncrystal.buildCrystalPackage rec {\n  version = \"0.5.0\";\n  pname = \"mint\";\n  src = fetchFromGitHub {\n    owner = \"mint-lang\";\n    repo = \"mint\";\n    rev = version;\n    sha256 = \"0vxbx38c390rd2ysvbwgh89v2232sh5rbsp3nk9wzb70jybpslvl\";\n  };\n\n  shardsFile = .\/shards.nix;\n  crystalBinaries.mint.src = \"src\/mint.cr\";\n\n  buildInputs = [ openssl ];\n}\n" }
,{ "url": "languages-frameworks\/dhall\/", "title": "Dhall", "text": "DhallThe Nixpkgs support for Dhall assumes some familiarity with Dhall's language support for importing Dhall\nexpressions, which is documented here:\n\n  - dhall-lang.org - Installing packages\n\nRemote importsNixpkgs bypasses Dhall's support for remote imports using Dhall's semantic integrity checks. Specifically,\nany Dhall import can be protected by an integrity check like:\n\nhttps:\/\/prelude.dhall-lang.org\/v20.1.0\/package.dhall\n  sha256:26b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n\n… and if the import is cached then the interpreter will load the import from cache instead of fetching the URL.\n\nNixpkgs uses this trick to add all of a Dhall expression's dependencies into the cache so that the Dhall interpreter\nnever needs to resolve any remote URLs. In fact, Nixpkgs uses a Dhall interpreter with remote imports disabled when\npackaging Dhall expressions to enforce that the interpreter never resolves a remote import. This means that Nixpkgs only\nsupports building Dhall expressions if all of their remote imports are protected by semantic integrity checks.\n\nInstead of remote imports, Nixpkgs uses Nix to fetch remote Dhall code. For example, the Prelude Dhall package uses\npkgs.fetchFromGitHub to fetch the dhall-lang repository containing the Prelude. Relying exclusively on Nix to fetch\nDhall code ensures that Dhall packages built using Nix remain pure and also behave well when built within a sandbox.\n\nPackaging a Dhall expression from scratchWe can illustrate how Nixpkgs integrates Dhall by beginning from the following\ntrivial Dhall expression with one dependency (the Prelude):\n\n-- .\/true.dhall\n\nlet Prelude = https:\/\/prelude.dhall-lang.org\/v20.1.0\/package.dhall\n\nin  Prelude.Bool.not False\n\nAs written, this expression cannot be built using Nixpkgs because the expression does not protect the Prelude import\nwith a semantic integrity check, so the first step is to freeze the expression using dhall freeze, like this:\n\n$ dhall freeze --inplace .\/true.dhall\n\n… which gives us:\n\n-- .\/true.dhall\n\nlet Prelude =\n      https:\/\/prelude.dhall-lang.org\/v20.1.0\/package.dhall\n        sha256:26b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n\nin  Prelude.Bool.not False\n\nTo package that expression, we create a .\/true.nix file containing the following specification for the Dhall package:\n\n# .\/true.nix\n\n{ buildDhallPackage, Prelude }:\n\nbuildDhallPackage {\n  name = \"true\";\n  code = .\/true.dhall;\n  dependencies = [ Prelude ];\n  source = true;\n}\n\n… and we complete the build by incorporating that Dhall package into the pkgs.dhallPackages hierarchy using an overlay,\nlike this:\n\n# .\/example.nix\n\nlet\n  nixpkgs = builtins.fetchTarball {\n    url    = \"https:\/\/github.com\/NixOS\/nixpkgs\/archive\/94b2848559b12a8ed1fe433084686b2a81123c99.tar.gz\";\n    sha256 = \"1pbl4c2dsaz2lximgd31m96jwbps6apn3anx8cvvhk1gl9rkg107\";\n  };\n\n  dhallOverlay = self: super: {\n    true = self.callPackage .\/true.nix { };\n  };\n\n  overlay = self: super: {\n    dhallPackages = super.dhallPackages.override (old: {\n      overrides =\n        self.lib.composeExtensions (old.overrides or (_: _: {})) dhallOverlay;\n    });\n  };\n\n  pkgs = import nixpkgs { config = {}; overlays = [ overlay ]; };\n\nin\n  pkgs\n\n… which we can then build using this command:\n\n$ nix build --file .\/example.nix dhallPackages.true\n\nContents of a Dhall packageThe above package produces the following directory tree:\n\n$ tree -a .\/result\nresult\n├── .cache\n│   └── dhall\n│       └── 122027abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\n├── binary.dhall\n└── source.dhall\n\n… where:\n\n  - source.dhall contains the result of interpreting our Dhall package:\n    \n    $ cat .\/result\/source.dhall\n    True\n\n  - The .cache subdirectory contains one binary cache product encoding the same result as source.dhall:\n    \n    $ dhall decode < .\/result\/.cache\/dhall\/122027abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\n    True\n\n  - binary.dhall contains a Dhall expression which handles fetching and decoding the same cache product:\n    \n    $ cat .\/result\/binary.dhall\n    missing sha256:27abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\n    $ cp -r .\/result\/.cache .cache\n    \n    $ chmod -R u+w .cache\n    \n    $ XDG_CACHE_HOME=.cache dhall --file .\/result\/binary.dhall\n    True\n\nThe source.dhall file is only present for packages that specify source = true;. By default, Dhall packages omit the\nsource.dhall in order to conserve disk space when they are used exclusively as dependencies. For example, if we build\nthe Prelude package it will only contain the binary encoding of the expression:\n\n$ nix build --file .\/example.nix dhallPackages.Prelude\n\n$ tree -a result\nresult\n├── .cache\n│   └── dhall\n│       └── 122026b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n└── binary.dhall\n\n2 directories, 2 files\n\nTypically, you only specify source = true; for the top-level Dhall expression of interest (such as our example true.nix\nDhall package). However, if you wish to specify source = true for all Dhall packages, then you can amend the Dhall\noverlay like this:\n\n  dhallOverrides = self: super: {\n    # Enable source for all Dhall packages\n    buildDhallPackage =\n      args: super.buildDhallPackage (args \/\/ { source = true; });\n\n    true = self.callPackage .\/true.nix { };\n  };\n\n… and now the Prelude will contain the fully decoded result of interpreting the Prelude:\n\n$ nix build --file .\/example.nix dhallPackages.Prelude\n\n$ tree -a result\nresult\n├── .cache\n│   └── dhall\n│       └── 122026b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n├── binary.dhall\n└── source.dhall\n\n$ cat .\/result\/source.dhall\n{ Bool =\n  { and =\n      \\(_ : List Bool) ->\n        List\/fold Bool _ Bool (\\(_ : Bool) -> \\(_ : Bool) -> _@1 && _) True\n  , build = \\(_ : Type -> _ -> _@1 -> _@2) -> _ Bool True False\n  , even =\n      \\(_ : List Bool) ->\n        List\/fold Bool _ Bool (\\(_ : Bool) -> \\(_ : Bool) -> _@1 == _) True\n  , fold =\n      \\(_ : Bool) ->\n…\n\nPackaging functionsWe already saw an example of using buildDhallPackage to create a Dhall package from a single file,\nbut most Dhall packages consist of more than one file and there are two derived utilities that you may find more useful\nwhen packaging multiple files:\n\n  - buildDhallDirectoryPackage - build a Dhall package from a local directory\n\n  - buildDhallGitHubPackage - build a Dhall package from a GitHub repository\n\nThe buildDhallPackage is the lowest-level function and accepts the following arguments:\n\n  - name: The name of the derivation\n\n  - dependencies: Dhall dependencies to build and cache ahead of time\n\n  - code: The top-level expression to build for this package\n    \n    Note that the code field accepts an arbitrary Dhall expression. You're not limited to just a file.\n\n  - source: Set to true to include the decoded result as source.dhall in the build product, at the expense of requiring\n    more disk space\n\n  - documentationRoot: Set to the root directory of the package if you want dhall-docs to generate documentation\n    underneath the docs subdirectory of the build product\n\nThe buildDhallDirectoryPackage is a higher-level function implemented in terms of buildDhallPackage that accepts the\nfollowing arguments:\n\n  - name: Same as buildDhallPackage\n\n  - dependencies: Same as buildDhallPackage\n\n  - source: Same as buildDhallPackage\n\n  - src: The directory containing Dhall code that you want to turn into a Dhall package\n\n  - file: The top-level file (package.dhall by default) that is the entrypoint to the rest of the package\n\n  - document: Set to true to generate documentation for the package\n\nThe buildDhallGitHubPackage is another higher-level function implemented in terms of buildDhallPackage that accepts the\nfollowing arguments:\n\n  - name: Same as buildDhallPackage\n\n  - dependencies: Same as buildDhallPackage\n\n  - source: Same as buildDhallPackage\n\n  - owner: The owner of the repository\n\n  - repo: The repository name\n\n  - rev: The desired revision (or branch, or tag)\n\n  - directory: The subdirectory of the Git repository to package (if a directory other than the root of the repository)\n\n  - file: The top-level file (${directory}\/package.dhall by default) that is the entrypoint to the rest of the package\n\n  - document: Set to true to generate documentation for the package\n\nAdditionally, buildDhallGitHubPackage accepts the same arguments as fetchFromGitHub, such as sha256 or fetchSubmodules.\n\ndhall-to-nixpkgsYou can use the dhall-to-nixpkgs command-line utility to automate packaging Dhall code. For example:\n\n$ nix-env --install --attr haskellPackages.dhall-nixpkgs\n\n$ nix-env --install --attr nix-prefetch-git  # Used by dhall-to-nixpkgs\n\n$ dhall-to-nixpkgs github https:\/\/github.com\/Gabriel439\/dhall-semver.git\n{ buildDhallGitHubPackage, Prelude }:\n  buildDhallGitHubPackage {\n    name = \"dhall-semver\";\n    githubBase = \"github.com\";\n    owner = \"Gabriel439\";\n    repo = \"dhall-semver\";\n    rev = \"2d44ae605302ce5dc6c657a1216887fbb96392a4\";\n    fetchSubmodules = false;\n    sha256 = \"0y8shvp8srzbjjpmnsvz9c12ciihnx1szs0yzyi9ashmrjvd0jcz\";\n    directory = \"\";\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [ (Prelude.overridePackage { file = \"package.dhall\"; }) ];\n    }\n\nThe utility takes care of automatically detecting remote imports and converting them to package dependencies. You can\nalso use the utility on local Dhall directories, too:\n\n$ dhall-to-nixpkgs directory ~\/proj\/dhall-semver\n{ buildDhallDirectoryPackage, Prelude }:\n  buildDhallDirectoryPackage {\n    name = \"proj\";\n    src = \/Users\/gabriel\/proj\/dhall-semver;\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [ (Prelude.overridePackage { file = \"package.dhall\"; }) ];\n    }\n\nOverriding dependency versionsSuppose that we change our true.dhall example expression to depend on an older version of\nthe Prelude (19.0.0):\n\n-- .\/true.dhall\n\nlet Prelude =\n      https:\/\/prelude.dhall-lang.org\/v19.0.0\/package.dhall\n        sha256:eb693342eb769f782174157eba9b5924cf8ac6793897fc36a31ccbd6f56dafe2\n\nin  Prelude.Bool.not False\n\nIf we try to rebuild that expression the build will fail:\n\n$ nix build --file .\/example.nix dhallPackages.true\nbuilder for '\/nix\/store\/0f1hla7ff1wiaqyk1r2ky4wnhnw114fi-true.drv' failed with exit code 1; last 10 log lines:\n\n  Dhall was compiled without the 'with-http' flag.\n\n  The requested URL was: https:\/\/prelude.dhall-lang.org\/v19.0.0\/package.dhall\n\n\n  4│       https:\/\/prelude.dhall-lang.org\/v19.0.0\/package.dhall\n  5│         sha256:eb693342eb769f782174157eba9b5924cf8ac6793897fc36a31ccbd6f56dafe2\n\n  \/nix\/store\/rsab4y99h14912h4zplqx2iizr5n4rc2-true.dhall:4:7\n[1 built (1 failed), 0.0 MiB DL]\nerror: build of '\/nix\/store\/0f1hla7ff1wiaqyk1r2ky4wnhnw114fi-true.drv' failed\n\n… because the default Prelude selected by Nixpkgs revision 94b2848559b12a8ed1fe433084686b2a81123c99is is version 20.1.0,\nwhich doesn't have the same integrity check as version 19.0.0. This means that version 19.0.0 is not cached and the\ninterpreter is not allowed to fall back to importing the URL.\n\nHowever, we can override the default Prelude version by using dhall-to-nixpkgs to create a Dhall package for our desired\nPrelude:\n\n$ dhall-to-nixpkgs github https:\/\/github.com\/dhall-lang\/dhall-lang.git \\\n    --name Prelude \\\n    --directory Prelude \\\n    --rev v19.0.0 \\\n    > Prelude.nix\n\n… and then referencing that package in our Dhall overlay, by either overriding the Prelude globally for all packages,\nlike this:\n\n  dhallOverrides = self: super: {\n    true = self.callPackage .\/true.nix { };\n\n    Prelude = self.callPackage .\/Prelude.nix { };\n  };\n\n… or selectively overriding the Prelude dependency for just the true package, like this:\n\n  dhallOverrides = self: super: {\n    true = self.callPackage .\/true.nix {\n      Prelude = self.callPackage .\/Prelude.nix { };\n    };\n  };\n\nOverridesYou can override any of the arguments to buildDhallGitHubPackage or buildDhallDirectoryPackage using the\noverridePackage attribute of a package. For example, suppose we wanted to selectively enable source = true just for the\nPrelude. We can do that like this:\n\n  dhallOverrides = self: super: {\n    Prelude = super.Prelude.overridePackage { source = true; };\n\n    …\n  };\n" }
,{ "url": "languages-frameworks\/dotnet\/", "title": "Dotnet", "text": "DotnetLocal Development WorkflowFor local development, it's recommended to use nix-shell to create a dotnet environment:\n\n# shell.nix\nwith import <nixpkgs> {};\n\nmkShell {\n  name = \"dotnet-env\";\n  packages = [\n    dotnet-sdk_3\n  ];\n}\n\nUsing many sdks in a workflowIt's very likely that more than one sdk will be needed on a given project. Dotnet provides\nseveral different frameworks (E.g dotnetcore, aspnetcore, etc.) as well as many versions for a given framework.\nNormally, dotnet is able to fetch a framework and install it relative to the executable. However, this would mean\nwriting to the nix store in nixpkgs, which is read-only. To support the many-sdk use case, one can compose an\nenvironment using dotnetCorePackages.combinePackages:\n\nwith import <nixpkgs> {};\n\nmkShell {\n  name = \"dotnet-env\";\n  packages = [\n    (with dotnetCorePackages; combinePackages [\n      sdk_3_1\n      sdk_3_0\n      sdk_2_1\n    ])\n  ];\n}\n\nThis will produce a dotnet installation that has the dotnet 3.1, 3.0, and 2.1 sdk. The first sdk listed will have it's\ncli utility present in the resulting environment. Example info output:\n\n$ dotnet --info\n.NET Core SDK (reflecting any global.json):\n Version:   3.1.101\n Commit:    b377529961\n\n...\n\n.NET Core SDKs installed:\n  2.1.803 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/sdk]\n  3.0.102 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/sdk]\n  3.1.101 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/sdk]\n\n.NET Core runtimes installed:\n  Microsoft.AspNetCore.All 2.1.15 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/shared\/Microsoft.AspNetCore.All]\n  Microsoft.AspNetCore.App 2.1.15 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/shared\/Microsoft.AspNetCore.App]\n  Microsoft.AspNetCore.App 3.0.2 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/shared\/Microsoft.AspNetCore.App]\n  Microsoft.AspNetCore.App 3.1.1 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/shared\/Microsoft.AspNetCore.App]\n  Microsoft.NETCore.App 2.1.15 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/shared\/Microsoft.NETCore.App]\n  Microsoft.NETCore.App 3.0.2 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/shared\/Microsoft.NETCore.App]\n  Microsoft.NETCore.App 3.1.1 [\/nix\/store\/iiv98i2jdi226dgh4jzkkj2ww7f8jgpd-dotnet-core-combined\/shared\/Microsoft.NETCore.App]\n\ndotnet-sdk vs dotnetCorePackages.sdkThe dotnetCorePackages.sdk_X_Y is preferred over the old dotnet-sdk as both major\nand minor version are very important for a dotnet environment. If a given minor version isn't present (or was changed),\nthen this will likely break your ability to build a project.\n\ndotnetCorePackages.sdk vs dotnetCorePackages.net vs dotnetCorePackages.netcore vs dotnetCorePackages.aspnetcoreThe\ndotnetCorePackages.sdk contains both a runtime and the full sdk of a given version. The net, netcore and aspnetcore\npackages are meant to serve as minimal runtimes to deploy alongside already built applications. For runtime versions >=\n.NET 5 net is used while netcore is used for older .NET Core runtime version.\n\nPackaging a Dotnet ApplicationIdeally, we would like to build against the sdk, then only have the dotnet runtime\navailable in the runtime closure.\n\nTODO: Create closure-friendly way to package dotnet applications\n" }
,{ "url": "languages-frameworks\/emscripten\/", "title": "Emscripten", "text": "EmscriptenEmscripten: An LLVM-to-JavaScript Compiler\n\nThis section of the manual covers how to use emscripten in nixpkgs.\n\nMinimal requirements:\n\n  - nix\n  - nixpkgs\n\nModes of use of emscripten:\n\n  - Imperative usage (on the command line):\n    \n    If you want to work with emcc, emconfigure and emmake as you are used to from Ubuntu and similar distributions you\n    can use these commands:\n    \n      - nix-env -i emscripten\n      - nix-shell -p emscripten\n\n  - Declarative usage:\n    \n    This mode is far more power full since this makes use of nix for dependency management of emscripten libraries and\n    targets by using the mkDerivation which is implemented by pkgs.emscriptenStdenv and pkgs.buildEmscriptenPackage. The\n    source for the packages is in pkgs\/top-level\/emscripten-packages.nix and the abstraction behind it in\n    pkgs\/development\/em-modules\/generic\/default.nix.\n    \n      - build and install all packages:\n        \n          - nix-env -iA emscriptenPackages\n    \n      - dev-shell for zlib implementation hacking:\n        \n          - nix-shell -A emscriptenPackages.zlib\n\nImperative usageA few things to note:\n\n  - export EMCC_DEBUG=2 is nice for debugging\n  - ~\/.emscripten, the build artifact cache sometimes creates issues and needs to be removed from time to time\n\nDeclarative usageLet's see two different examples from pkgs\/top-level\/emscripten-packages.nix:\n\n  - pkgs.zlib.override\n  - pkgs.buildEmscriptenPackage\n\nBoth are interesting concepts.\n\nA special requirement of the pkgs.buildEmscriptenPackage is the doCheck = true is a default meaning that each\nemscriptenPackage requires a checkPhase implemented.\n\n  - Use export EMCC_DEBUG=2 from within a emscriptenPackage's phase to get more detailed debug output what is going\n    wrong.\n  - ~\/.emscripten cache is requiring us to set HOME=$TMPDIR in individual phases. This makes compilation slower but also\n    makes it more deterministic.\n\nUsage 1: pkgs.zlib.overrideThis example uses zlib from nixpkgs but instead of compiling C to ELF it compiles C to JS\nsince we were using pkgs.zlib.override and changed stdenv to pkgs.emscriptenStdenv. A few adaptions and hacks were set\nin place to make it working. One advantage is that when pkgs.zlib is updated, it will automatically update this package\nas well. However, this can also be the downside...\n\nSee the zlib example:\n\nzlib = (pkgs.zlib.override {\n  stdenv = pkgs.emscriptenStdenv;\n}).overrideDerivation\n(old: rec {\n  buildInputs = old.buildInputs ++ [ pkg-config ];\n  # we need to reset this setting!\n  NIX_CFLAGS_COMPILE=\"\";\n  configurePhase = ''\n    # FIXME: Some tests require writing at $HOME\n    HOME=$TMPDIR\n    runHook preConfigure\n\n    #export EMCC_DEBUG=2\n    emconfigure .\/configure --prefix=$out --shared\n\n    runHook postConfigure\n  '';\n  dontStrip = true;\n  outputs = [ \"out\" ];\n  buildPhase = ''\n    emmake make\n  '';\n  installPhase = ''\n    emmake make install\n  '';\n  checkPhase = ''\n    echo \"================= testing zlib using node =================\"\n\n    echo \"Compiling a custom test\"\n    set -x\n    emcc -O2 -s EMULATE_FUNCTION_POINTER_CASTS=1 test\/example.c -DZ_SOLO \\\n    libz.so.${old.version} -I . -o example.js\n\n    echo \"Using node to execute the test\"\n    ${pkgs.nodejs}\/bin\/node .\/example.js\n\n    set +x\n    if [ $? -ne 0 ]; then\n      echo \"test failed for some reason\"\n      exit 1;\n    else\n      echo \"it seems to work! very good.\"\n    fi\n    echo \"================= \/testing zlib using node =================\"\n  '';\n\n  postPatch = pkgs.lib.optionalString pkgs.stdenv.isDarwin ''\n    substituteInPlace configure \\\n      --replace '\/usr\/bin\/libtool' 'ar' \\\n      --replace 'AR=\"libtool\"' 'AR=\"ar\"' \\\n      --replace 'ARFLAGS=\"-o\"' 'ARFLAGS=\"-r\"'\n  '';\n});\n\nUsage 2: pkgs.buildEmscriptenPackageThis xmlmirror example features a emscriptenPackage which is defined completely from\nthis context and no pkgs.zlib.override is used.\n\nxmlmirror = pkgs.buildEmscriptenPackage rec {\n  name = \"xmlmirror\";\n\n  buildInputs = [ pkg-config autoconf automake libtool gnumake libxml2 nodejs openjdk json_c ];\n  nativeBuildInputs = [ pkg-config zlib ];\n\n  src = pkgs.fetchgit {\n    url = \"https:\/\/gitlab.com\/odfplugfest\/xmlmirror.git\";\n    rev = \"4fd7e86f7c9526b8f4c1733e5c8b45175860a8fd\";\n    sha256 = \"1jasdqnbdnb83wbcnyrp32f36w3xwhwp0wq8lwwmhqagxrij1r4b\";\n  };\n\n  configurePhase = ''\n    rm -f fastXmlLint.js*\n    # a fix for ERROR:root:For asm.js, TOTAL_MEMORY must be a multiple of 16MB, was 234217728\n    # https:\/\/gitlab.com\/odfplugfest\/xmlmirror\/issues\/8\n    sed -e \"s\/TOTAL_MEMORY=234217728\/TOTAL_MEMORY=268435456\/g\" -i Makefile.emEnv\n    # https:\/\/github.com\/kripken\/emscripten\/issues\/6344\n    # https:\/\/gitlab.com\/odfplugfest\/xmlmirror\/issues\/9\n    sed -e \"s\/\\$(JSONC_LDFLAGS) \\$(ZLIB_LDFLAGS) \\$(LIBXML20_LDFLAGS)\/\\$(JSONC_LDFLAGS) \\$(LIBXML20_LDFLAGS) \\$(ZLIB_LDFLAGS) \/g\" -i Makefile.emEnv\n    # https:\/\/gitlab.com\/odfplugfest\/xmlmirror\/issues\/11\n    sed -e \"s\/-o fastXmlLint.js\/-s EXTRA_EXPORTED_RUNTIME_METHODS='[\\\"ccall\\\", \\\"cwrap\\\"]' -o fastXmlLint.js\/g\" -i Makefile.emEnv\n  '';\n\n  buildPhase = ''\n    HOME=$TMPDIR\n    make -f Makefile.emEnv\n  '';\n\n  outputs = [ \"out\" \"doc\" ];\n\n  installPhase = ''\n    mkdir -p $out\/share\n    mkdir -p $doc\/share\/${name}\n\n    cp Demo* $out\/share\n    cp -R codemirror-5.12 $out\/share\n    cp fastXmlLint.js* $out\/share\n    cp *.xsd $out\/share\n    cp *.js $out\/share\n    cp *.xhtml $out\/share\n    cp *.html $out\/share\n    cp *.json $out\/share\n    cp *.rng $out\/share\n    cp README.md $doc\/share\/${name}\n  '';\n  checkPhase = ''\n\n  '';\n};\n\nDeclarative debuggingUse nix-shell -I nixpkgs=\/some\/dir\/nixpkgs -A emscriptenPackages.libz and from there you can go\ntrough the individual steps. This makes it easy to build a good unit test or list the files of the project.\n\n1.  nix-shell -I nixpkgs=\/some\/dir\/nixpkgs -A emscriptenPackages.libz\n2.  cd \/tmp\/\n3.  unpackPhase\n4.  cd libz-1.2.3\n5.  configurePhase\n6.  buildPhase\n7.  ... happy hacking...\n\nSummaryUsing this toolchain makes it easy to leverage nix from NixOS, MacOSX or even Windows (WSL+ubuntu+nix). This\ntoolchain is reproducible, behaves like the rest of the packages from nixpkgs and contains a set of well working\nexamples to learn and adapt from.\n\nIf in trouble, ask the maintainers.\n" }
,{ "url": "languages-frameworks\/gnome\/", "title": "GNOME", "text": "GNOMEPackaging GNOME applicationsPrograms in the GNOME universe are written in various languages but they all use\nGObject-based libraries like GLib, GTK or GStreamer. These libraries are often modular, relying on looking into certain\ndirectories to find their modules. However, due to Nix’s specific file system organization, this will fail without our\nintervention. Fortunately, the libraries usually allow overriding the directories through environment variables, either\nnatively or thanks to a patch in nixpkgs. Wrapping the executables to ensure correct paths are available to the\napplication constitutes a significant part of packaging a modern desktop application. In this section, we will describe\nvarious modules needed by such applications, environment variables needed to make the modules load, and finally a script\nthat will do the work for us.\n\nSettingsGSettings API is often used for storing settings. GSettings schemas are required, to know the type and other\nmetadata of the stored values. GLib looks for glib-2.0\/schemas\/gschemas.compiled files inside the directories of\nXDG_DATA_DIRS.\n\nOn Linux, GSettings API is implemented using dconf backend. You will need to add dconf GIO module to GIO_EXTRA_MODULES\nvariable, otherwise the memory backend will be used and the saved settings will not be persistent.\n\nLast you will need the dconf database D-Bus service itself. You can enable it using programs.dconf.enable.\n\nSome applications will also require gsettings-desktop-schemas for things like reading proxy configuration or user\ninterface customization. This dependency is often not mentioned by upstream, you should grep for org.gnome.desktop and\norg.gnome.system to see if the schemas are needed.\n\nGdkPixbuf loadersGTK applications typically use GdkPixbuf to load images. But gdk-pixbuf package only supports basic\nbitmap formats like JPEG, PNG or TIFF, requiring to use third-party loader modules for other formats. This is especially\npainful since GTK itself includes SVG icons, which cannot be rendered without a loader provided by librsvg.\n\nUnlike other libraries mentioned in this section, GdkPixbuf only supports a single value in its controlling environment\nvariable GDK_PIXBUF_MODULE_FILE. It is supposed to point to a cache file containing information about the available\nloaders. Each loader package will contain a lib\/gdk-pixbuf-2.0\/2.10.0\/loaders.cache file describing the default loaders\nin gdk-pixbuf package plus the loader contained in the package itself. If you want to use multiple third-party loaders,\nyou will need to create your own cache file manually. Fortunately, this is pretty rare as not many loaders exist.\n\ngdk-pixbuf contains a setup hook that sets GDK_PIXBUF_MODULE_FILE from dependencies but as mentioned in further section,\nit is pretty limited. Loaders should propagate this setup hook.\n\nIconsWhen an application uses icons, an icon theme should be available in XDG_DATA_DIRS during runtime. The package for\nthe default, icon-less hicolor-icon-theme (should be propagated by every icon theme) contains a setup hook that will\npick up icon themes from buildInputs and pass it to our wrapper. Unfortunately, relying on that would mean every user\nhas to download the theme included in the package expression no matter their preference. For that reason, we leave the\ninstallation of icon theme on the user. If you use one of the desktop environments, you probably already have an icon\ntheme installed.\n\nTo avoid costly file system access when locating icons, GTK, as well as Qt, can rely on icon-theme.cache files from the\nthemes' top-level directories. These files are generated using gtk-update-icon-cache, which is expected to be run\nwhenever an icon is added or removed to an icon theme (typically an application icon into hicolor theme) and some\nprograms do indeed run this after icon installation. However, since packages are installed into their own prefix by Nix,\nthis would lead to conflicts. For that reason, gtk3 provides a setup hook that will clean the file from installation.\nSince most applications only ship their own icon that will be loaded on start-up, it should not affect them too much. On\nthe other hand, icon themes are much larger and more widely used so we need to cache them. Because we recommend\ninstalling icon themes globally, we will generate the cache files from all packages in a profile using a NixOS module.\nYou can enable the cache generation using gtk.iconCache.enable option if your desktop environment does not already do\nthat.\n\nPackaging icon themesIcon themes may inherit from other icon themes. The inheritance is specified using the Inherits key\nin the index.theme file distributed with the icon theme. According to the icon theme specification, icons not provided\nby the theme are looked for in its parent icon themes. Therefore the parent themes should be installed as dependencies\nfor a more complete experience regarding the icon sets used.\n\nThe package hicolor-icon-theme provides a setup hook which makes symbolic links for the parent themes into the directory\nshare\/icons of the current theme directory in the nix store, making sure they can be found at runtime. For that to work\nthe packages providing parent icon themes should be listed as propagated build dependencies, together with\nhicolor-icon-theme.\n\nAlso make sure that icon-theme.cache is installed for each theme provided by the package, and set dontDropIconThemeCache\nto true so that the cache file is not removed by the gtk3 setup hook.\n\nGTK ThemesPreviously, a GTK theme needed to be in XDG_DATA_DIRS. This is no longer necessary for most programs since GTK\nincorporated Adwaita theme. Some programs (for example, those designed for elementary HIG) might require a special theme\nlike pantheon.elementary-gtk-theme.\n\nGObject introspection typelibsGObject introspection allows applications to use C libraries in other languages easily. It\ndoes this through typelib files searched in GI_TYPELIB_PATH.\n\nVarious plug-insIf your application uses GStreamer or Grilo, you should set GST_PLUGIN_SYSTEM_PATH_1_0 and\nGRL_PLUGIN_PATH, respectively.\n\nOnto wrapGAppsHookGiven the requirements above, the package expression would become messy quickly:\n\npreFixup = ''\n  for f in $(find $out\/bin\/ $out\/libexec\/ -type f -executable); do\n    wrapProgram \"$f\" \\\n      --prefix GIO_EXTRA_MODULES : \"${getLib dconf}\/lib\/gio\/modules\" \\\n      --prefix XDG_DATA_DIRS : \"$out\/share\" \\\n      --prefix XDG_DATA_DIRS : \"$out\/share\/gsettings-schemas\/${name}\" \\\n      --prefix XDG_DATA_DIRS : \"${gsettings-desktop-schemas}\/share\/gsettings-schemas\/${gsettings-desktop-schemas.name}\" \\\n      --prefix XDG_DATA_DIRS : \"${hicolor-icon-theme}\/share\" \\\n      --prefix GI_TYPELIB_PATH : \"${lib.makeSearchPath \"lib\/girepository-1.0\" [ pango json-glib ]}\"\n  done\n'';\n\nFortunately, there is [wrapGAppsHook]{#ssec-gnome-hooks-wrapgappshook}. It works in conjunction with other setup hooks\nthat populate environment variables, and it will then wrap all executables in bin and libexec directories using said\nvariables.\n\nFor convenience, it also adds dconf.lib for a GIO module implementing a GSettings backend using dconf, gtk3 for\nGSettings schemas, and librsvg for GdkPixbuf loader to the closure. There is also\n[wrapGAppsHook4]{#ssec-gnome-hooks-wrapgappshook4}, which replaces GTK 3 with GTK 4. And in case you are packaging a\nprogram without a graphical interface, you might want to use [wrapGAppsNoGuiHook]{#ssec-gnome-hooks-wrapgappsnoguihook},\nwhich runs the same script as wrapGAppsHook but does not bring gtk3 and librsvg into the closure.\n\n  - wrapGAppsHook itself will add the package’s share directory to XDG_DATA_DIRS.\n\n  -  glib setup hook will populate GSETTINGS_SCHEMAS_PATH and then wrapGAppsHook will prepend it to XDG_DATA_DIRS.\n\n  -  gdk-pixbuf setup hook will populate GDK_PIXBUF_MODULE_FILE with the path to biggest loaders.cache file from the\n    dependencies containing GdkPixbuf loaders. This works fine when there are only two packages containing loaders\n    (gdk-pixbuf and e.g. librsvg) – it will choose the second one, reasonably expecting that it will be bigger since it\n    describes extra loader in addition to the default ones. But when there are more than two loader packages, this logic\n    will break. One possible solution would be constructing a custom cache file for each package containing a program\n    like services\/x11\/gdk-pixbuf.nix NixOS module does. wrapGAppsHook copies the GDK_PIXBUF_MODULE_FILE environment\n    variable into the produced wrapper.\n\n  -  One of gtk3’s setup hooks will remove icon-theme.cache files from package’s icon theme directories to avoid\n    conflicts. Icon theme packages should prevent this with dontDropIconThemeCache = true;.\n\n  -  dconf.lib is a dependency of wrapGAppsHook, which then also adds it to the GIO_EXTRA_MODULES variable.\n\n  -  hicolor-icon-theme’s setup hook will add icon themes to XDG_ICON_DIRS which is prepended to XDG_DATA_DIRS by\n    wrapGAppsHook.\n\n  -  gobject-introspection setup hook populates GI_TYPELIB_PATH variable with lib\/girepository-1.0 directories of\n    dependencies, which is then added to wrapper by wrapGAppsHook. It also adds share directories of dependencies to\n    XDG_DATA_DIRS, which is intended to promote GIR files but it also pollutes the closures of packages using\n    wrapGAppsHook.\n    \n        The setup hook currently does not work in expressions with strictDeps enabled, like Python packages. In those cases,\n    you will need to disable it with strictDeps = false;. \n\n  -  Setup hooks of gst_all_1.gstreamer and grilo will populate the GST_PLUGIN_SYSTEM_PATH_1_0 and GRL_PLUGIN_PATH\n    variables, respectively, which will then be added to the wrapper by wrapGAppsHook.\n\nYou can also pass additional arguments to makeWrapper using gappsWrapperArgs in preFixup hook:\n\npreFixup = ''\n  gappsWrapperArgs+=(\n    # Thumbnailers\n    --prefix XDG_DATA_DIRS : \"${gdk-pixbuf}\/share\"\n    --prefix XDG_DATA_DIRS : \"${librsvg}\/share\"\n    --prefix XDG_DATA_DIRS : \"${shared-mime-info}\/share\"\n  )\n'';\n\nUpdating GNOME packagesMost GNOME package offer updateScript, it is therefore possible to update to latest source\ntarball by running nix-shell maintainers\/scripts\/update.nix --argstr package gnome.nautilus or even en masse with\nnix-shell maintainers\/scripts\/update.nix --argstr path gnome. Read the package’s NEWS file to see what changed.\n\nFrequently encountered issuesGLib-GIO-ERROR **: 06:04:50.903: No GSettings schemas are installed on the systemThere are\nno schemas available in XDG_DATA_DIRS. Temporarily add a random package containing schemas like\ngsettings-desktop-schemas to buildInputs. glib and wrapGAppsHook setup hooks will take care of making the schemas\navailable to application and you will see the actual missing schemas with the next error. Or you can try looking through\nthe source code for the actual schemas used.\n\nGLib-GIO-ERROR **: 06:04:50.903: Settings schema ‘org.gnome.foo’ is not installedPackage is missing some GSettings\nschemas. You can find out the package containing the schema with nix-locate org.gnome.foo.gschema.xml and let the hooks\nhandle the wrapping as above.\n\nWhen using wrapGAppsHook with special derivers you can end up with double wrapped binaries.This is because derivers like\npython.pkgs.buildPythonApplication or qt5.mkDerivation have setup-hooks automatically added that produce wrappers with\nmakeWrapper. The simplest way to workaround that is to disable the wrapGAppsHook automatic wrapping with dontWrapGApps =\ntrue; and pass the arguments it intended to pass to makeWrapper to another.\n\nIn the case of a Python application it could look like:\n\npython3.pkgs.buildPythonApplication {\n  pname = \"gnome-music\";\n  version = \"3.32.2\";\n\n  nativeBuildInputs = [\n    wrapGAppsHook\n    gobject-introspection\n    ...\n  ];\n\n  dontWrapGApps = true;\n\n  # Arguments to be passed to `makeWrapper`, only used by buildPython*\n  preFixup = ''\n    makeWrapperArgs+=(\"''${gappsWrapperArgs[@]}\")\n  '';\n}\n\nAnd for a QT app like:\n\nmkDerivation {\n  pname = \"calibre\";\n  version = \"3.47.0\";\n\n  nativeBuildInputs = [\n    wrapGAppsHook\n    qmake\n    ...\n  ];\n\n  dontWrapGApps = true;\n\n  # Arguments to be passed to `makeWrapper`, only used by qt5’s mkDerivation\n  preFixup = ''\n    qtWrapperArgs+=(\"''${gappsWrapperArgs[@]}\")\n  '';\n}\n\nI am packaging a project that cannot be wrapped, like a library or GNOME Shell extension.You can rely on applications\ndepending on the library setting the necessary environment variables but that is often easy to miss. Instead we\nrecommend to patch the paths in the source code whenever possible. Here are some examples:\n\n  -  Replacing a GI_TYPELIB_PATH in GNOME Shell extension – we are using substituteAll to include the path to a typelib\n    into a patch.\n\n  -  The following examples are hardcoding GSettings schema paths. To get the schema paths we use the functions\n    \n      - glib.getSchemaPath Takes a nix package attribute as an argument.\n    \n      - glib.makeSchemaPath Takes a package output like $out and a derivation name. You should use this if the schemas\n        you need to hardcode are in the same derivation.\n    \n        Hard-coding GSettings schema path in Vala plug-in (dynamically loaded library) – here, substituteAll cannot be used\n    since the schema comes from the same package preventing us from pass its path to the function, probably due to a Nix\n    bug.\n    \n        Hard-coding GSettings schema path in C library – nothing special other than using Coccinelle patch to generate the\n    patch itself.\n\nI need to wrap a binary outside bin and libexec directories.You can manually trigger the wrapping with wrapGApp in\npreFixup phase. It takes a path to a program as a first argument; the remaining arguments are passed directly to\nwrapProgram function.\n" }
,{ "url": "languages-frameworks\/go\/", "title": "Go", "text": "GoGo modulesThe function buildGoModule builds Go programs managed with Go modules. It builds a Go Modules through a two\nphase build:\n\n  - An intermediate fetcher derivation. This derivation will be used to fetch all of the dependencies of the Go module.\n  - A final derivation will use the output of the intermediate derivation to build the binaries and produce the final\n    output.\n\nExample for buildGoModuleIn the following is an example expression using buildGoModule, the following arguments are of\nspecial significance to the function:\n\n  - vendorSha256: is the hash of the output of the intermediate fetcher derivation. vendorSha256 can also take null as\n    an input. When null is used as a value, rather than fetching the dependencies and vendoring them, we use the\n    vendoring included within the source repo. If you'd like to not have to update this field on dependency changes, run\n    go mod vendor in your source repo and set vendorSha256 = null;\n  - runVend: runs the vend command to generate the vendor directory. This is useful if your code depends on c code and\n    go mod tidy does not include the needed sources to build.\n\npet = buildGoModule rec {\n  pname = \"pet\";\n  version = \"0.3.4\";\n\n  src = fetchFromGitHub {\n    owner = \"knqyf263\";\n    repo = \"pet\";\n    rev = \"v${version}\";\n    sha256 = \"0m2fzpqxk7hrbxsgqplkg7h2p7gv6s1miymv3gvw0cz039skag0s\";\n  };\n\n  vendorSha256 = \"1879j77k96684wi554rkjxydrj8g3hpp0kvxz03sd8dmwr3lh83j\";\n\n  runVend = true;\n\n  meta = with lib; {\n    description = \"Simple command-line snippet manager, written in Go\";\n    homepage = \"https:\/\/github.com\/knqyf263\/pet\";\n    license = licenses.mit;\n    maintainers = with maintainers; [ kalbasit ];\n    platforms = platforms.linux ++ platforms.darwin;\n  };\n}\n\nbuildGoPackage (legacy)The function buildGoPackage builds legacy Go programs, not supporting Go modules.\n\nExample for buildGoPackageIn the following is an example expression using buildGoPackage, the following arguments are of\nspecial significance to the function:\n\n  - goPackagePath specifies the package's canonical Go import path.\n  - goDeps is where the Go dependencies of a Go program are listed as a list of package source identified by Go import\n    path. It could be imported as a separate deps.nix file for readability. The dependency data structure is described\n    below.\n\ndeis = buildGoPackage rec {\n  pname = \"deis\";\n  version = \"1.13.0\";\n\n  goPackagePath = \"github.com\/deis\/deis\";\n\n  src = fetchFromGitHub {\n    owner = \"deis\";\n    repo = \"deis\";\n    rev = \"v${version}\";\n    sha256 = \"1qv9lxqx7m18029lj8cw3k7jngvxs4iciwrypdy0gd2nnghc68sw\";\n  };\n\n  goDeps = .\/deps.nix;\n}\n\nThe goDeps attribute can be imported from a separate nix file that defines which Go libraries are needed and should be\nincluded in GOPATH for buildPhase:\n\n# deps.nix\n[ # goDeps is a list of Go dependencies.\n  {\n    # goPackagePath specifies Go package import path.\n    goPackagePath = \"gopkg.in\/yaml.v2\";\n    fetch = {\n      # `fetch type` that needs to be used to get package source.\n      # If `git` is used there should be `url`, `rev` and `sha256` defined next to it.\n      type = \"git\";\n      url = \"https:\/\/gopkg.in\/yaml.v2\";\n      rev = \"a83829b6f1293c91addabc89d0571c246397bbf4\";\n      sha256 = \"1m4dsmk90sbi17571h6pld44zxz7jc4lrnl4f27dpd1l8g5xvjhh\";\n    };\n  }\n  {\n    goPackagePath = \"github.com\/docopt\/docopt-go\";\n    fetch = {\n      type = \"git\";\n      url = \"https:\/\/github.com\/docopt\/docopt-go\";\n      rev = \"784ddc588536785e7299f7272f39101f7faccc3f\";\n      sha256 = \"0wwz48jl9fvl1iknvn9dqr4gfy1qs03gxaikrxxp9gry6773v3sj\";\n    };\n  }\n]\n\nTo extract dependency information from a Go package in automated way use go2nix. It can produce complete derivation and\ngoDeps file for Go programs.\n\nYou may use Go packages installed into the active Nix profiles by adding the following to your ~\/.bashrc:\n\nfor p in $NIX_PROFILES; do\n    GOPATH=\"$p\/share\/go:$GOPATH\"\ndone\n\nAttributes used by the buildersBoth buildGoModule and buildGoPackage can be tweaked to behave slightly differently, if\nthe following attributes are used:\n\nbuildFlagsArray and buildFlags:These attributes set build flags supported by go build. We recommend using\nbuildFlagsArray.\n\n  buildFlagsArray = [\n    \"-tags=release\"\n  ];\n\nldflagsArguments to pass to the Go linker tool via the -ldflags argument of go build. The most common use case for this\nargument is to make the resulting executable aware of its own version. For example:\n\n  ldflags = [\n    \"-s\" \"-w\"\n    \"-X main.Version=${version}\"\n    \"-X main.Commit=${version}\"\n  ];\n\ndeleteVendorRemoves the pre-existing vendor directory. This should only be used if the dependencies included in the\nvendor folder are broken or incomplete.\n\nsubPackagesLimits the builder from building child packages that have not been listed. If subPackages is not specified,\nall child packages will be built.\n" }
,{ "url": "languages-frameworks\/haskell\/", "title": "Haskell", "text": "HaskellThe documentation for the Haskell infrastructure is published at https:\/\/haskell4nix.readthedocs.io\/. The source\ncode for that site lives in the doc\/ sub-directory of the cabal2nix Git repository and changes can be submitted there.\n" }
,{ "url": "languages-frameworks\/idris\/", "title": "Idris", "text": "IdrisInstalling IdrisThe easiest way to get a working idris version is to install the idris attribute:\n\n$ # On NixOS\n$ nix-env -i nixos.idris\n$ # On non-NixOS\n$ nix-env -i nixpkgs.idris\n\nThis however only provides the prelude and base libraries. To install idris with additional libraries, you can use the\nidrisPackages.with-packages function, e.g. in an overlay in ~\/.config\/nixpkgs\/overlays\/my-idris.nix:\n\nself: super: {\n  myIdris = with self.idrisPackages; with-packages [ contrib pruviloj ];\n}\n\nAnd then:\n\n$ # On NixOS\n$ nix-env -iA nixos.myIdris\n$ # On non-NixOS\n$ nix-env -iA nixpkgs.myIdris\n\nTo see all available Idris packages:\n\n$ # On NixOS\n$ nix-env -qaPA nixos.idrisPackages\n$ # On non-NixOS\n$ nix-env -qaPA nixpkgs.idrisPackages\n\nSimilarly, entering a nix-shell:\n\n$ nix-shell -p 'idrisPackages.with-packages (with idrisPackages; [ contrib pruviloj ])'\n\nStarting Idris with library supportTo have access to these libraries in idris, call it with an argument -p <library\nname> for each library:\n\n$ nix-shell -p 'idrisPackages.with-packages (with idrisPackages; [ contrib pruviloj ])'\n[nix-shell:~]$ idris -p contrib -p pruviloj\n\nA listing of all available packages the Idris binary has access to is available via --listlibs:\n\n$ idris --listlibs\n00prelude-idx.ibc\npruviloj\nbase\ncontrib\nprelude\n00pruviloj-idx.ibc\n00base-idx.ibc\n00contrib-idx.ibc\n\nBuilding an Idris project with NixAs an example of how a Nix expression for an Idris package can be created, here is the\none for idrisPackages.yaml:\n\n{ lib\n, build-idris-package\n, fetchFromGitHub\n, contrib\n, lightyear\n}:\nbuild-idris-package  {\n  name = \"yaml\";\n  version = \"2018-01-25\";\n\n  # This is the .ipkg file that should be built, defaults to the package name\n  # In this case it should build `Yaml.ipkg` instead of `yaml.ipkg`\n  # This is only necessary because the yaml packages ipkg file is\n  # different from its package name here.\n  ipkgName = \"Yaml\";\n  # Idris dependencies to provide for the build\n  idrisDeps = [ contrib lightyear ];\n\n  src = fetchFromGitHub {\n    owner = \"Heather\";\n    repo = \"Idris.Yaml\";\n    rev = \"5afa51ffc839844862b8316faba3bafa15656db4\";\n    sha256 = \"1g4pi0swmg214kndj85hj50ccmckni7piprsxfdzdfhg87s0avw7\";\n  };\n\n  meta = with lib; {\n    description = \"Idris YAML lib\";\n    homepage = \"https:\/\/github.com\/Heather\/Idris.Yaml\";\n    license = licenses.mit;\n    maintainers = [ maintainers.brainrape ];\n  };\n}\n\nAssuming this file is saved as yaml.nix, it's buildable using\n\n$ nix-build -E '(import <nixpkgs> {}).idrisPackages.callPackage .\/yaml.nix {}'\n\nOr it's possible to use\n\nwith import <nixpkgs> {};\n\n{\n  yaml = idrisPackages.callPackage .\/yaml.nix {};\n}\n\nin another file (say default.nix) to be able to build it with\n\n$ nix-build -A yaml\n\nPassing options to idris commandsThe build-idris-package function provides also optional input values to set additional\noptions for the used idris commands.\n\nSpecifically, you can set idrisBuildOptions, idrisTestOptions, idrisInstallOptions and idrisDocOptions to provide\nadditional options to the idris command respectively when building, testing, installing and generating docs for your\npackage.\n\nFor example you could set\n\nbuild-idris-package {\n  idrisBuildOptions = [ \"--log\" \"1\" \"--verbose\" ]\n\n  ...\n}\n\nto require verbose output during idris build phase.\n" }
,{ "url": "languages-frameworks\/ios\/", "title": "iOS", "text": "iOSThis component is basically a wrapper\/workaround that makes it possible to expose an Xcode installation as a Nix\npackage by means of symlinking to the relevant executables on the host system.\n\nSince Xcode can't be packaged with Nix, nor we can publish it as a Nix package (because of its license) this is\nbasically the only integration strategy making it possible to do iOS application builds that integrate with other\ncomponents of the Nix ecosystem\n\nThe primary objective of this project is to use the Nix expression language to specify how iOS apps can be built from\nsource code, and to automatically spawn iOS simulator instances for testing.\n\nThis component also makes it possible to use Hydra, the Nix-based continuous integration server to regularly build iOS\napps and to do wireless ad-hoc installations of enterprise IPAs on iOS devices through Hydra.\n\nThe Xcode build environment implements a number of features.\n\nDeploying a proxy component wrapper exposing XcodeThe first use case is deploying a Nix package that provides symlinks\nto the Xcode installation on the host system. This package can be used as a build input to any build function\nimplemented in the Nix expression language that requires Xcode.\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import .\/xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcodeenv.composeXcodeWrapper {\n  version = \"9.2\";\n  xcodeBaseDir = \"\/Applications\/Xcode.app\";\n}\n\nBy deploying the above expression with nix-build and inspecting its content you will notice that several Xcode-related\nexecutables are exposed as a Nix package:\n\n$ ls result\/bin\nlrwxr-xr-x  1 sander  staff  94  1 jan  1970 Simulator -> \/Applications\/Xcode.app\/Contents\/Developer\/Applications\/Simulator.app\/Contents\/MacOS\/Simulator\nlrwxr-xr-x  1 sander  staff  17  1 jan  1970 codesign -> \/usr\/bin\/codesign\nlrwxr-xr-x  1 sander  staff  17  1 jan  1970 security -> \/usr\/bin\/security\nlrwxr-xr-x  1 sander  staff  21  1 jan  1970 xcode-select -> \/usr\/bin\/xcode-select\nlrwxr-xr-x  1 sander  staff  61  1 jan  1970 xcodebuild -> \/Applications\/Xcode.app\/Contents\/Developer\/usr\/bin\/xcodebuild\nlrwxr-xr-x  1 sander  staff  14  1 jan  1970 xcrun -> \/usr\/bin\/xcrun\n\nBuilding an iOS applicationWe can build an iOS app executable for the simulator, or an IPA\/xcarchive file for release\npurposes, e.g. ad-hoc, enterprise or store installations, by executing the xcodeenv.buildApp {} function:\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import .\/xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcodeenv.buildApp {\n  name = \"MyApp\";\n  src = .\/myappsources;\n  sdkVersion = \"11.2\";\n\n  target = null; # Corresponds to the name of the app by default\n  configuration = null; # Release for release builds, Debug for debug builds\n  scheme = null; # -scheme will correspond to the app name by default\n  sdk = null; # null will set it to 'iphonesimulator` for simulator builds or `iphoneos` to real builds\n  xcodeFlags = \"\";\n\n  release = true;\n  certificateFile = .\/mycertificate.p12;\n  certificatePassword = \"secret\";\n  provisioningProfile = .\/myprovisioning.profile;\n  signMethod = \"ad-hoc\"; # 'enterprise' or 'store'\n  generateIPA = true;\n  generateXCArchive = false;\n\n  enableWirelessDistribution = true;\n  installURL = \"\/installipa.php\";\n  bundleId = \"mycompany.myapp\";\n  appVersion = \"1.0\";\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"\/Applications\/Xcode.app\";\n}\n\nThe above function takes a variety of parameters:\n\n  - The name and src parameters are mandatory and specify the name of the app and the location where the source code\n    resides\n  - sdkVersion specifies which version of the iOS SDK to use.\n\nIt also possile to adjust the xcodebuild parameters. This is only needed in rare circumstances. In most cases the\ndefault values should suffice:\n\n  - Specifies which xcodebuild target to build. By default it takes the target that has the same name as the app.\n  - The configuration parameter can be overridden if desired. By default, it will do a debug build for the simulator and\n    a release build for real devices.\n  - The scheme parameter specifies which -scheme parameter to propagate to xcodebuild. By default, it corresponds to the\n    app name.\n  - The sdk parameter specifies which SDK to use. By default, it picks iphonesimulator for simulator builds and iphoneos\n    for release builds.\n  - The xcodeFlags parameter specifies arbitrary command line parameters that should be propagated to xcodebuild.\n\nBy default, builds are carried out for the iOS simulator. To do release builds (builds for real iOS devices), you must\nset the release parameter to true. In addition, you need to set the following parameters:\n\n  - certificateFile refers to a P12 certificate file.\n  - certificatePassword specifies the password of the P12 certificate.\n  - provisioningProfile refers to the provision profile needed to sign the app\n  - signMethod should refer to ad-hoc for signing the app with an ad-hoc certificate, enterprise for enterprise\n    certificates and app-store for App store certificates.\n  - generateIPA specifies that we want to produce an IPA file (this is probably what you want)\n  - generateXCArchive specifies thet we want to produce an xcarchive file.\n\nWhen building IPA files on Hydra and when it is desired to allow iOS devices to install IPAs by browsing to the Hydra\nbuild products page, you can enable the enableWirelessDistribution parameter.\n\nWhen enabled, you need to configure the following options:\n\n  - The installURL parameter refers to the URL of a PHP script that composes the itms-services:\/\/ URL allowing iOS\n    devices to install the IPA file.\n  - bundleId refers to the bundle ID value of the app\n  - appVersion refers to the app's version number\n\nTo use wireless adhoc distributions, you must also install the corresponding PHP script on a web server (see section:\n'Installing the PHP script for wireless ad hoc installations from Hydra' for more information).\n\nIn addition to the build parameters, you can also specify any parameters that the xcodeenv.composeXcodeWrapper {}\nfunction takes. For example, the xcodeBaseDir parameter can be overridden to refer to a different Xcode version.\n\nSpawning simulator instancesIn addition to building iOS apps, we can also automatically spawn simulator instances:\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import .\/xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcode.simulateApp {\n  name = \"simulate\";\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"\/Applications\/Xcode.app\";\n}\n\nThe above expression produces a script that starts the simulator from the provided Xcode installation. The script can be\nstarted as follows:\n\n.\/result\/bin\/run-test-simulator\n\nBy default, the script will show an overview of UDID for all available simulator instances and asks you to pick one. You\ncan also provide a UDID as a command-line parameter to launch an instance automatically:\n\n.\/result\/bin\/run-test-simulator 5C93129D-CF39-4B1A-955F-15180C3BD4B8\n\nYou can also extend the simulator script to automatically deploy and launch an app in the requested simulator instance:\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import .\/xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcode.simulateApp {\n  name = \"simulate\";\n  bundleId = \"mycompany.myapp\";\n  app = xcode.buildApp {\n    # ...\n  };\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"\/Applications\/Xcode.app\";\n}\n\nBy providing the result of an xcode.buildApp {} function and configuring the app bundle id, the app gets deployed\nautomatically and started.\n\nTroubleshootingIn some rare cases, it may happen that after a failure, changes are not picked up. Most likely, this is\ncaused by a derived data cache that Xcode maintains. To wipe it you can run:\n\n$ rm -rf ~\/Library\/Developer\/Xcode\/DerivedData\n" }
,{ "url": "languages-frameworks\/java\/", "title": "Java", "text": "JavaAnt-based Java packages are typically built from source as follows:\n\nstdenv.mkDerivation {\n  name = \"...\";\n  src = fetchurl { ... };\n\n  nativeBuildInputs = [ jdk ant ];\n\n  buildPhase = \"ant\";\n}\n\nNote that jdk is an alias for the OpenJDK (self-built where available, or pre-built via Zulu). Platforms with OpenJDK\nnot (yet) in Nixpkgs (Aarch32, Aarch64) point to the (unfree) oraclejdk.\n\nJAR files that are intended to be used by other packages should be installed in $out\/share\/java. JDKs have a stdenv\nsetup hook that add any JARs in the share\/java directories of the build inputs to the CLASSPATH environment variable.\nFor instance, if the package libfoo installs a JAR named foo.jar in its share\/java directory, and another package\ndeclares the attribute\n\nbuildInputs = [ libfoo ];\nnativeBuildInputs = [ jdk ];\n\nthen CLASSPATH will be set to \/nix\/store\/...-libfoo\/share\/java\/foo.jar.\n\nPrivate JARs should be installed in a location like $out\/share\/package-name.\n\nIf your Java package provides a program, you need to generate a wrapper script to run it using a JRE. You can use\nmakeWrapper for this:\n\nnativeBuildInputs = [ makeWrapper ];\n\ninstallPhase = ''\n  mkdir -p $out\/bin\n  makeWrapper ${jre}\/bin\/java $out\/bin\/foo \\\n    --add-flags \"-cp $out\/share\/java\/foo.jar org.foo.Main\"\n'';\n\nSince the introduction of the Java Platform Module System in Java 9, Java distributions typically no longer ship with a\ngeneral-purpose JRE: instead, they allow generating a JRE with only the modules required for your application(s).\nBecause we can't predict what modules will be needed on a general-purpose system, the default jre package is the full\nJDK. When building a minimal system\/image, you can override the modules parameter on jre_minimal to build a JRE with\nonly the modules relevant for you:\n\nlet\n  my_jre = pkgs.jre_minimal.override {\n    modules = [\n      # The modules used by 'something' and 'other' combined:\n      \"java.base\"\n      \"java.logging\"\n    ];\n  };\n  something = (pkgs.something.override { jre = my_jre; });\n  other = (pkgs.other.override { jre = my_jre; });\nin\n  ...\n\nNote all JDKs passthru home, so if your application requires environment variables like JAVA_HOME being set, that can be\ndone in a generic fashion with the --set argument of makeWrapper:\n\n--set JAVA_HOME ${jdk.home}\n\nIt is possible to use a different Java compiler than javac from the OpenJDK. For instance, to use the GNU Java Compiler:\n\nnativeBuildInputs = [ gcj ant ];\n\nHere, Ant will automatically use gij (the GNU Java Runtime) instead of the OpenJRE.\n" }
,{ "url": "languages-frameworks\/lua\/", "title": "User’s Guide to Lua Infrastructure", "text": "User’s Guide to Lua InfrastructureUsing LuaOverview of LuaSeveral versions of the Lua interpreter are available: luajit,\nlua 5.1, 5.2, 5.3. The attribute lua refers to the default interpreter, it is also possible to refer to specific\nversions, e.g. lua5_2 refers to Lua 5.2.\n\nLua libraries are in separate sets, with one set per interpreter version.\n\nThe interpreters have several common attributes. One of these attributes is pkgs, which is a package set of Lua\nlibraries for this specific interpreter. E.g., the busted package corresponding to the default interpreter is\nlua.pkgs.busted, and the lua 5.2 version is lua5_2.pkgs.busted. The main package set contains aliases to these package\nsets, e.g. luaPackages refers to lua5_1.pkgs and lua52Packages to lua5_2.pkgs.\n\nInstalling Lua and packagesLua environment defined in separate .nix fileCreate a file, e.g. build.nix, with the\nfollowing expression\n\nwith import <nixpkgs> {};\n\nlua5_2.withPackages (ps: with ps; [ busted luafilesystem ])\n\nand install it in your profile with\n\nnix-env -if build.nix\n\nNow you can use the Lua interpreter, as well as the extra packages (busted, luafilesystem) that you added to the\nenvironment.\n\nLua environment defined in ~\/.config\/nixpkgs\/config.nixIf you prefer to, you could also add the environment as a package\noverride to the Nixpkgs set, e.g. using config.nix,\n\n{ # ...\n\n  packageOverrides = pkgs: with pkgs; {\n    myLuaEnv = lua5_2.withPackages (ps: with ps; [ busted luafilesystem ]);\n  };\n}\n\nand install it in your profile with\n\nnix-env -iA nixpkgs.myLuaEnv\n\nThe environment is installed by referring to the attribute, and considering the nixpkgs channel was used.\n\nLua environment defined in \/etc\/nixos\/configuration.nixFor the sake of completeness, here's another example how to\ninstall the environment system-wide.\n\n{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (lua.withPackages(ps: with ps; [ busted luafilesystem ]))\n  ];\n}\n\nHow to override a Lua package using overlays?Use the following overlay template:\n\nfinal: prev:\n{\n\n  lua = prev.lua.override {\n    packageOverrides = luaself: luaprev: {\n\n      luarocks-nix = luaprev.luarocks-nix.overrideAttrs(oa: {\n        pname = \"luarocks-nix\";\n        src = \/home\/my_luarocks\/repository;\n      });\n  };\n\n  luaPackages = lua.pkgs;\n}\n\nTemporary Lua environment with nix-shellThere are two methods for loading a shell with Lua packages. The first and\nrecommended method is to create an environment with lua.buildEnv or lua.withPackages and load that. E.g.\n\n$ nix-shell -p 'lua.withPackages(ps: with ps; [ busted luafilesystem ])'\n\nopens a shell from which you can launch the interpreter\n\n[nix-shell:~] lua\n\nThe other method, which is not recommended, does not create an environment and requires you to list the packages\ndirectly,\n\n$ nix-shell -p lua.pkgs.busted lua.pkgs.luafilesystem\n\nAgain, it is possible to launch the interpreter from the shell. The Lua interpreter has the attribute pkgs which\ncontains all Lua libraries for that specific interpreter.\n\nDeveloping with LuaNow that you know how to get a working Lua environment with Nix, it is time to go forward and start\nactually developing with Lua. There are two ways to package lua software, either it is on luarocks and most of it can be\ntaken care of by the luarocks2nix converter or the packaging has to be done manually. Let's present the luarocks way\nfirst and the manual one in a second time.\n\nPackaging a library on luarocksLuarocks.org is the main repository of lua packages. The site proposes two types of\npackages, the rockspec and the src.rock (equivalent of a rockspec but with the source). These packages can have\ndifferent build types such as cmake, builtin etc .\n\nLuarocks-based packages are generated in pkgs\/development\/lua-modules\/generated-packages.nix from the whitelist\nmaintainers\/scripts\/luarocks-packages.csv and updated by running maintainers\/scripts\/update-luarocks-packages.\n\nluarocks2nix is a tool capable of generating nix derivations from both rockspec and src.rock (and favors the src.rock).\nThe automation only goes so far though and some packages need to be customized. These customizations go in\npkgs\/development\/lua-modules\/overrides.nix. For instance if the rockspec defines external_dependencies, these need to be\nmanually added in its rockspec file then it won't work.\n\nYou can try converting luarocks packages to nix packages with the command nix-shell -p luarocks-nix and then luarocks\nnix PKG_NAME. Nix rely on luarocks to install lua packages, basically it runs: luarocks make --deps-mode=none --tree\n$out\n\nPackaging a library manuallyYou can develop your package as you usually would, just don't forget to wrap it within a\ntoLuaModule call, for instance\n\nmynewlib = toLuaModule ( stdenv.mkDerivation { ... });\n\nThere is also the buildLuaPackage function that can be used when lua modules are not packaged for luarocks. You can see\na few examples at pkgs\/top-level\/lua-packages.nix.\n\nLua ReferenceLua interpretersVersions 5.1, 5.2 and 5.3 of the lua interpreter are available as respectively lua5_1,\nlua5_2 and lua5_3. Luajit is available too. The Nix expressions for the interpreters can be found in\npkgs\/development\/interpreters\/lua-5.\n\nAttributes on lua interpreters packagesEach interpreter has the following attributes:\n\n  - interpreter. Alias for ${pkgs.lua}\/bin\/lua.\n  - buildEnv. Function to build lua interpreter environments with extra packages bundled together. See section\n    lua.buildEnv function for usage and documentation.\n  - withPackages. Simpler interface to buildEnv.\n  - pkgs. Set of Lua packages for that specific interpreter. The package set can be modified by overriding the\n    interpreter and passing packageOverrides.\n\nbuildLuarocksPackage functionThe buildLuarocksPackage function is implemented in\npkgs\/development\/interpreters\/lua-5\/build-lua-package.nix The following is an example:\n\nluaposix = buildLuarocksPackage {\n  pname = \"luaposix\";\n  version = \"34.0.4-1\";\n\n  src = fetchurl {\n    url    = \"https:\/\/raw.githubusercontent.com\/rocks-moonscript-org\/moonrocks-mirror\/master\/luaposix-34.0.4-1.src.rock\";\n    sha256 = \"0yrm5cn2iyd0zjd4liyj27srphvy0gjrjx572swar6zqr4dwjqp2\";\n  };\n  disabled = (luaOlder \"5.1\") || (luaAtLeast \"5.4\");\n  propagatedBuildInputs = [ bit32 lua std_normalize ];\n\n  meta = with lib; {\n    homepage = \"https:\/\/github.com\/luaposix\/luaposix\/\";\n    description = \"Lua bindings for POSIX\";\n    maintainers = with maintainers; [ vyp lblasc ];\n    license.fullName = \"MIT\/X11\";\n  };\n};\n\nThe buildLuarocksPackage delegates most tasks to luarocks:\n\n  - it adds luarocks as an unpacker for src.rock files (zip files really).\n  - configurePhasewrites a temporary luarocks configuration file which location is exported via the environment\n    variableLUAROCKS_CONFIG`.\n  - the buildPhase does nothing.\n  - installPhase calls luarocks make --deps-mode=none --tree $out to build and install the package\n  - In the postFixup phase, the wrapLuaPrograms bash function is called to wrap all programs in the $out\/bin\/* directory\n    to include $PATH environment variable and add dependent libraries to script's LUA_PATH and LUA_CPATH.\n\nBy default meta.platforms is set to the same value as the interpreter unless overridden otherwise.\n\nbuildLuaApplication functionThe buildLuaApplication function is practically the same as buildLuaPackage. The difference\nis that buildLuaPackage by default prefixes the names of the packages with the version of the interpreter. Because with\nan application we're not interested in multiple version the prefix is dropped.\n\nlua.withPackages functionThe lua.withPackages takes a function as an argument that is passed the set of lua packages and\nreturns the list of packages to be included in the environment. Using the withPackages function, the previous example\nfor the luafilesystem environment can be written like this:\n\nwith import <nixpkgs> {};\n\nlua.withPackages (ps: [ps.luafilesystem])\n\nwithPackages passes the correct package set for the specific interpreter version as an argument to the function. In the\nabove example, ps equals luaPackages. But you can also easily switch to using lua5_2:\n\nwith import <nixpkgs> {};\n\nlua5_2.withPackages (ps: [ps.lua])\n\nNow, ps is set to lua52Packages, matching the version of the interpreter.\n\nPossible Todos  - export\/use version specific variables such as LUA_PATH_5_2\/LUAROCKS_CONFIG_5_2\n  - let luarocks check for dependencies via exporting the different rocktrees in temporary config\n\nLua Contributing guidelinesFollowing rules should be respected:\n\n  - Make sure libraries build for all Lua interpreters.\n  - Commit names of Lua libraries should reflect that they are Lua libraries, so write for example\n    luaPackages.luafilesystem: 1.11 -> 1.12.\n" }
,{ "url": "languages-frameworks\/maven\/", "title": "Maven", "text": "MavenMaven is a well-known build tool for the Java ecosystem however it has some challenges when integrating into the\nNix build system.\n\nThe following provides a list of common patterns with how to package a Maven project (or any JVM language that can\nexport to Maven) as a Nix package.\n\nFor the purposes of this example let's consider a very basic Maven project with the following pom.xml with a single\ndependency on emoji-java.\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\"\n        xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/xsd\/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0<\/modelVersion>\n  <groupId>io.github.fzakaria<\/groupId>\n  <artifactId>maven-demo<\/artifactId>\n  <version>1.0<\/version>\n  <packaging>jar<\/packaging>\n  <name>NixOS Maven Demo<\/name>\n\n  <dependencies>\n    <dependency>\n        <groupId>com.vdurmont<\/groupId>\n        <artifactId>emoji-java<\/artifactId>\n        <version>5.1.1<\/version>\n      <\/dependency>\n  <\/dependencies>\n<\/project>\n\nOur main class file will be very simple:\n\nimport com.vdurmont.emoji.EmojiParser;\n\npublic class Main {\n  public static void main(String[] args) {\n    String str = \"NixOS :grinning: is super cool :smiley:!\";\n    String result = EmojiParser.parseToUnicode(str);\n    System.out.println(result);\n  }\n}\n\nYou find this demo project at https:\/\/github.com\/fzakaria\/nixos-maven-example\n\nSolving for dependenciesbuildMaven with NixOS\/mvn2nix-maven-plugin⚠️ Although buildMaven is the \"blessed\" way within\nnixpkgs, as of 2020, it hasn't seen much activity in quite a while.\n\nbuildMaven is an alternative method that tries to follow similar patterns of other programming languages by generating a\nlock file. It relies on the maven plugin mvn2nix-maven-plugin.\n\nFirst you generate a project-info.json file using the maven plugin.\n\nThis should be executed in the project's source repository or be told which pom.xml to execute with.\n\n# run this step within the project's source repository\n❯ mvn org.nixos.mvn2nix:mvn2nix-maven-plugin:mvn2nix\n\n❯ cat project-info.json | jq | head\n{\n  \"project\": {\n    \"artifactId\": \"maven-demo\",\n    \"groupId\": \"org.nixos\",\n    \"version\": \"1.0\",\n    \"classifier\": \"\",\n    \"extension\": \"jar\",\n    \"dependencies\": [\n      {\n        \"artifactId\": \"maven-resources-plugin\",\n\nThis file is then given to the buildMaven function, and it returns 2 attributes.\n\nrepo: A Maven repository that is a symlink farm of all the dependencies found in the project-info.json\n\nbuild: A simple derivation that runs through mvn compile & mvn package to build the JAR. You may use this as inspiration\nfor more complicated derivations.\n\nHere is an example of building the Maven repository\n\n{ pkgs ? import <nixpkgs> { } }:\nwith pkgs;\n(buildMaven .\/project-info.json).repo\n\nThe benefit over the double invocation as we will see below, is that the \/nix\/store entry is a linkFarm of every\npackage, so that changes to your dependency set doesn't involve downloading everything from scratch.\n\n❯ tree $(nix-build --no-out-link build-maven-repository.nix) | head\n\/nix\/store\/g87va52nkc8jzbmi1aqdcf2f109r4dvn-maven-repository\n├── antlr\n│   └── antlr\n│       └── 2.7.2\n│           ├── antlr-2.7.2.jar -> \/nix\/store\/d027c8f2cnmj5yrynpbq2s6wmc9cb559-antlr-2.7.2.jar\n│           └── antlr-2.7.2.pom -> \/nix\/store\/mv42fc5gizl8h5g5vpywz1nfiynmzgp2-antlr-2.7.2.pom\n├── avalon-framework\n│   └── avalon-framework\n│       └── 4.1.3\n│           ├── avalon-framework-4.1.3.jar -> \/nix\/store\/iv5fp3955w3nq28ff9xfz86wvxbiw6n9-avalon-framework-4.1.3.jar\n\nDouble Invocation⚠️ This pattern is the simplest but may cause unnecessary rebuilds due to the output hash changing.\n\nThe double invocation is a simple way to get around the problem that nix-build may be sandboxed and have no Internet\nconnectivity.\n\nIt treats the entire Maven repository as a single source to be downloaded, relying on Maven's dependency resolution to\nsatisfy the output hash. This is similar to fetchers like fetchgit, except it has to run a Maven build to determine what\nto download.\n\nThe first step will be to build the Maven project as a fixed-output derivation in order to collect the Maven repository\n-- below is an example.\n\nTraditionally the Maven repository is at ~\/.m2\/repository. We will override this to be the $out directory.\n\n{ lib, stdenv, maven }:\nstdenv.mkDerivation {\n  name = \"maven-repository\";\n  buildInputs = [ maven ];\n  src = .\/.; # or fetchFromGitHub, cleanSourceWith, etc\n  buildPhase = ''\n    mvn package -Dmaven.repo.local=$out\n  '';\n\n  # keep only *.{pom,jar,sha1,nbm} and delete all ephemeral files with lastModified timestamps inside\n  installPhase = ''\n    find $out -type f \\\n      -name \\*.lastUpdated -or \\\n      -name resolver-status.properties -or \\\n      -name _remote.repositories \\\n      -delete\n  '';\n\n  # don't do any fixup\n  dontFixup = true;\n  outputHashAlgo = \"sha256\";\n  outputHashMode = \"recursive\";\n  # replace this with the correct SHA256\n  outputHash = lib.fakeSha256;\n}\n\nThe build will fail, and tell you the expected outputHash to place. When you've set the hash, the build will return with\na \/nix\/store entry whose contents are the full Maven repository.\n\nSome additional files are deleted that would cause the output hash to change potentially on subsequent runs.\n\n❯ tree $(nix-build --no-out-link double-invocation-repository.nix) | head\n\/nix\/store\/8kicxzp98j68xyi9gl6jda67hp3c54fq-maven-repository\n├── backport-util-concurrent\n│   └── backport-util-concurrent\n│       └── 3.1\n│           ├── backport-util-concurrent-3.1.pom\n│           └── backport-util-concurrent-3.1.pom.sha1\n├── classworlds\n│   └── classworlds\n│       ├── 1.1\n│       │   ├── classworlds-1.1.jar\n\nIf your package uses SNAPSHOT dependencies or version ranges; there is a strong likelihood that over-time your output\nhash will change since the resolved dependencies may change. Hence this method is less recommended then using\nbuildMaven.\n\nBuilding a JARRegardless of which strategy is chosen above, the step to build the derivation is the same.\n\n{ stdenv, maven, callPackage }:\n# pick a repository derivation, here we will use buildMaven\nlet repository = callPackage .\/build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball \"https:\/\/github.com\/fzakaria\/nixos-maven-example\/archive\/main.tar.gz\";\n  buildInputs = [ maven ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    install -Dm644 target\/${pname}-${version}.jar $out\/share\/java\n  '';\n}\n\nWe place the library in $out\/share\/java since JDK package has a stdenv setup hook that adds any JARs in the share\/java\ndirectories of the build inputs to the CLASSPATH environment.\n\n❯ tree $(nix-build --no-out-link build-jar.nix)\n\/nix\/store\/7jw3xdfagkc2vw8wrsdv68qpsnrxgvky-maven-demo-1.0\n└── share\n    └── java\n        └── maven-demo-1.0.jar\n\n2 directories, 1 file\n\nRunnable JARThe previous example builds a jar file but that's not a file one can run.\n\nYou need to use it with java -jar $out\/share\/java\/output.jar and make sure to provide the required dependencies on the\nclasspath.\n\nThe following explains how to use makeWrapper in order to make the derivation produce an executable that will run the\nJAR file you created.\n\nWe will use the same repository we built above (either double invocation or buildMaven) to setup a CLASSPATH for our\nJAR.\n\nThe following two methods are more suited to Nix then building an UberJar which may be the more traditional approach.\n\nCLASSPATHThis is ideal if you are providing a derivation for nixpkgs and don't want to patch the project's pom.xml.\n\nWe will read the Maven repository and flatten it to a single list. This list will then be concatenated with the\nCLASSPATH separator to create the full classpath.\n\nWe make sure to provide this classpath to the makeWrapper.\n\n{ stdenv, maven, callPackage, makeWrapper, jre }:\nlet\n  repository = callPackage .\/build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball\n    \"https:\/\/github.com\/fzakaria\/nixos-maven-example\/archive\/main.tar.gz\";\n  buildInputs = [ maven makeWrapper ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    mkdir -p $out\/bin\n\n    classpath=$(find ${repository} -name \"*.jar\" -printf ':%h\/%f');\n    install -Dm644 target\/${pname}-${version}.jar $out\/share\/java\n    # create a wrapper that will automatically set the classpath\n    # this should be the paths from the dependency derivation\n    makeWrapper ${jre}\/bin\/java $out\/bin\/${pname} \\\n          --add-flags \"-classpath $out\/share\/java\/${pname}-${version}.jar:''${classpath#:}\" \\\n          --add-flags \"Main\"\n  '';\n}\n\nMANIFEST file via Maven PluginThis is ideal if you are the project owner and want to change your pom.xml to set the\nCLASSPATH within it.\n\nAugment the pom.xml to create a JAR with the following manifest:\n\n<build>\n  <plugins>\n    <plugin>\n        <artifactId>maven-jar-plugin<\/artifactId>\n        <configuration>\n            <archive>\n                <manifest>\n                    <addClasspath>true<\/addClasspath>\n                    <classpathPrefix>..\/..\/repository\/<\/classpathPrefix>\n                    <classpathLayoutType>repository<\/classpathLayoutType>\n                    <mainClass>Main<\/mainClass>\n                <\/manifest>\n                <manifestEntries>\n                    <Class-Path>.<\/Class-Path>\n                <\/manifestEntries>\n            <\/archive>\n        <\/configuration>\n    <\/plugin>\n  <\/plugins>\n<\/build>\n\nThe above plugin instructs the JAR to look for the necessary dependencies in the lib\/ relative folder. The layout of the\nfolder is also in the maven repository style.\n\n❯ unzip -q -c $(nix-build --no-out-link runnable-jar.nix)\/share\/java\/maven-demo-1.0.jar META-INF\/MANIFEST.MF\n\nManifest-Version: 1.0\nArchiver-Version: Plexus Archiver\nBuilt-By: nixbld\nClass-Path: . ..\/..\/repository\/com\/vdurmont\/emoji-java\/5.1.1\/emoji-jav\n a-5.1.1.jar ..\/..\/repository\/org\/json\/json\/20170516\/json-20170516.jar\nCreated-By: Apache Maven 3.6.3\nBuild-Jdk: 1.8.0_265\nMain-Class: Main\n\nWe will modify the derivation above to add a symlink to our repository so that it's accessible to our JAR during the\ninstallPhase.\n\n{ stdenv, maven, callPackage, makeWrapper, jre }:\n# pick a repository derivation, here we will use buildMaven\nlet repository = callPackage .\/build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball\n    \"https:\/\/github.com\/fzakaria\/nixos-maven-example\/archive\/main.tar.gz\";\n  buildInputs = [ maven makeWrapper ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    mkdir -p $out\/bin\n\n    # create a symbolic link for the repository directory\n    ln -s ${repository} $out\/repository\n\n    install -Dm644 target\/${pname}-${version}.jar $out\/share\/java\n    # create a wrapper that will automatically set the classpath\n    # this should be the paths from the dependency derivation\n    makeWrapper ${jre}\/bin\/java $out\/bin\/${pname} \\\n          --add-flags \"-jar $out\/share\/java\/${pname}-${version}.jar\"\n  '';\n}\n\nOur script produces a dependency on jre rather than jdk to restrict the runtime closure necessary to run the\napplication.\n\nThis will give you an executable shell-script that launches your JAR with all the dependencies available.\n\n❯ tree $(nix-build --no-out-link runnable-jar.nix)\n\/nix\/store\/8d4c3ibw8ynsn01ibhyqmc1zhzz75s26-maven-demo-1.0\n├── bin\n│   └── maven-demo\n├── repository -> \/nix\/store\/g87va52nkc8jzbmi1aqdcf2f109r4dvn-maven-repository\n└── share\n    └── java\n        └── maven-demo-1.0.jar\n\n❯ $(nix-build --no-out-link --option tarball-ttl 1 runnable-jar.nix)\/bin\/maven-demo\nNixOS 😀 is super cool 😃!\n" }
,{ "url": "languages-frameworks\/node\/", "title": "Node.js", "text": "Node.jsThe pkgs\/development\/node-packages folder contains a generated collection of NPM packages that can be installed\nwith the Nix package manager.\n\nAs a rule of thumb, the package set should only provide end user software packages, such as command-line utilities.\nLibraries should only be added to the package set if there is a non-NPM package that requires it.\n\nWhen it is desired to use NPM libraries in a development project, use the node2nix generator directly on the\npackage.json configuration file of the project.\n\nThe package set provides support for the official stable Node.js versions. The latest stable LTS release in\nnodePackages, as well as the latest stable Current release in nodePackages_latest.\n\nIf your package uses native addons, you need to examine what kind of native build system it uses. Here are some\nexamples:\n\n  - node-gyp\n  - node-gyp-builder\n  - node-pre-gyp\n\nAfter you have identified the correct system, you need to override your package expression while adding in build system\nas a build input. For example, dat requires node-gyp-build, so we override its expression in default.nix:\n\n    dat = super.dat.override {\n      buildInputs = [ self.node-gyp-build pkgs.libtool pkgs.autoconf pkgs.automake ];\n      meta.broken = since \"12\";\n    };\n\nTo add a package from NPM to nixpkgs:\n\n1.  Modify pkgs\/development\/node-packages\/node-packages.json to add, update or remove package entries to have it\n    included in nodePackages and nodePackages_latest.\n2.  Run the script: (cd pkgs\/development\/node-packages && .\/generate.sh).\n3.  Build your new package to test your changes: cd \/path\/to\/nixpkgs && nix-build -A\n    nodePackages.<new-or-updated-package>. To build against the latest stable Current Node.js version (e.g. 14.x):\n    nix-build -A nodePackages_latest.<new-or-updated-package>\n4.  Add and commit all modified and generated files.\n\nFor more information about the generation process, consult the README.md file of the node2nix tool.\n" }
,{ "url": "languages-frameworks\/ocaml\/", "title": "OCaml", "text": "OCamlOCaml libraries should be installed in $(out)\/lib\/ocaml\/${ocaml.version}\/site-lib\/. Such directories are\nautomatically added to the $OCAMLPATH environment variable when building another package that depends on them or when\nopening a nix-shell.\n\nGiven that most of the OCaml ecosystem is now built with dune, nixpkgs includes a convenience build support function\ncalled buildDunePackage that will build an OCaml package using dune, OCaml and findlib and any additional dependencies\nprovided as buildInputs or propagatedBuildInputs.\n\nHere is a simple package example. It defines an (optional) attribute minimumOCamlVersion that will be used to throw a\ndescriptive evaluation error if building with an older OCaml is attempted. It uses the fetchFromGitHub fetcher to get\nits source. It sets the doCheck (optional) attribute to true which means that tests will be run with dune runtest -p\nangstrom after the build (dune build -p angstrom) is complete. It uses alcotest as a build input (because it is needed\nto run the tests) and bigstringaf and result as propagated build inputs (thus they will also be available to libraries\ndepending on this library). The library will be installed using the angstrom.install file that dune generates.\n\n{ lib\n, fetchFromGitHub\n, buildDunePackage\n, alcotest\n, result\n, bigstringaf\n}:\n\nbuildDunePackage rec {\n  pname = \"angstrom\";\n  version = \"0.10.0\";\n\n  minimumOCamlVersion = \"4.03\";\n\n  src = fetchFromGitHub {\n    owner  = \"inhabitedtype\";\n    repo   = pname;\n    rev    = version;\n    sha256 = \"0lh6024yf9ds0nh9i93r9m6p5psi8nvrqxl5x7jwl13zb0r9xfpw\";\n  };\n\n  buildInputs = [ alcotest ];\n  propagatedBuildInputs = [ bigstringaf result ];\n  doCheck = true;\n\n  meta = with lib; {\n    homepage = \"https:\/\/github.com\/inhabitedtype\/angstrom\";\n    description = \"OCaml parser combinators built for speed and memory efficiency\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ sternenseemann ];\n  };\n}\n\nHere is a second example, this time using a source archive generated with dune-release. It is a good idea to use this\narchive when it is available as it will usually contain substituted variables such as a %%VERSION%% field. This library\ndoes not depend on any other OCaml library and no tests are run after building it.\n\n{ lib\n, fetchurl\n, buildDunePackage\n}:\n\nbuildDunePackage rec {\n  pname = \"wtf8\";\n  version = \"1.0.1\";\n\n  minimumOCamlVersion = \"4.01\";\n\n  src = fetchurl {\n    url = \"https:\/\/github.com\/flowtype\/ocaml-${pname}\/releases\/download\/v${version}\/${pname}-${version}.tbz\";\n    sha256 = \"1msg3vycd3k8qqj61sc23qks541cxpb97vrnrvrhjnqxsqnh6ygq\";\n  };\n\n  meta = with lib; {\n    homepage = \"https:\/\/github.com\/flowtype\/ocaml-wtf8\";\n    description = \"WTF-8 is a superset of UTF-8 that allows unpaired surrogates.\";\n    license = licenses.mit;\n    maintainers = [ maintainers.eqyiel ];\n  };\n}\n" }
,{ "url": "languages-frameworks\/perl\/", "title": "Perl", "text": "PerlRunning perl programs on the shellWhen executing a Perl script, it is possible you get an error such as\n.\/myscript.pl: bad interpreter: \/usr\/bin\/perl: no such file or directory. This happens when the script expects Perl to\nbe installed at \/usr\/bin\/perl, which is not the case when using Perl from nixpkgs. You can fix the script by changing\nthe first line to:\n\n#!\/usr\/bin\/env perl\n\nto take the Perl installation from the PATH environment variable, or invoke Perl directly with:\n\n$ perl .\/myscript.pl\n\nWhen the script is using a Perl library that is not installed globally, you might get an error such as Can't locate\nDB_File.pm in @INC (you may need to install the DB_File module). In that case, you can use nix-shell to start an ad-hoc\nshell with that library installed, for instance:\n\n$ nix-shell -p perl perlPackages.DBFile --run .\/myscript.pl\n\nIf you are always using the script in places where nix-shell is available, you can embed the nix-shell invocation in the\nshebang like this:\n\n#!\/usr\/bin\/env nix-shell\n#! nix-shell -i perl -p perl perlPackages.DBFile\n\nPackaging Perl programsNixpkgs provides a function buildPerlPackage, a generic package builder function for any Perl\npackage that has a standard Makefile.PL. It’s implemented in pkgs\/development\/perl-modules\/generic.\n\nPerl packages from CPAN are defined in pkgs\/top-level\/perl-packages.nix rather than pkgs\/all-packages.nix. Most Perl\npackages are so straight-forward to build that they are defined here directly, rather than having a separate function\nfor each package called from perl-packages.nix. However, more complicated packages should be put in a separate file,\ntypically in pkgs\/development\/perl-modules. Here is an example of the former:\n\nClassC3 = buildPerlPackage rec {\n  name = \"Class-C3-0.21\";\n  src = fetchurl {\n    url = \"mirror:\/\/cpan\/authors\/id\/F\/FL\/FLORA\/${name}.tar.gz\";\n    sha256 = \"1bl8z095y4js66pwxnm7s853pi9czala4sqc743fdlnk27kq94gz\";\n  };\n};\n\nNote the use of mirror:\/\/cpan\/, and the ${name} in the URL definition to ensure that the name attribute is consistent\nwith the source that we’re actually downloading. Perl packages are made available in all-packages.nix through the\nvariable perlPackages. For instance, if you have a package that needs ClassC3, you would typically write\n\nfoo = import ..\/path\/to\/foo.nix {\n  inherit stdenv fetchurl ...;\n  inherit (perlPackages) ClassC3;\n};\n\nin all-packages.nix. You can test building a Perl package as follows:\n\n$ nix-build -A perlPackages.ClassC3\n\nbuildPerlPackage adds perl- to the start of the name attribute, so the package above is actually called\nperl-Class-C3-0.21. So to install it, you can say:\n\n$ nix-env -i perl-Class-C3\n\n(Of course you can also install using the attribute name: nix-env -i -A perlPackages.ClassC3.)\n\nSo what does buildPerlPackage do? It does the following:\n\n1.  In the configure phase, it calls perl Makefile.PL to generate a Makefile. You can set the variable makeMakerFlags to\n    pass flags to Makefile.PL\n2.  It adds the contents of the PERL5LIB environment variable to #! ...\/bin\/perl line of Perl scripts as -Idir flags.\n    This ensures that a script can find its dependencies. (This can cause this shebang line to become too long for\n    Darwin to handle; see the note below.)\n3.  In the fixup phase, it writes the propagated build inputs (propagatedBuildInputs) to the file\n    $out\/nix-support\/propagated-user-env-packages. nix-env recursively installs all packages listed in this file when\n    you install a package that has it. This ensures that a Perl package can find its dependencies.\n\nbuildPerlPackage is built on top of stdenv, so everything can be customised in the usual way. For instance, the\nBerkeleyDB module has a preConfigure hook to generate a configuration file used by Makefile.PL:\n\n{ buildPerlPackage, fetchurl, db }:\n\nbuildPerlPackage rec {\n  name = \"BerkeleyDB-0.36\";\n\n  src = fetchurl {\n    url = \"mirror:\/\/cpan\/authors\/id\/P\/PM\/PMQS\/${name}.tar.gz\";\n    sha256 = \"07xf50riarb60l1h6m2dqmql8q5dij619712fsgw7ach04d8g3z1\";\n  };\n\n  preConfigure = ''\n    echo \"LIB = ${db.out}\/lib\" > config.in\n    echo \"INCLUDE = ${db.dev}\/include\" >> config.in\n  '';\n}\n\nDependencies on other Perl packages can be specified in the buildInputs and propagatedBuildInputs attributes. If\nsomething is exclusively a build-time dependency, use buildInputs; if it’s (also) a runtime dependency, use\npropagatedBuildInputs. For instance, this builds a Perl module that has runtime dependencies on a bunch of other\nmodules:\n\nClassC3Componentised = buildPerlPackage rec {\n  name = \"Class-C3-Componentised-1.0004\";\n  src = fetchurl {\n    url = \"mirror:\/\/cpan\/authors\/id\/A\/AS\/ASH\/${name}.tar.gz\";\n    sha256 = \"0xql73jkcdbq4q9m0b0rnca6nrlvf5hyzy8is0crdk65bynvs8q1\";\n  };\n  propagatedBuildInputs = [\n    ClassC3 ClassInspector TestException MROCompat\n  ];\n};\n\nOn Darwin, if a script has too many -Idir flags in its first line (its “shebang line”), it will not run. This can be\nworked around by calling the shortenPerlShebang function from the postInstall phase:\n\n{ lib, stdenv, buildPerlPackage, fetchurl, shortenPerlShebang }:\n\nImageExifTool = buildPerlPackage {\n  pname = \"Image-ExifTool\";\n  version = \"11.50\";\n\n  src = fetchurl {\n    url = \"https:\/\/www.sno.phy.queensu.ca\/~phil\/exiftool\/Image-ExifTool-11.50.tar.gz\";\n    sha256 = \"0d8v48y94z8maxkmw1rv7v9m0jg2dc8xbp581njb6yhr7abwqdv3\";\n  };\n\n  buildInputs = lib.optional stdenv.isDarwin shortenPerlShebang;\n  postInstall = lib.optional stdenv.isDarwin ''\n    shortenPerlShebang $out\/bin\/exiftool\n  '';\n};\n\nThis will remove the -I flags from the shebang line, rewrite them in the use lib form, and put them on the next line\ninstead. This function can be given any number of Perl scripts as arguments; it will modify them in-place.\n\nGeneration from CPANNix expressions for Perl packages can be generated (almost) automatically from CPAN. This is done by\nthe program nix-generate-from-cpan, which can be installed as follows:\n\n$ nix-env -i nix-generate-from-cpan\n\nThis program takes a Perl module name, looks it up on CPAN, fetches and unpacks the corresponding package, and prints a\nNix expression on standard output. For example:\n\n$ nix-generate-from-cpan XML::Simple\n  XMLSimple = buildPerlPackage rec {\n    name = \"XML-Simple-2.22\";\n    src = fetchurl {\n      url = \"mirror:\/\/cpan\/authors\/id\/G\/GR\/GRANTM\/${name}.tar.gz\";\n      sha256 = \"b9450ef22ea9644ae5d6ada086dc4300fa105be050a2030ebd4efd28c198eb49\";\n    };\n    propagatedBuildInputs = [ XMLNamespaceSupport XMLSAX XMLSAXExpat ];\n    meta = {\n      description = \"An API for simple XML files\";\n      license = with lib.licenses; [ artistic1 gpl1Plus ];\n    };\n  };\n\nThe output can be pasted into pkgs\/top-level\/perl-packages.nix or wherever else you need it.\n\nCross-compiling modulesNixpkgs has experimental support for cross-compiling Perl modules. In many cases, it will just\nwork out of the box, even for modules with native extensions. Sometimes, however, the Makefile.PL for a module may\n(indirectly) import a native module. In that case, you will need to make a stub for that module that will satisfy the\nMakefile.PL and install it into lib\/perl5\/site_perl\/cross_perl\/${perl.version}. See the postInstall for DBI for an\nexample.\n" }
,{ "url": "languages-frameworks\/php\/", "title": "PHP", "text": "PHPUser GuideOverviewSeveral versions of PHP are available on Nix, each of which having a wide variety of extensions and\nlibraries available.\n\nThe different versions of PHP that nixpkgs provides are located under attributes named based on major and minor version\nnumber; e.g., php74 is PHP 7.4.\n\nOnly versions of PHP that are supported by upstream for the entirety of a given NixOS release will be included in that\nrelease of NixOS. See PHP Supported Versions.\n\nThe attribute php refers to the version of PHP considered most stable and thoroughly tested in nixpkgs for any given\nrelease of NixOS - not necessarily the latest major release from upstream.\n\nAll available PHP attributes are wrappers around their respective binary PHP package and provide commonly used\nextensions this way. The real PHP 7.4 package, i.e. the unwrapped one, is available as php74.unwrapped; see the next\nsection for more details.\n\nInteractive tools built on PHP are put in php.packages; composer is for example available at php.packages.composer.\n\nMost extensions that come with PHP, as well as some popular third-party ones, are available in php.extensions; for\nexample, the opcache extension shipped with PHP is available at php.extensions.opcache and the third-party ImageMagick\nextension at php.extensions.imagick.\n\nInstalling PHP with extensionsA PHP package with specific extensions enabled can be built using php.withExtensions. This\nis a function which accepts an anonymous function as its only argument; the function should accept two named parameters:\nenabled - a list of currently enabled extensions and all - the set of all extensions, and return a list of wanted\nextensions. For example, a PHP package with all default extensions and ImageMagick enabled:\n\nphp.withExtensions ({ enabled, all }:\n  enabled ++ [ all.imagick ])\n\nTo exclude some, but not all, of the default extensions, you can filter the enabled list like this:\n\nphp.withExtensions ({ enabled, all }:\n  (lib.filter (e: e != php.extensions.opcache) enabled)\n  ++ [ all.imagick ])\n\nTo build your list of extensions from the ground up, you can simply ignore enabled:\n\nphp.withExtensions ({ all, ... }: with all; [ imagick opcache ])\n\nphp.withExtensions provides extensions by wrapping a minimal php base package, providing a php.ini file listing all\nextensions to be loaded. You can access this package through the php.unwrapped attribute; useful if you, for example,\nneed access to the dev output. The generated php.ini file can be accessed through the php.phpIni attribute.\n\nIf you want a PHP build with extra configuration in the php.ini file, you can use php.buildEnv. This function takes two\nnamed and optional parameters: extensions and extraConfig. extensions takes an extension specification equivalent to\nthat of php.withExtensions, extraConfig a string of additional php.ini configuration parameters. For example, a PHP\npackage with the opcache and ImageMagick extensions enabled, and memory_limit set to 256M:\n\nphp.buildEnv {\n  extensions = { all, ... }: with all; [ imagick opcache ];\n  extraConfig = \"memory_limit=256M\";\n}\n\nExample setup for phpfpmYou can use the previous examples in a phpfpm pool called foo as follows:\n\nlet\n  myPhp = php.withExtensions ({ all, ... }: with all; [ imagick opcache ]);\nin {\n  services.phpfpm.pools.\"foo\".phpPackage = myPhp;\n};\n\nlet\n  myPhp = php.buildEnv {\n    extensions = { all, ... }: with all; [ imagick opcache ];\n    extraConfig = \"memory_limit=256M\";\n  };\nin {\n  services.phpfpm.pools.\"foo\".phpPackage = myPhp;\n};\n\nExample usage with nix-shellThis brings up a temporary environment that contains a PHP interpreter with the extensions\nimagick and opcache enabled:\n\nnix-shell -p 'php.withExtensions ({ all, ... }: with all; [ imagick opcache ])'\n\nInstalling PHP packages with extensionsAll interactive tools use the PHP package you get them from, so all packages at\nphp.packages.* use the php package with its default extensions. Sometimes this default set of extensions isn't enough\nand you may want to extend it. A common case of this is the composer package: a project may depend on certain extensions\nand composer won't work with that project unless those extensions are loaded.\n\nExample of building composer with additional extensions:\n\n(php.withExtensions ({ all, enabled }:\n  enabled ++ (with all; [ imagick redis ]))\n).packages.composer\n\nOverriding PHP packagesphp-packages.nix form a scope, allowing us to override the packages defined within. For example,\nto apply a patch to a mysqlnd extension, you can simply pass an overlay-style function to php’s packageOverrides\nargument:\n\nphp.override {\n  packageOverrides = final: prev: {\n    extensions = prev.extensions \/\/ {\n      mysqlnd = prev.extensions.mysqlnd.overrideAttrs (attrs: {\n        patches = attrs.patches or [] ++ [\n          …\n        ];\n      });\n    };\n  };\n}\n" }
,{ "url": "languages-frameworks\/python\/", "title": "Python", "text": "PythonUser GuideUsing PythonOverviewSeveral versions of the Python interpreter are available on Nix, as well as a high\namount of packages. The attribute python3 refers to the default interpreter, which is currently CPython 3.8. The\nattribute python refers to CPython 2.7 for backwards-compatibility. It is also possible to refer to specific versions,\ne.g. python38 refers to CPython 3.8, and pypy refers to the default PyPy interpreter.\n\nPython is used a lot, and in different ways. This affects also how it is packaged. In the case of Python on Nix, an\nimportant distinction is made between whether the package is considered primarily an application, or whether it should\nbe used as a library, i.e., of primary interest are the modules in site-packages that should be importable.\n\nIn the Nixpkgs tree Python applications can be found throughout, depending on what they do, and are called from the main\npackage set. Python libraries, however, are in separate sets, with one set per interpreter version.\n\nThe interpreters have several common attributes. One of these attributes is pkgs, which is a package set of Python\nlibraries for this specific interpreter. E.g., the toolz package corresponding to the default interpreter is\npython.pkgs.toolz, and the CPython 3.8 version is python38.pkgs.toolz. The main package set contains aliases to these\npackage sets, e.g. pythonPackages refers to python.pkgs and python38Packages to python38.pkgs.\n\nInstalling Python and packagesThe Nix and NixOS manuals explain how packages are generally installed. In the case of\nPython and Nix, it is important to make a distinction between whether the package is considered an application or a\nlibrary.\n\nApplications on Nix are typically installed into your user profile imperatively using nix-env -i, and on NixOS\ndeclaratively by adding the package name to environment.systemPackages in \/etc\/nixos\/configuration.nix. Dependencies\nsuch as libraries are automatically installed and should not be installed explicitly.\n\nThe same goes for Python applications. Python applications can be installed in your profile, and will be wrapped to find\ntheir exact library dependencies, without impacting other applications or polluting your user environment.\n\nBut Python libraries you would like to use for development cannot be installed, at least not individually, because they\nwon't be able to find each other resulting in import errors. Instead, it is possible to create an environment with\npython.buildEnv or python.withPackages where the interpreter and other executables are wrapped to be able to find each\nother and all of the modules.\n\nIn the following examples we will start by creating a simple, ad-hoc environment with a nix-shell that has numpy and\ntoolz in Python 3.8; then we will create a re-usable environment in a single-file Python script; then we will create a\nfull Python environment for development with this same environment.\n\nPhilosphically, this should be familiar to users who are used to a venv style of development: individual projects create\ntheir own Python environments without impacting the global environment or each other.\n\nAd-hoc temporary Python environment with nix-shellThe simplest way to start playing with the way nix wraps and sets up\nPython environments is with nix-shell at the cmdline. These environments create a temporary shell session with a Python\nand a precise list of packages (plus their runtime dependencies), with no other Python packages in the Python\ninterpreter's scope.\n\nTo create a Python 3.8 session with numpy and toolz available, run:\n\n$ nix-shell -p 'python38.withPackages(ps: with ps; [ numpy toolz ])'\n\nBy default nix-shell will start a bash session with this interpreter in our PATH, so if we then run:\n\n[nix-shell:~\/src\/nixpkgs]$ python3\nPython 3.8.1 (default, Dec 18 2019, 19:06:26)\n[GCC 9.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import numpy; import toolz\n\nNote that no other modules are in scope, even if they were imperatively installed into our user environment as a\ndependency of a Python application:\n\n>>> import requests\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'requests'\n\nWe can add as many additional modules onto the nix-shell as we need, and we will still get 1 wrapped Python interpreter.\nWe can start the interpreter directly like so:\n\n$ nix-shell -p 'python38.withPackages(ps: with ps; [ numpy toolz requests ])' --run python3\nthese derivations will be built:\n  \/nix\/store\/xbdsrqrsfa1yva5s7pzsra8k08gxlbz1-python3-3.8.1-env.drv\nbuilding '\/nix\/store\/xbdsrqrsfa1yva5s7pzsra8k08gxlbz1-python3-3.8.1-env.drv'...\ncreated 277 symlinks in user environment\nPython 3.8.1 (default, Dec 18 2019, 19:06:26)\n[GCC 9.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import requests\n>>>\n\nNotice that this time it built a new Python environment, which now includes requests. Building an environment just\ncreates wrapper scripts that expose the selected dependencies to the interpreter while re-using the actual modules. This\nmeans if any other env has installed requests or numpy in a different context, we don't need to recompile them -- we\njust recompile the wrapper script that sets up an interpreter pointing to them. This matters much more for \"big\" modules\nlike pytorch or tensorflow.\n\nModule names usually match their names on pypi.org, but you can use the Nixpkgs search website to find them as well\n(along with non-python packages).\n\nAt this point we can create throwaway experimental Python environments with arbitrary dependencies. This is a good way\nto get a feel for how the Python interpreter and dependencies work in Nix and NixOS, but to do some actual development,\nwe'll want to make it a bit more persistent.\n\nRunning Python scripts and using nix-shell as shebangSometimes, we have a script whose header looks like this:\n\n#!\/usr\/bin\/env python3\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n\nExecuting this script requires a python3 that has numpy. Using what we learned in the previous section, we could startup\na shell and just run it like so:\n\n$ nix-shell -p 'python38.withPackages(ps: with ps; [ numpy ])' --run 'python3 foo.py'\nThe dot product of [1 2] and [3 4] is: 11\n\nBut if we maintain the script ourselves, and if there are more dependencies, it may be nice to encode those dependencies\nin source to make the script re-usable without that bit of knowledge. That can be done by using nix-shell as a shebang,\nlike so:\n\n#!\/usr\/bin\/env nix-shell\n#!nix-shell -i python3 -p \"python3.withPackages(ps: [ ps.numpy ])\"\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n\nThen we simply execute it, without requiring any environment setup at all!\n\n$ .\/foo.py\nThe dot product of [1 2] and [3 4] is: 11\n\nIf the dependencies are not available on the host where foo.py is executed, it will build or download them from a Nix\nbinary cache prior to starting up, prior that it is executed on a machine with a multi-user nix installation.\n\nThis provides a way to ship a self bootstrapping Python script, akin to a statically linked binary, where it can be run\non any machine (provided nix is installed) without having to assume that numpy is installed globally on the system.\n\nBy default it is pulling the import checkout of Nixpkgs itself from our nix channel, which is nice as it cache aligns\nwith our other package builds, but we can make it fully reproducible by pinning the nixpkgs import:\n\n#!\/usr\/bin\/env nix-shell\n#!nix-shell -i python3 -p \"python3.withPackages(ps: [ ps.numpy ])\"\n#!nix-shell -I nixpkgs=https:\/\/github.com\/NixOS\/nixpkgs\/archive\/d373d80b1207d52621961b16aa4a3438e4f98167.tar.gz\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n\nThis will execute with the exact same versions of Python 3.8, numpy, and system dependencies a year from now as it does\ntoday, because it will always use exactly git commit d373d80b1207d52621961b16aa4a3438e4f98167 of Nixpkgs for all of the\npackage versions.\n\nThis is also a great way to ensure the script executes identically on different servers.\n\nLoad environment from .nix expressionWe've now seen how to create an ad-hoc temporary shell session, and how to create a\nsingle script with Python dependencies, but in the course of normal development we're usually working in an entire\npackage repository.\n\nAs explained in the Nix manual, nix-shell can also load an expression from a .nix file. Say we want to have Python 3.8,\nnumpy and toolz, like before, in an environment. We can add a shell.nix file describing our dependencies:\n\nwith import <nixpkgs> {};\n(python38.withPackages (ps: [ps.numpy ps.toolz])).env\n\nAnd then at the command line, just typing nix-shell produces the same environment as before. In a normal project, we'll\nlikely have many more dependencies; this can provide a way for developers to share the environments with each other and\nwith CI builders.\n\nWhat's happening here?\n\n1.  We begin with importing the Nix Packages collections. import <nixpkgs> imports the <nixpkgs> function, {} calls it\n    and the with statement brings all attributes of nixpkgs in the local scope. These attributes form the main package\n    set.\n2.  Then we create a Python 3.8 environment with the withPackages function, as before.\n3.  The withPackages function expects us to provide a function as an argument that takes the set of all Python packages\n    and returns a list of packages to include in the environment. Here, we select the packages numpy and toolz from the\n    package set.\n\nTo combine this with mkShell you can:\n\nwith import <nixpkgs> {};\nlet\n  pythonEnv = python38.withPackages (ps: [\n    ps.numpy\n    ps.toolz\n  ]);\nin mkShell {\n  packages = [\n    pythonEnv\n\n    black\n    mypy\n\n    libffi\n    openssl\n  ];\n}\n\nThis will create a unified environment that has not just our Python interpreter and its Python dependencies, but also\ntools like black or mypy and libraries like libffi the openssl in scope. This is generic and can span any number of\ntools or languages across the Nixpkgs ecosystem.\n\nInstalling environments globally on the systemUp to now, we've been creating environments scoped to an ad-hoc shell\nsession, or a single script, or a single project. This is generally advisable, as it avoids pollution across contexts.\n\nHowever, sometimes we know we will often want a Python with some basic packages, and want this available without having\nto enter into a shell or build context. This can be useful to have things like vim\/emacs editors and plugins or shell\ntools \"just work\" without having to set them up, or when running other software that expects packages to be installed\nglobally.\n\nTo create your own custom environment, create a file in ~\/.config\/nixpkgs\/overlays\/ that looks like this:\n\n# ~\/.config\/nixpkgs\/overlays\/myEnv.nix\nself: super: {\n  myEnv = super.buildEnv {\n    name = \"myEnv\";\n    paths = [\n      # A Python 3 interpreter with some packages\n      (self.python3.withPackages (\n        ps: with ps; [\n          pyflakes\n          pytest\n          python-language-server\n        ]\n      ))\n\n      # Some other packages we'd like as part of this env\n      self.mypy\n      self.black\n      self.ripgrep\n      self.tmux\n    ];\n  };\n}\n\nYou can then build and install this to your profile with:\n\nnix-env -iA myEnv\n\nOne limitation of this is that you can only have 1 Python env installed globally, since they conflict on the python to\nload out of your PATH.\n\nIf you get a conflict or prefer to keep the setup clean, you can have nix-env atomically uninstall all other\nimperatively installed packages and replace your profile with just myEnv by using the --replace flag.\n\nEnvironment defined in \/etc\/nixos\/configuration.nixFor the sake of completeness, here's how to install the environment\nsystem-wide on NixOS.\n\n{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (python38.withPackages(ps: with ps; [ numpy toolz ]))\n  ];\n}\n\nDeveloping with PythonAbove, we were mostly just focused on use cases and what to do to get started creating working\nPython environments in nix.\n\nNow that you know the basics to be up and running, it is time to take a step back and take a deeper look at how Python\npackages are packaged on Nix. Then, we will look at how you can use development mode with your code.\n\nPython library packages in NixpkgsWith Nix all packages are built by functions. The main function in Nix for building\nPython libraries is buildPythonPackage. Let's see how we can build the toolz package.\n\n{ lib, buildPythonPackage, fetchPypi }:\n\nbuildPythonPackage rec {\n  pname = \"toolz\";\n  version = \"0.10.0\";\n\n  src = fetchPypi {\n    inherit pname version;\n    sha256 = \"08fdd5ef7c96480ad11c12d472de21acd32359996f69a5259299b540feba4560\";\n  };\n\n  doCheck = false;\n\n  meta = with lib; {\n    homepage = \"https:\/\/github.com\/pytoolz\/toolz\";\n    description = \"List processing tools and functional utilities\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\nWhat happens here? The function buildPythonPackage is called and as argument it accepts a set. In this case the set is a\nrecursive set, rec. One of the arguments is the name of the package, which consists of a basename (generally following\nthe name on PyPi) and a version. Another argument, src specifies the source, which in this case is fetched from PyPI\nusing the helper function fetchPypi. The argument doCheck is used to set whether tests should be run when building the\npackage. Furthermore, we specify some (optional) meta information. The output of the function is a derivation.\n\nAn expression for toolz can be found in the Nixpkgs repository. As explained in the introduction of this Python section,\na derivation of toolz is available for each interpreter version, e.g. python38.pkgs.toolz refers to the toolz derivation\ncorresponding to the CPython 3.8 interpreter.\n\nThe above example works when you're directly working on pkgs\/top-level\/python-packages.nix in the Nixpkgs repository.\nOften though, you will want to test a Nix expression outside of the Nixpkgs tree.\n\nThe following expression creates a derivation for the toolz package, and adds it along with a numpy package to a Python\nenvironment.\n\nwith import <nixpkgs> {};\n\n( let\n    my_toolz = python38.pkgs.buildPythonPackage rec {\n      pname = \"toolz\";\n      version = \"0.10.0\";\n\n      src = python38.pkgs.fetchPypi {\n        inherit pname version;\n        sha256 = \"08fdd5ef7c96480ad11c12d472de21acd32359996f69a5259299b540feba4560\";\n      };\n\n      doCheck = false;\n\n      meta = {\n        homepage = \"https:\/\/github.com\/pytoolz\/toolz\/\";\n        description = \"List processing tools and functional utilities\";\n      };\n    };\n\n  in python38.withPackages (ps: [ps.numpy my_toolz])\n).env\n\nExecuting nix-shell will result in an environment in which you can use Python 3.8 and the toolz package. As you can see\nwe had to explicitly mention for which Python version we want to build a package.\n\nSo, what did we do here? Well, we took the Nix expression that we used earlier to build a Python environment, and said\nthat we wanted to include our own version of toolz, named my_toolz. To introduce our own package in the scope of\nwithPackages we used a let expression. You can see that we used ps.numpy to select numpy from the nixpkgs package set\n(ps). We did not take toolz from the Nixpkgs package set this time, but instead took our own version that we introduced\nwith the let expression.\n\nHandling dependenciesOur example, toolz, does not have any dependencies on other Python packages or system libraries.\nAccording to the manual, buildPythonPackage uses the arguments buildInputs and propagatedBuildInputs to specify\ndependencies. If something is exclusively a build-time dependency, then the dependency should be included in\nbuildInputs, but if it is (also) a runtime dependency, then it should be added to propagatedBuildInputs. Test\ndependencies are considered build-time dependencies and passed to checkInputs.\n\nThe following example shows which arguments are given to buildPythonPackage in order to build datashape.\n\n{ lib, buildPythonPackage, fetchPypi, numpy, multipledispatch, dateutil, pytest }:\n\nbuildPythonPackage rec {\n  pname = \"datashape\";\n  version = \"0.4.7\";\n\n  src = fetchPypi {\n    inherit pname version;\n    sha256 = \"14b2ef766d4c9652ab813182e866f493475e65e558bed0822e38bf07bba1a278\";\n  };\n\n  checkInputs = [ pytest ];\n  propagatedBuildInputs = [ numpy multipledispatch dateutil ];\n\n  meta = with lib; {\n    homepage = \"https:\/\/github.com\/ContinuumIO\/datashape\";\n    description = \"A data description language\";\n    license = licenses.bsd2;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\nWe can see several runtime dependencies, numpy, multipledispatch, and dateutil. Furthermore, we have one checkInputs,\ni.e. pytest. pytest is a test runner and is only used during the checkPhase and is therefore not added to\npropagatedBuildInputs.\n\nIn the previous case we had only dependencies on other Python packages to consider. Occasionally you have also system\nlibraries to consider. E.g., lxml provides Python bindings to libxml2 and libxslt. These libraries are only required\nwhen building the bindings and are therefore added as buildInputs.\n\n{ lib, pkgs, buildPythonPackage, fetchPypi }:\n\nbuildPythonPackage rec {\n  pname = \"lxml\";\n  version = \"3.4.4\";\n\n  src = fetchPypi {\n    inherit pname version;\n    sha256 = \"16a0fa97hym9ysdk3rmqz32xdjqmy4w34ld3rm3jf5viqjx65lxk\";\n  };\n\n  buildInputs = [ pkgs.libxml2 pkgs.libxslt ];\n\n  meta = with lib; {\n    description = \"Pythonic binding for the libxml2 and libxslt libraries\";\n    homepage = \"https:\/\/lxml.de\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ sjourdois ];\n  };\n}\n\nIn this example lxml and Nix are able to work out exactly where the relevant files of the dependencies are. This is not\nalways the case.\n\nThe example below shows bindings to The Fastest Fourier Transform in the West, commonly known as FFTW. On Nix we have\nseparate packages of FFTW for the different types of floats (\"single\", \"double\", \"long-double\"). The bindings need all\nthree types, and therefore we add all three as buildInputs. The bindings don't expect to find each of them in a\ndifferent folder, and therefore we have to set LDFLAGS and CFLAGS.\n\n{ lib, pkgs, buildPythonPackage, fetchPypi, numpy, scipy }:\n\nbuildPythonPackage rec {\n  pname = \"pyFFTW\";\n  version = \"0.9.2\";\n\n  src = fetchPypi {\n    inherit pname version;\n    sha256 = \"f6bbb6afa93085409ab24885a1a3cdb8909f095a142f4d49e346f2bd1b789074\";\n  };\n\n  buildInputs = [ pkgs.fftw pkgs.fftwFloat pkgs.fftwLongDouble];\n\n  propagatedBuildInputs = [ numpy scipy ];\n\n  # Tests cannot import pyfftw. pyfftw works fine though.\n  doCheck = false;\n\n  preConfigure = ''\n    export LDFLAGS=\"-L${pkgs.fftw.dev}\/lib -L${pkgs.fftwFloat.out}\/lib -L${pkgs.fftwLongDouble.out}\/lib\"\n    export CFLAGS=\"-I${pkgs.fftw.dev}\/include -I${pkgs.fftwFloat.dev}\/include -I${pkgs.fftwLongDouble.dev}\/include\"\n  '';\n\n  meta = with lib; {\n    description = \"A pythonic wrapper around FFTW, the FFT library, presenting a unified interface for all the supported transforms\";\n    homepage = \"http:\/\/hgomersall.github.com\/pyFFTW\";\n    license = with licenses; [ bsd2 bsd3 ];\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\nNote also the line doCheck = false;, we explicitly disabled running the test-suite.\n\nTesting Python PackagesIt is highly encouraged to have testing as part of the package build. This helps to avoid\nsituations where the package was able to build and install, but is not usable at runtime. Currently, all packages will\nuse the test command provided by the setup.py (i.e. python setup.py test). However, this is currently deprecated\nhttps:\/\/github.com\/pypa\/setuptools\/pull\/1878 and your package should provide its own checkPhase.\n\nNOTE: The checkPhase for python maps to the installCheckPhase on a normal derivation. This is due to many python\npackages not behaving well to the pre-installed version of the package. Version info, and natively compiled extensions\ngenerally only exist in the install directory, and thus can cause issues when a test suite asserts on that behavior.\n\nNOTE: Tests should only be disabled if they don't agree with nix (e.g. external dependencies, network access, flakey\ntests), however, as many tests should be enabled as possible. Failing tests can still be a good indication that the\npackage is not in a valid state.\n\nUsing pytestPytest is the most common test runner for python repositories. A trivial test run would be:\n\n  checkInputs = [ pytest ];\n  checkPhase = \"pytest\";\n\nHowever, many repositories' test suites do not translate well to nix's build sandbox, and will generally need many tests\nto be disabled.\n\nTo filter tests using pytest, one can do the following:\n\n  checkInputs = [ pytest ];\n  # avoid tests which need additional data or touch network\n  checkPhase = ''\n    pytest tests\/ --ignore=tests\/integration -k 'not download and not update'\n  '';\n\n--ignore will tell pytest to ignore that file or directory from being collected as part of a test run. This is useful is\na file uses a package which is not available in nixpkgs, thus skipping that test file is much easier than having to\ncreate a new package.\n\n-k is used to define a predicate for test names. In this example, we are filtering out tests which contain download or\nupdate in their test case name. Only one -k argument is allowed, and thus a long predicate should be concatenated with\n“\\” and wrapped to the next line.\n\nNOTE: In pytest==6.0.1, the use of “\\” to continue a line (e.g. -k 'not download \\') has been removed, in this case,\nit's recommended to use pytestCheckHook.\n\nUsing pytestCheckHookpytestCheckHook is a convenient hook which will substitute the setuptools test command for a\ncheckPhase which runs pytest. This is also beneficial when a package may need many items disabled to run the test suite.\n\nUsing the example above, the analagous pytestCheckHook usage would be:\n\n  checkInputs = [ pytestCheckHook ];\n\n  # requires additional data\n  pytestFlagsArray = [ \"tests\/\" \"--ignore=tests\/integration\" ];\n\n  disabledTests = [\n    # touches network\n    \"download\"\n    \"update\"\n  ];\n\n  disabledTestPaths = [\n    \"tests\/test_failing.py\"\n  ];\n\nThis is expecially useful when tests need to be conditionallydisabled, for example:\n\n  disabledTests = [\n    # touches network\n    \"download\"\n    \"update\"\n  ] ++ lib.optionals (pythonAtLeast \"3.8\") [\n    # broken due to python3.8 async changes\n    \"async\"\n  ] ++ lib.optionals stdenv.isDarwin [\n    # can fail when building with other packages\n    \"socket\"\n  ];\n\nTrying to concatenate the related strings to disable tests in a regular checkPhase would be much harder to read. This\nalso enables us to comment on why specific tests are disabled.\n\nUsing pythonImportsCheckAlthough unit tests are highly prefered to validate correctness of a package, not all packages\nhave test suites that can be ran easily, and some have none at all. To help ensure the package still works,\npythonImportsCheck can attempt to import the listed modules.\n\n  pythonImportsCheck = [ \"requests\" \"urllib\" ];\n\nroughly translates to:\n\n  postCheck = ''\n    PYTHONPATH=$out\/${python.sitePackages}:$PYTHONPATH\n    python -c \"import requests; import urllib\"\n  '';\n\nHowever, this is done in it's own phase, and not dependent on whether doCheck = true;\n\nThis can also be useful in verifying that the package doesn't assume commonly present packages (e.g. setuptools)\n\nDevelop local packageAs a Python developer you're likely aware of development mode (python setup.py develop); instead of\ninstalling the package this command creates a special link to the project code. That way, you can run updated code\nwithout having to reinstall after each and every change you make. Development mode is also available. Let's see how you\ncan use it.\n\nIn the previous Nix expression the source was fetched from an url. We can also refer to a local source instead using src\n= .\/path\/to\/source\/tree;\n\nIf we create a shell.nix file which calls buildPythonPackage, and if src is a local source, and if the local source has\na setup.py, then development mode is activated.\n\nIn the following example we create a simple environment that has a Python 3.8 version of our package in it, as well as\nits dependencies and other packages we like to have in the environment, all specified with propagatedBuildInputs.\nIndeed, we can just add any package we like to have in our environment to propagatedBuildInputs.\n\nwith import <nixpkgs> {};\nwith python38Packages;\n\nbuildPythonPackage rec {\n  name = \"mypackage\";\n  src = .\/path\/to\/package\/source;\n  propagatedBuildInputs = [ pytest numpy pkgs.libsndfile ];\n}\n\nIt is important to note that due to how development mode is implemented on Nix it is not possible to have multiple\npackages simultaneously in development mode.\n\nOrganising your packagesSo far we discussed how you can use Python on Nix, and how you can develop with it. We've looked\nat how you write expressions to package Python packages, and we looked at how you can create environments in which\nspecified packages are available.\n\nAt some point you'll likely have multiple packages which you would like to be able to use in different projects. In\norder to minimise unnecessary duplication we now look at how you can maintain a repository with your own packages. The\nimportant functions here are import and callPackage.\n\nIncluding a derivation using callPackageEarlier we created a Python environment using withPackages, and included the\ntoolz package via a let expression. Let's split the package definition from the environment definition.\n\nWe first create a function that builds toolz in ~\/path\/to\/toolz\/release.nix\n\n{ lib, buildPythonPackage }:\n\nbuildPythonPackage rec {\n  pname = \"toolz\";\n  version = \"0.10.0\";\n\n  src = fetchPypi {\n    inherit pname version;\n    sha256 = \"08fdd5ef7c96480ad11c12d472de21acd32359996f69a5259299b540feba4560\";\n  };\n\n  meta = with lib; {\n    homepage = \"https:\/\/github.com\/pytoolz\/toolz\/\";\n    description = \"List processing tools and functional utilities\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\nIt takes an argument buildPythonPackage. We now call this function using callPackage in the definition of our\nenvironment\n\nwith import <nixpkgs> {};\n\n( let\n    toolz = callPackage \/path\/to\/toolz\/release.nix {\n      buildPythonPackage = python38Packages.buildPythonPackage;\n    };\n  in python38.withPackages (ps: [ ps.numpy toolz ])\n).env\n\nImportant to remember is that the Python version for which the package is made depends on the python derivation that is\npassed to buildPythonPackage. Nix tries to automatically pass arguments when possible, which is why generally you don't\nexplicitly define which python derivation should be used. In the above example we use buildPythonPackage that is part of\nthe set python38Packages, and in this case the python38 interpreter is automatically used.\n\nReferenceInterpretersVersions 2.7, 3.6, 3.7, 3.8 and 3.9 of the CPython interpreter are available as respectively\npython27, python36, python37, python38 and python39. The aliases python2 and python3 correspond to respectively python27\nand python39. The attribute python maps to python2. The PyPy interpreters compatible with Python 2.7 and 3 are available\nas pypy27 and pypy3, with aliases pypy2 mapping to pypy27 and pypy mapping to pypy2. The Nix expressions for the\ninterpreters can be found in pkgs\/development\/interpreters\/python.\n\nAll packages depending on any Python interpreter get appended out\/{python.sitePackages} to $PYTHONPATH if such directory\nexists.\n\nMissing tkinter module standard libraryTo reduce closure size the Tkinter\/tkinter is available as a separate package,\npythonPackages.tkinter.\n\nAttributes on interpreters packagesEach interpreter has the following attributes:\n\n  - libPrefix. Name of the folder in ${python}\/lib\/ for corresponding interpreter.\n  - interpreter. Alias for ${python}\/bin\/${executable}.\n  - buildEnv. Function to build python interpreter environments with extra packages bundled together. See section\n    python.buildEnv function for usage and documentation.\n  - withPackages. Simpler interface to buildEnv. See section python.withPackages function for usage and documentation.\n  - sitePackages. Alias for lib\/${libPrefix}\/site-packages.\n  - executable. Name of the interpreter executable, e.g. python3.8.\n  - pkgs. Set of Python packages for that specific interpreter. The package set can be modified by overriding the\n    interpreter and passing packageOverrides.\n\nOptimizationsThe Python interpreters are by default not build with optimizations enabled, because the builds are in that\ncase not reproducible. To enable optimizations, override the interpreter of interest, e.g using\n\nlet\n  pkgs = import .\/. {};\n  mypython = pkgs.python3.override {\n    enableOptimizations = true;\n    reproducibleBuild = false;\n    self = mypython;\n  };\nin mypython\n\nBuilding packages and applicationsPython libraries and applications that use setuptools or distutils are typically built\nwith respectively the buildPythonPackage and buildPythonApplication functions. These two functions also support\ninstalling a wheel.\n\nAll Python packages reside in pkgs\/top-level\/python-packages.nix and all applications elsewhere. In case a package is\nused as both a library and an application, then the package should be in pkgs\/top-level\/python-packages.nix since only\nthose packages are made available for all interpreter versions. The preferred location for library expressions is in\npkgs\/development\/python-modules. It is important that these packages are called from pkgs\/top-level\/python-packages.nix\nand not elsewhere, to guarantee the right version of the package is built.\n\nBased on the packages defined in pkgs\/top-level\/python-packages.nix an attribute set is created for each available\nPython interpreter. The available sets are\n\n  - pkgs.python27Packages\n  - pkgs.python36Packages\n  - pkgs.python37Packages\n  - pkgs.python38Packages\n  - pkgs.python39Packages\n  - pkgs.pypyPackages\n\nand the aliases\n\n  - pkgs.python2Packages pointing to pkgs.python27Packages\n  - pkgs.python3Packages pointing to pkgs.python38Packages\n  - pkgs.pythonPackages pointing to pkgs.python2Packages\n\nbuildPythonPackage functionThe buildPythonPackage function is implemented in\npkgs\/development\/interpreters\/python\/mk-python-derivation using setup hooks.\n\nThe following is an example:\n\n{ lib, buildPythonPackage, fetchPypi, hypothesis, setuptools-scm, attrs, py, setuptools, six, pluggy }:\n\nbuildPythonPackage rec {\n  pname = \"pytest\";\n  version = \"3.3.1\";\n\n  src = fetchPypi {\n    inherit pname version;\n    sha256 = \"cf8436dc59d8695346fcd3ab296de46425ecab00d64096cebe79fb51ecb2eb93\";\n  };\n\n  postPatch = ''\n    # don't test bash builtins\n    rm testing\/test_argcomplete.py\n  '';\n\n  checkInputs = [ hypothesis ];\n  nativeBuildInputs = [ setuptools-scm ];\n  propagatedBuildInputs = [ attrs py setuptools six pluggy ];\n\n  meta = with lib; {\n    maintainers = with maintainers; [ domenkozar lovek323 madjar lsix ];\n    description = \"Framework for writing tests\";\n  };\n}\n\nThe buildPythonPackage mainly does four things:\n\n  - In the buildPhase, it calls ${python.interpreter} setup.py bdist_wheel to build a wheel binary zipfile.\n  - In the installPhase, it installs the wheel file using pip install *.whl.\n  - In the postFixup phase, the wrapPythonPrograms bash function is called to wrap all programs in the $out\/bin\/*\n    directory to include $PATH environment variable and add dependent libraries to script's sys.path.\n  - In the installCheck phase, ${python.interpreter} setup.py test is ran.\n\nBy default tests are run because doCheck = true. Test dependencies, like e.g. the test runner, should be added to\ncheckInputs.\n\nBy default meta.platforms is set to the same value as the interpreter unless overridden otherwise.\n\nbuildPythonPackage parametersAll parameters from stdenv.mkDerivation function are still supported. The following are\nspecific to buildPythonPackage:\n\n  - catchConflicts ? true: If true, abort package build if a package name appears more than once in dependency tree.\n    Default is true.\n  - disabled ? false: If true, package is not built for the particular Python interpreter version.\n  - dontWrapPythonPrograms ? false: Skip wrapping of Python programs.\n  - permitUserSite ? false: Skip setting the PYTHONNOUSERSITE environment variable in wrapped programs.\n  - format ? \"setuptools\": Format of the source. Valid options are \"setuptools\", \"pyproject\", \"flit\", \"wheel\", and\n    \"other\". \"setuptools\" is for when the source has a setup.py and setuptools is used to build a wheel, flit, in case\n    flit should be used to build a wheel, and wheel in case a wheel is provided. Use other when a custom buildPhase\n    and\/or installPhase is needed.\n  - makeWrapperArgs ? []: A list of strings. Arguments to be passed to makeWrapper, which wraps generated binaries. By\n    default, the arguments to makeWrapper set PATH and PYTHONPATH environment variables before calling the binary.\n    Additional arguments here can allow a developer to set environment variables which will be available when the binary\n    is run. For example, makeWrapperArgs = [\"--set FOO BAR\" \"--set BAZ QUX\"].\n  - namePrefix: Prepends text to ${name} parameter. In case of libraries, this defaults to \"python3.8-\" for Python 3.8,\n    etc., and in case of applications to \"\".\n  - pipInstallFlags ? []: A list of strings. Arguments to be passed to pip install. To pass options to python setup.py\n    install, use --install-option. E.g., pipInstallFlags=[\"--install-option='--cpp_implementation'\"].\n  - pythonPath ? []: List of packages to be added into $PYTHONPATH. Packages in pythonPath are not propagated (contrary\n    to propagatedBuildInputs).\n  - preShellHook: Hook to execute commands before shellHook.\n  - postShellHook: Hook to execute commands after shellHook.\n  - removeBinByteCode ? true: Remove bytecode from \/bin. Bytecode is only created when the filenames end with .py.\n  - setupPyGlobalFlags ? []: List of flags passed to setup.py command.\n  - setupPyBuildFlags ? []: List of flags passed to setup.py build_ext command.\n\nThe stdenv.mkDerivation function accepts various parameters for describing build inputs (see \"Specifying dependencies\").\nThe following are of special interest for Python packages, either because these are primarily used, or because their\nbehaviour is different:\n\n  - nativeBuildInputs ? []: Build-time only dependencies. Typically executables as well as the items listed in\n    setup_requires.\n  - buildInputs ? []: Build and\/or run-time dependencies that need to be compiled for the host machine. Typically\n    non-Python libraries which are being linked.\n  - checkInputs ? []: Dependencies needed for running the checkPhase. These are added to nativeBuildInputs when doCheck\n    = true. Items listed in tests_require go here.\n  - propagatedBuildInputs ? []: Aside from propagating dependencies, buildPythonPackage also injects code into and wraps\n    executables with the paths included in this list. Items listed in install_requires go here.\n\nOverriding Python packagesThe buildPythonPackage function has a overridePythonAttrs method that can be used to override\nthe package. In the following example we create an environment where we have the blaze package using an older version of\npandas. We override first the Python interpreter and pass packageOverrides which contains the overrides for packages in\nthe package set.\n\nwith import <nixpkgs> {};\n\n(let\n  python = let\n    packageOverrides = self: super: {\n      pandas = super.pandas.overridePythonAttrs(old: rec {\n        version = \"0.19.1\";\n        src =  super.fetchPypi {\n          pname = \"pandas\";\n          inherit version;\n          sha256 = \"08blshqj9zj1wyjhhw3kl2vas75vhhicvv72flvf1z3jvapgw295\";\n        };\n      });\n    };\n  in pkgs.python3.override {inherit packageOverrides; self = python;};\n\nin python.withPackages(ps: [ps.blaze])).env\n\nbuildPythonApplication functionThe buildPythonApplication function is practically the same as buildPythonPackage. The\nmain purpose of this function is to build a Python package where one is interested only in the executables, and not\nimportable modules. For that reason, when adding this package to a python.buildEnv, the modules won't be made available.\n\nAnother difference is that buildPythonPackage by default prefixes the names of the packages with the version of the\ninterpreter. Because this is irrelevant for applications, the prefix is omitted.\n\nWhen packaging a Python application with buildPythonApplication, it should be called with callPackage and passed python\nor pythonPackages (possibly specifying an interpreter version), like this:\n\n{ lib, python3Packages }:\n\npython3Packages.buildPythonApplication rec {\n  pname = \"luigi\";\n  version = \"2.7.9\";\n\n  src = python3Packages.fetchPypi {\n    inherit pname version;\n    sha256 = \"035w8gqql36zlan0xjrzz9j4lh9hs0qrsgnbyw07qs7lnkvbdv9x\";\n  };\n\n  propagatedBuildInputs = with python3Packages; [ tornado_4 python-daemon ];\n\n  meta = with lib; {\n    ...\n  };\n}\n\nThis is then added to all-packages.nix just as any other application would be.\n\nluigi = callPackage ..\/applications\/networking\/cluster\/luigi { };\n\nSince the package is an application, a consumer doesn't need to care about Python versions or modules, which is why they\ndon't go in pythonPackages.\n\ntoPythonApplication functionA distinction is made between applications and libraries, however, sometimes a package is\nused as both. In this case the package is added as a library to python-packages.nix and as an application to\nall-packages.nix. To reduce duplication the toPythonApplication can be used to convert a library to an application.\n\nThe Nix expression shall use buildPythonPackage and be called from python-packages.nix. A reference shall be created\nfrom all-packages.nix to the attribute in python-packages.nix, and the toPythonApplication shall be applied to the\nreference:\n\nyoutube-dl = with pythonPackages; toPythonApplication youtube-dl;\n\ntoPythonModule functionIn some cases, such as bindings, a package is created using stdenv.mkDerivation and added as\nattribute in all-packages.nix. The Python bindings should be made available from python-packages.nix. The toPythonModule\nfunction takes a derivation and makes certain Python-specific modifications.\n\nopencv = toPythonModule (pkgs.opencv.override {\n  enablePython = true;\n  pythonPackages = self;\n});\n\nDo pay attention to passing in the right Python version!\n\npython.buildEnv functionPython environments can be created using the low-level pkgs.buildEnv function. This example\nshows how to create an environment that has the Pyramid Web Framework. Saving the following as default.nix\n\nwith import <nixpkgs> {};\n\npython.buildEnv.override {\n  extraLibs = [ pythonPackages.pyramid ];\n  ignoreCollisions = true;\n}\n\nand running nix-build will create\n\n\/nix\/store\/cf1xhjwzmdki7fasgr4kz6di72ykicl5-python-2.7.8-env\n\nwith wrapped binaries in bin\/.\n\nYou can also use the env attribute to create local environments with needed packages installed. This is somewhat\ncomparable to virtualenv. For example, running nix-shell with the following shell.nix\n\nwith import <nixpkgs> {};\n\n(python3.buildEnv.override {\n  extraLibs = with python3Packages; [ numpy requests ];\n}).env\n\nwill drop you into a shell where Python will have the specified packages in its path.\n\npython.buildEnv arguments  - extraLibs: List of packages installed inside the environment.\n  - postBuild: Shell command executed after the build of environment.\n  - ignoreCollisions: Ignore file collisions inside the environment (default is false).\n  - permitUserSite: Skip setting the PYTHONNOUSERSITE environment variable in wrapped binaries in the environment.\n\npython.withPackages functionThe python.withPackages function provides a simpler interface to the python.buildEnv\nfunctionality. It takes a function as an argument that is passed the set of python packages and returns the list of the\npackages to be included in the environment. Using the withPackages function, the previous example for the Pyramid Web\nFramework environment can be written like this:\n\nwith import <nixpkgs> {};\n\npython.withPackages (ps: [ps.pyramid])\n\nwithPackages passes the correct package set for the specific interpreter version as an argument to the function. In the\nabove example, ps equals pythonPackages. But you can also easily switch to using python3:\n\nwith import <nixpkgs> {};\n\npython3.withPackages (ps: [ps.pyramid])\n\nNow, ps is set to python3Packages, matching the version of the interpreter.\n\nAs python.withPackages simply uses python.buildEnv under the hood, it also supports the env attribute. The shell.nix\nfile from the previous section can thus be also written like this:\n\nwith import <nixpkgs> {};\n\n(python38.withPackages (ps: [ps.numpy ps.requests])).env\n\nIn contrast to python.buildEnv, python.withPackages does not support the more advanced options such as ignoreCollisions\n= true or postBuild. If you need them, you have to use python.buildEnv.\n\nPython 2 namespace packages may provide __init__.py that collide. In that case python.buildEnv should be used with\nignoreCollisions = true.\n\nSetup hooksThe following are setup hooks specifically for Python packages. Most of these are used in buildPythonPackage.\n\n  - eggUnpackhook to move an egg to the correct folder so it can be installed with the eggInstallHook\n  - eggBuildHook to skip building for eggs.\n  - eggInstallHook to install eggs.\n  - flitBuildHook to build a wheel using flit.\n  - pipBuildHook to build a wheel using pip and PEP 517. Note a build system (e.g. setuptools or flit) should still be\n    added as nativeBuildInput.\n  - pipInstallHook to install wheels.\n  - pytestCheckHook to run tests with pytest. See example usage.\n  - pythonCatchConflictsHook to check whether a Python package is not already existing.\n  - pythonImportsCheckHook to check whether importing the listed modules works.\n  - pythonRemoveBinBytecode to remove bytecode from the \/bin folder.\n  - setuptoolsBuildHook to build a wheel using setuptools.\n  - setuptoolsCheckHook to run tests with python setup.py test.\n  - venvShellHook to source a Python 3 venv at the venvDir location. A venv is created if it does not yet exist.\n    postVenvCreation can be used to to run commands only after venv is first created.\n  - wheelUnpackHook to move a wheel to the correct folder so it can be installed with the pipInstallHook.\n\nDevelopment modeDevelopment or editable mode is supported. To develop Python packages buildPythonPackage has additional\nlogic inside shellPhase to run pip install -e . --prefix $TMPDIR\/for the package.\n\nWarning: shellPhase is executed only if setup.py exists.\n\nGiven a default.nix:\n\nwith import <nixpkgs> {};\n\npythonPackages.buildPythonPackage {\n  name = \"myproject\";\n  buildInputs = with pythonPackages; [ pyramid ];\n\n  src = .\/.;\n}\n\nRunning nix-shell with no arguments should give you the environment in which the package would be built with nix-build.\n\nShortcut to setup environments with C headers\/libraries and Python packages:\n\nnix-shell -p pythonPackages.pyramid zlib libjpeg git\n\nNote: There is a boolean value lib.inNixShell set to true if nix-shell is invoked.\n\nToolsPackages inside nixpkgs are written by hand. However many tools exist in community to help save time. No tool is\npreferred at the moment.\n\n  - pypi2nix: Generate Nix expressions for your Python project. Note that sharing derivations from pypi2nix with nixpkgs\n    is possible but not encouraged.\n  - nixpkgs-pytools\n  - poetry2nix\n\nDeterministic buildsThe Python interpreters are now built deterministically. Minor modifications had to be made to the\ninterpreters in order to generate deterministic bytecode. This has security implications and is relevant for those using\nPython in a nix-shell.\n\nWhen the environment variable DETERMINISTIC_BUILD is set, all bytecode will have timestamp 1. The buildPythonPackage\nfunction sets DETERMINISTIC_BUILD=1 and PYTHONHASHSEED=0. Both are also exported in nix-shell.\n\nAutomatic testsIt is recommended to test packages as part of the build process. Source distributions (sdist) often\ninclude test files, but not always.\n\nBy default the command python setup.py test is run as part of the checkPhase, but often it is necessary to pass a custom\ncheckPhase. An example of such a situation is when py.test is used.\n\nCommon issues  - Non-working tests can often be deselected. By default buildPythonPackage runs python setup.py test.\n    Most Python modules follows the standard test protocol where the pytest runner can be used instead. py.test supports\n    a -k parameter to ignore test methods or classes:\n    \n    buildPythonPackage {\n      # ...\n      # assumes the tests are located in tests\n      checkInputs = [ pytest ];\n      checkPhase = ''\n        py.test -k 'not function_name and not other_function' tests\n      '';\n    }\n\n  - Tests that attempt to access $HOME can be fixed by using the following work-around before running tests (e.g.\n    preCheck): export HOME=$(mktemp -d)\n\nFAQHow to solve circular dependencies?Consider the packages A and B that depend on each other. When packaging B, a\nsolution is to override package A not to depend on B as an input. The same should also be done when packaging A.\n\nHow to override a Python package?We can override the interpreter and pass packageOverrides. In the following example we\nrename the pandas package and build it.\n\nwith import <nixpkgs> {};\n\n(let\n  python = let\n    packageOverrides = self: super: {\n      pandas = super.pandas.overridePythonAttrs(old: {name=\"foo\";});\n    };\n  in pkgs.python38.override {inherit packageOverrides;};\n\nin python.withPackages(ps: [ps.pandas])).env\n\nUsing nix-build on this expression will build an environment that contains the package pandas but with the new name foo.\n\nAll packages in the package set will use the renamed package. A typical use case is to switch to another version of a\ncertain package. For example, in the Nixpkgs repository we have multiple versions of django and scipy. In the following\nexample we use a different version of scipy and create an environment that uses it. All packages in the Python package\nset will now use the updated scipy version.\n\nwith import <nixpkgs> {};\n\n( let\n    packageOverrides = self: super: {\n      scipy = super.scipy_0_17;\n    };\n  in (pkgs.python38.override {inherit packageOverrides;}).withPackages (ps: [ps.blaze])\n).env\n\nThe requested package blaze depends on pandas which itself depends on scipy.\n\nIf you want the whole of Nixpkgs to use your modifications, then you can use overlays as explained in this manual. In\nthe following example we build a inkscape using a different version of numpy.\n\nlet\n  pkgs = import <nixpkgs> {};\n  newpkgs = import pkgs.path { overlays = [ (self: super: {\n    python38 = let\n      packageOverrides = python-self: python-super: {\n        numpy = python-super.numpy_1_18;\n      };\n    in super.python38.override {inherit packageOverrides;};\n  } ) ]; };\nin newpkgs.inkscape\n\npython setup.py bdist_wheel cannot create .whlExecuting python setup.py bdist_wheel in a nix-shell fails with\n\nValueError: ZIP does not support timestamps before 1980\n\nThis is because files from the Nix store (which have a timestamp of the UNIX epoch of January 1, 1970) are included in\nthe .ZIP, but .ZIP archives follow the DOS convention of counting timestamps from 1980.\n\nThe command bdist_wheel reads the SOURCE_DATE_EPOCH environment variable, which nix-shell sets to 1. Unsetting this\nvariable or giving it a value corresponding to 1980 or later enables building wheels.\n\nUse 1980 as timestamp:\n\nnix-shell --run \"SOURCE_DATE_EPOCH=315532800 python3 setup.py bdist_wheel\"\n\nor the current time:\n\nnix-shell --run \"SOURCE_DATE_EPOCH=$(date +%s) python3 setup.py bdist_wheel\"\n\nor unset SOURCE_DATE_EPOCH:\n\nnix-shell --run \"unset SOURCE_DATE_EPOCH; python3 setup.py bdist_wheel\"\n\ninstall_data \/ data_files problemsIf you get the following error:\n\ncould not create '\/nix\/store\/6l1bvljpy8gazlsw2aw9skwwp4pmvyxw-python-2.7.8\/etc':\nPermission denied\n\nThis is a known bug in setuptools. Setuptools install_data does not respect --prefix. An example of such package using\nthe feature is pkgs\/tools\/X11\/xpra\/default.nix.\n\nAs workaround install it as an extra preInstall step:\n\n${python.interpreter} setup.py install_data --install-dir=$out --root=$out\nsed -i '\/ = data\\_files\/d' setup.py\n\nRationale of non-existent global site-packagesOn most operating systems a global site-packages is maintained. This\nhowever becomes problematic if you want to run multiple Python versions or have multiple versions of certain libraries\nfor your projects. Generally, you would solve such issues by creating virtual environments using virtualenv.\n\nOn Nix each package has an isolated dependency tree which, in the case of Python, guarantees the right versions of the\ninterpreter and libraries or packages are available. There is therefore no need to maintain a global site-packages.\n\nIf you want to create a Python environment for development, then the recommended method is to use nix-shell, either with\nor without the python.buildEnv function.\n\nHow to consume Python modules using pip in a virtual environment like I am used to on other Operating Systems?While this\napproach is not very idiomatic from Nix perspective, it can still be useful when dealing with pre-existing projects or\nin situations where it's not feasible or desired to write derivations for all required dependencies.\n\nThis is an example of a default.nix for a nix-shell, which allows to consume a virtual environment created by venv, and\ninstall Python modules through pip the traditional way.\n\nCreate this default.nix file, together with a requirements.txt and simply execute nix-shell.\n\nwith import <nixpkgs> { };\n\nlet\n  pythonPackages = python3Packages;\nin pkgs.mkShell rec {\n  name = \"impurePythonEnv\";\n  venvDir = \".\/.venv\";\n  buildInputs = [\n    # A Python interpreter including the 'venv' module is required to bootstrap\n    # the environment.\n    pythonPackages.python\n\n    # This execute some shell code to initialize a venv in $venvDir before\n    # dropping into the shell\n    pythonPackages.venvShellHook\n\n    # Those are dependencies that we would like to use from nixpkgs, which will\n    # add them to PYTHONPATH and thus make them accessible from within the venv.\n    pythonPackages.numpy\n    pythonPackages.requests\n\n    # In this particular example, in order to compile any binary extensions they may\n    # require, the Python modules listed in the hypothetical requirements.txt need\n    # the following packages to be installed locally:\n    taglib\n    openssl\n    git\n    libxml2\n    libxslt\n    libzip\n    zlib\n  ];\n\n  # Run this command, only after creating the virtual environment\n  postVenvCreation = ''\n    unset SOURCE_DATE_EPOCH\n    pip install -r requirements.txt\n  '';\n\n  # Now we can execute any commands within the virtual environment.\n  # This is optional and can be left out to run pip manually.\n  postShellHook = ''\n    # allow pip to install wheels\n    unset SOURCE_DATE_EPOCH\n  '';\n\n}\n\nIn case the supplied venvShellHook is insufficient, or when Python 2 support is needed, you can define your own shell\nhook and adapt to your needs like in the following example:\n\nwith import <nixpkgs> { };\n\nlet\n  venvDir = \".\/.venv\";\n  pythonPackages = python3Packages;\nin pkgs.mkShell rec {\n  name = \"impurePythonEnv\";\n  buildInputs = [\n    pythonPackages.python\n    # Needed when using python 2.7\n    # pythonPackages.virtualenv\n    # ...\n  ];\n\n  # This is very close to how venvShellHook is implemented, but\n  # adapted to use 'virtualenv'\n  shellHook = ''\n    SOURCE_DATE_EPOCH=$(date +%s)\n\n    if [ -d \"${venvDir}\" ]; then\n      echo \"Skipping venv creation, '${venvDir}' already exists\"\n    else\n      echo \"Creating new venv environment in path: '${venvDir}'\"\n      # Note that the module venv was only introduced in python 3, so for 2.7\n      # this needs to be replaced with a call to virtualenv\n      ${pythonPackages.python.interpreter} -m venv \"${venvDir}\"\n    fi\n\n    # Under some circumstances it might be necessary to add your virtual\n    # environment to PYTHONPATH, which you can do here too;\n    # PYTHONPATH=$PWD\/${venvDir}\/${pythonPackages.python.sitePackages}\/:$PYTHONPATH\n\n    source \"${venvDir}\/bin\/activate\"\n\n    # As in the previous example, this is optional.\n    pip install -r requirements.txt\n  '';\n}\n\nNote that the pip install is an imperative action. So every time nix-shell is executed it will attempt to download the\nPython modules listed in requirements.txt. However these will be cached locally within the virtualenv folder and not\ndownloaded again.\n\nHow to override a Python package from configuration.nix?If you need to change a package's attribute(s) from\nconfiguration.nix you could do:\n\n  nixpkgs.config.packageOverrides = super: {\n    python = super.python.override {\n      packageOverrides = python-self: python-super: {\n        twisted = python-super.twisted.overrideAttrs (oldAttrs: {\n          src = super.fetchPipy {\n            pname = \"twisted\";\n            version = \"19.10.0\";\n            sha256 = \"7394ba7f272ae722a74f3d969dcf599bc4ef093bc392038748a490f1724a515d\";\n            extension = \"tar.bz2\";\n          };\n        });\n      };\n    };\n  };\n\npythonPackages.twisted is now globally overridden. All packages and also all NixOS services that reference twisted (such\nas services.buildbot-worker) now use the new definition. Note that python-super refers to the old package set and\npython-self to the new, overridden version.\n\nTo modify only a Python package set instead of a whole Python derivation, use this snippet:\n\n  myPythonPackages = pythonPackages.override {\n    overrides = self: super: {\n      twisted = ...;\n    };\n  }\n\nHow to override a Python package using overlays?Use the following overlay template:\n\nself: super: {\n  python = super.python.override {\n    packageOverrides = python-self: python-super: {\n      twisted = python-super.twisted.overrideAttrs (oldAttrs: {\n        src = super.fetchPypi {\n          pname = \"twisted\";\n          version = \"19.10.0\";\n          sha256 = \"7394ba7f272ae722a74f3d969dcf599bc4ef093bc392038748a490f1724a515d\";\n          extension = \"tar.bz2\";\n        };\n      });\n    };\n  };\n}\n\nHow to use Intel’s MKL with numpy and scipy?MKL can be configured using an overlay. See the section \"Using overlays to\nconfigure alternatives\".\n\nWhat inputs do setup_requires, install_requires and tests_require map to?In a setup.py or setup.cfg it is common to\ndeclare dependencies:\n\n  - setup_requires corresponds to nativeBuildInputs\n  - install_requires corresponds to propagatedBuildInputs\n  - tests_require corresponds to checkInputs\n\nContributingContributing guidelinesThe following rules are desired to be respected:\n\n  - Python libraries are called from python-packages.nix and packaged with buildPythonPackage. The expression of a\n    library should be in pkgs\/development\/python-modules\/<name>\/default.nix.\n  - Python applications live outside of python-packages.nix and are packaged with buildPythonApplication.\n  - Make sure libraries build for all Python interpreters.\n  - By default we enable tests. Make sure the tests are found and, in the case of libraries, are passing for all\n    interpreters. If certain tests fail they can be disabled individually. Try to avoid disabling the tests altogether.\n    In any case, when you disable tests, leave a comment explaining why.\n  - Commit names of Python libraries should reflect that they are Python libraries, so write for example\n    pythonPackages.numpy: 1.11 -> 1.12.\n  - Attribute names in python-packages.nix as well as pnames should match the library's name on PyPI, but be normalized\n    according to PEP 0503. This means that characters should be converted to lowercase and . and _ should be replaced by\n    a single - (foo-bar-baz instead of Foo__Bar.baz). If necessary, pname has to be given a different value within\n    fetchPypi.\n  - Attribute names in python-packages.nix should be sorted alphanumerically to avoid merge conflicts and ease locating\n    attributes.\n" }
,{ "url": "languages-frameworks\/qt\/", "title": "Qt", "text": "QtWriting Nix expressions for Qt libraries and applications is largely similar as for other C++ software. This section\nassumes some knowledge of the latter. There are two problems that the Nixpkgs Qt infrastructure addresses, which are not\nshared by other C++ software:\n\n1.  There are usually multiple supported versions of Qt in Nixpkgs. All of a package's dependencies must be built with\n    the same version of Qt. This is similar to the version constraints imposed on interpreted languages like Python.\n2.  Qt makes extensive use of runtime dependency detection. Runtime dependencies are made into build dependencies\n    through wrappers.\n\nNix expression for a Qt package (default.nix)\n\n<programlisting>\n{ stdenv, lib, qtbase, wrapQtAppsHook }: <co xml:id='qt-default-nix-co-1' \/>\n\nstdenv.mkDerivation {\n  pname = \"myapp\";\n  version = \"1.0\";\n\n  buildInputs = [ qtbase ];\n  nativeBuildInputs = [ wrapQtAppsHook ]; <co xml:id='qt-default-nix-co-2' \/>\n}\n<\/programlisting>\n\n <calloutlist>\n  <callout arearefs='qt-default-nix-co-1'>\n   <para>\n    Import Qt modules directly, that is: <literal>qtbase<\/literal>, <literal>qtdeclarative<\/literal>, etc.\n    <emphasis>Do not<\/emphasis> import Qt package sets such as <literal>qt5<\/literal>\n    because the Qt versions of dependencies may not be coherent, causing build and runtime failures.\n   <\/para>\n  <\/callout>\n  <callout arearefs='qt-default-nix-co-2'>\n    <para>\n      All Qt packages must include <literal>wrapQtAppsHook<\/literal> in\n      <literal>nativeBuildInputs<\/literal>, or you must explicitly set\n      <literal>dontWrapQtApps<\/literal>.\n    <\/para>\n  <\/callout>\n <\/calloutlist>\n\nLocating runtime dependenciesQt applications must be wrapped to find runtime dependencies. Include wrapQtAppsHook in\nnativeBuildInputs:\n\n{ stdenv, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n}\n\nAdd entries to qtWrapperArgs are to modify the wrappers created by wrapQtAppsHook:\n\n{ stdenv, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n  qtWrapperArgs = [ ''--prefix PATH : \/path\/to\/bin'' ];\n}\n\nThe entries are passed as arguments to wrapProgram.\n\nSet dontWrapQtApps to stop applications from being wrapped automatically. Wrap programs manually with wrapQtApp, using\nthe syntax of wrapProgram:\n\n{ stdenv, lib, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n  dontWrapQtApps = true;\n  preFixup = ''\n      wrapQtApp \"$out\/bin\/myapp\" --prefix PATH : \/path\/to\/bin\n  '';\n}\n\nwrapQtAppsHook ignores files that are non-ELF executables. This means that scripts won't be automatically wrapped so\nyou'll need to manually wrap them as previously mentioned. An example of when you'd always need to do this is with\nPython applications that use PyQt. \n\nAdding a library to NixpkgsAdd Qt libraries to qt5-packages.nix to make them available for every supported Qt version.\n\nExample adding a Qt libraryThe following represents the contents of qt5-packages.nix.\n\n{\n  # ...\n\n  mylib = callPackage ..\/path\/to\/mylib {};\n\n  # ...\n}\n\nLibraries are built with every available version of Qt. Use the meta.broken attribute to disable the package for\nunsupported Qt versions:\n\n{ stdenv, lib, qtbase }:\n\nstdenv.mkDerivation {\n  # ...\n  # Disable this library with Qt < 5.9.0\n  meta.broken = lib.versionOlder qtbase.version \"5.9.0\";\n}\n\nAdding an application to NixpkgsAdd Qt applications to qt5-packages.nix. Add an alias to all-packages.nix to select the\nQt 5 version used for the application.\n\nExample adding a Qt applicationThe following represents the contents of qt5-packages.nix.\n\n{\n  # ...\n\n  myapp = callPackage ..\/path\/to\/myapp {};\n\n  # ...\n}\n\nThe following represents the contents of all-packages.nix.\n\n{\n  # ...\n\n  myapp = libsForQt5.myapp;\n\n  # ...\n}\n" }
,{ "url": "languages-frameworks\/r\/", "title": "R", "text": "RInstallationDefine an environment for R that contains all the libraries that you'd like to use by adding the following\nsnippet to your $HOME\/.config\/nixpkgs\/config.nix file:\n\n{\n    packageOverrides = super: let self = super.pkgs; in\n    {\n\n        rEnv = super.rWrapper.override {\n            packages = with self.rPackages; [\n                devtools\n                ggplot2\n                reshape2\n                yaml\n                optparse\n                ];\n        };\n    };\n}\n\nThen you can use nix-env -f \"<nixpkgs>\" -iA rEnv to install it into your user profile. The set of available libraries\ncan be discovered by running the command nix-env -f \"<nixpkgs>\" -qaP -A rPackages. The first column from that output is\nthe name that has to be passed to rWrapper in the code snipped above.\n\nHowever, if you'd like to add a file to your project source to make the environment available for other contributors,\nyou can create a default.nix file like so:\n\nwith import <nixpkgs> {};\n{\n  myProject = stdenv.mkDerivation {\n    name = \"myProject\";\n    version = \"1\";\n    src = if lib.inNixShell then null else nix;\n\n    buildInputs = with rPackages; [\n      R\n      ggplot2\n      knitr\n    ];\n  };\n}\n\nand then run nix-shell . to be dropped into a shell with those packages available.\n\nRStudioRStudio uses a standard set of packages and ignores any custom R environments or installed packages you may have.\nTo create a custom environment, see rstudioWrapper, which functions similarly to rWrapper:\n\n{\n    packageOverrides = super: let self = super.pkgs; in\n    {\n\n        rstudioEnv = super.rstudioWrapper.override {\n            packages = with self.rPackages; [\n                dplyr\n                ggplot2\n                reshape2\n                ];\n        };\n    };\n}\n\nThen like above, nix-env -f \"<nixpkgs>\" -iA rstudioEnv will install this into your user profile.\n\nAlternatively, you can create a self-contained shell.nix without the need to modify any configuration files:\n\n{ pkgs ? import <nixpkgs> {}\n}:\n\npkgs.rstudioWrapper.override {\n  packages = with pkgs.rPackages; [ dplyr ggplot2 reshape2 ];\n}\n\nExecuting nix-shell will then drop you into an environment equivalent to the one above. If you need additional packages\njust add them to the list and re-enter the shell.\n\nUpdating the package set\n\nnix-shell generate-shell.nix\n\nRscript generate-r-packages.R cran  > cran-packages.nix.new\nmv cran-packages.nix.new cran-packages.nix\n\nRscript generate-r-packages.R bioc  > bioc-packages.nix.new\nmv bioc-packages.nix.new bioc-packages.nix\n\nRscript generate-r-packages.R bioc-annotation > bioc-annotation-packages.nix.new\nmv bioc-annotation-packages.nix.new bioc-annotation-packages.nix\n\nRscript generate-r-packages.R bioc-experiment > bioc-experiment-packages.nix.new\nmv bioc-experiment-packages.nix.new bioc-experiment-packages.nix\n\ngenerate-r-packages.R <repo> reads <repo>-packages.nix, therefor the renaming.\n\nTesting if the Nix-expression could be evaluated\n\nnix-build test-evaluation.nix --dry-run\n\nIf this exits fine, the expression is ok. If not, you have to edit default.nix\n" }
,{ "url": "languages-frameworks\/ruby\/", "title": "Ruby", "text": "RubyUsing RubySeveral versions of Ruby interpreters are available on Nix, as well as over 250 gems and many applications\nwritten in Ruby. The attribute ruby refers to the default Ruby interpreter, which is currently MRI 2.6. It's also\npossible to refer to specific versions, e.g. ruby_2_y, jruby, or mruby.\n\nIn the Nixpkgs tree, Ruby packages can be found throughout, depending on what they do, and are called from the main\npackage set. Ruby gems, however are separate sets, and there's one default set for each interpreter (currently MRI\nonly).\n\nThere are two main approaches for using Ruby with gems. One is to use a specifically locked Gemfile for an application\nthat has very strict dependencies. The other is to depend on the common gems, which we'll explain further down, and rely\non them being updated regularly.\n\nThe interpreters have common attributes, namely gems, and withPackages. So you can refer to ruby.gems.nokogiri, or\nruby_2_6.gems.nokogiri to get the Nokogiri gem already compiled and ready to use.\n\nSince not all gems have executables like nokogiri, it's usually more convenient to use the withPackages function like\nthis: ruby.withPackages (p: with p; [ nokogiri ]). This will also make sure that the Ruby in your environment will be\nable to find the gem and it can be used in your Ruby code (for example via ruby or irb executables) via require\n\"nokogiri\" as usual.\n\nTemporary Ruby environment with nix-shellRather than having a single Ruby environment shared by all Ruby development\nprojects on a system, Nix allows you to create separate environments per project. nix-shell gives you the possibility to\ntemporarily load another environment akin to a combined chruby or rvm and bundle exec.\n\nThere are two methods for loading a shell with Ruby packages. The first and recommended method is to create an\nenvironment with ruby.withPackages and load that.\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\"\n\nThe other method, which is not recommended, is to create an environment and list all the packages directly.\n\n$ nix-shell -p ruby.gems.nokogiri ruby.gems.pry\n\nAgain, it's possible to launch the interpreter from the shell. The Ruby interpreter has the attribute gems which\ncontains all Ruby gems for that specific interpreter.\n\nLoad Ruby environment from .nix expressionAs explained in the Nix manual, nix-shell can also load an expression from a\n.nix file. Say we want to have Ruby 2.6, nokogori, and pry. Consider a shell.nix file with:\n\nwith import <nixpkgs> {};\nruby.withPackages (ps: with ps; [ nokogiri pry ])\n\nWhat's happening here?\n\n1.  We begin with importing the Nix Packages collections. import <nixpkgs> imports the <nixpkgs> function, {} calls it\n    and the with statement brings all attributes of nixpkgs in the local scope. These attributes form the main package\n    set.\n2.  Then we create a Ruby environment with the withPackages function.\n3.  The withPackages function expects us to provide a function as an argument that takes the set of all ruby gems and\n    returns a list of packages to include in the environment. Here, we select the packages nokogiri and pry from the\n    package set.\n\nExecute command with --runA convenient flag for nix-shell is --run. It executes a command in the nix-shell. We can e.g.\ndirectly open a pry REPL:\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"pry\"\n\nOr immediately require nokogiri in pry:\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"pry -rnokogiri\"\n\nOr run a script using this environment:\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"ruby example.rb\"\n\nUsing nix-shell as shebangIn fact, for the last case, there is a more convenient method. You can add a shebang to your\nscript specifying which dependencies nix-shell needs. With the following shebang, you can just execute .\/example.rb, and\nit will run with all dependencies.\n\n#! \/usr\/bin\/env nix-shell\n#! nix-shell -i ruby -p \"ruby.withPackages (ps: with ps; [ nokogiri rest-client ])\"\n\nrequire 'nokogiri'\nrequire 'rest-client'\n\nbody = RestClient.get('http:\/\/example.com').body\nputs Nokogiri::HTML(body).at('h1').text\n\nDeveloping with RubyUsing an existing GemfileIn most cases, you'll already have a Gemfile.lock listing all your\ndependencies. This can be used to generate a gemset.nix which is used to fetch the gems and combine them into a single\nenvironment. The reason why you need to have a separate file for this, is that Nix requires you to have a checksum for\neach input to your build. Since the Gemfile.lock that bundler generates doesn't provide us with checksums, we have to\nfirst download each gem, calculate its SHA256, and store it in this separate file.\n\nSo the steps from having just a Gemfile to a gemset.nix are:\n\n$ bundle lock\n$ bundix\n\nIf you already have a Gemfile.lock, you can simply run bundix and it will work the same.\n\nTo update the gems in your Gemfile.lock, you may use the bundix -l flag, which will create a new Gemfile.lock in case\nthe Gemfile has a more recent time of modification.\n\nOnce the gemset.nix is generated, it can be used in a bundlerEnv derivation. Here is an example you could use for your\nshell.nix:\n\n# ...\nlet\n  gems = bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = .\/.;\n  };\nin mkShell { packages = [ gems gems.wrappedRuby ]; }\n\nWith this file in your directory, you can run nix-shell to build and use the gems. The important parts here are\nbundlerEnv and wrappedRuby.\n\nThe bundlerEnv is a wrapper over all the gems in your gemset. This means that all the \/lib and \/bin directories will be\navailable, and the executables of all gems (even of indirect dependencies) will end up in your $PATH. The wrappedRuby\nprovides you with all executables that come with Ruby itself, but wrapped so they can easily find the gems in your\ngemset.\n\nOne common issue that you might have is that you have Ruby 2.6, but also bundler in your gemset. That leads to a\nconflict for \/bin\/bundle and \/bin\/bundler. You can resolve this by wrapping either your Ruby or your gems in a lowPrio\ncall. So in order to give the bundler from your gemset priority, it would be used like this:\n\n# ...\nmkShell { buildInputs = [ gems (lowPrio gems.wrappedRuby) ]; }\n\nGem-specific configurations and workaroundsIn some cases, especially if the gem has native extensions, you might need to\nmodify the way the gem is built.\n\nThis is done via a common configuration file that includes all of the workarounds for each gem.\n\nThis file lives at \/pkgs\/development\/ruby-modules\/gem-config\/default.nix, since it already contains a lot of entries, it\nshould be pretty easy to add the modifications you need for your needs.\n\nIn the meanwhile, or if the modification is for a private gem, you can also add the configuration to only your own\nenvironment.\n\nTwo places that allow this modification are the ruby derivation, or bundlerEnv.\n\nHere's the ruby one:\n\n{ pg_version ? \"10\", pkgs ? import <nixpkgs> { } }:\nlet\n  myRuby = pkgs.ruby.override {\n    defaultGemConfig = pkgs.defaultGemConfig \/\/ {\n      pg = attrs: {\n        buildFlags =\n        [ \"--with-pg-config=${pkgs.\"postgresql_${pg_version}\"}\/bin\/pg_config\" ];\n      };\n    };\n  };\nin myRuby.withPackages (ps: with ps; [ pg ])\n\nAnd an example with bundlerEnv:\n\n{ pg_version ? \"10\", pkgs ? import <nixpkgs> { } }:\nlet\n  gems = pkgs.bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = .\/.;\n    gemConfig = pkgs.defaultGemConfig \/\/ {\n      pg = attrs: {\n        buildFlags =\n        [ \"--with-pg-config=${pkgs.\"postgresql_${pg_version}\"}\/bin\/pg_config\" ];\n      };\n    };\n  };\nin mkShell { buildInputs = [ gems gems.wrappedRuby ]; }\n\nAnd finally via overlays:\n\n{ pg_version ? \"10\" }:\nlet\n  pkgs = import <nixpkgs> {\n    overlays = [\n      (self: super: {\n        defaultGemConfig = super.defaultGemConfig \/\/ {\n          pg = attrs: {\n            buildFlags = [\n              \"--with-pg-config=${\n                pkgs.\"postgresql_${pg_version}\"\n              }\/bin\/pg_config\"\n            ];\n          };\n        };\n      })\n    ];\n  };\nin pkgs.ruby.withPackages (ps: with ps; [ pg ])\n\nThen we can get whichever postgresql version we desire and the pg gem will always reference it correctly:\n\n$ nix-shell --argstr pg_version 9_4 --run 'ruby -rpg -e \"puts PG.library_version\"'\n90421\n\n$ nix-shell --run 'ruby -rpg -e \"puts PG.library_version\"'\n100007\n\nOf course for this use-case one could also use overlays since the configuration for pg depends on the postgresql alias,\nbut for demonstration purposes this has to suffice.\n\nAdding a gem to the default gemsetNow that you know how to get a working Ruby environment with Nix, it's time to go\nforward and start actually developing with Ruby. We will first have a look at how Ruby gems are packaged on Nix. Then,\nwe will look at how you can use development mode with your code.\n\nAll gems in the standard set are automatically generated from a single Gemfile. The dependency resolution is done with\nbundler and makes it more likely that all gems are compatible to each other.\n\nIn order to add a new gem to nixpkgs, you can put it into the \/pkgs\/development\/ruby-modules\/with-packages\/Gemfile and\nrun .\/maintainers\/scripts\/update-ruby-packages.\n\nTo test that it works, you can then try using the gem with:\n\nNIX_PATH=nixpkgs=$PWD nix-shell -p \"ruby.withPackages (ps: with ps; [ name-of-your-gem ])\"\n\nPackaging applicationsA common task is to add a ruby executable to nixpkgs, popular examples would be chef, jekyll, or\nsass. A good way to do that is to use the bundlerApp function, that allows you to make a package that only exposes the\nlisted executables, otherwise the package may cause conflicts through common paths like bin\/rake or bin\/bundler that\naren't meant to be used.\n\nThe absolute easiest way to do that is to write a Gemfile along these lines:\n\nsource 'https:\/\/rubygems.org' do\n  gem 'mdl'\nend\n\nIf you want to package a specific version, you can use the standard Gemfile syntax for that, e.g. gem 'mdl', '0.5.0',\nbut if you want the latest stable version anyway, it's easier to update by simply running the bundle lock and bundix\nsteps again.\n\nNow you can also make a default.nix that looks like this:\n\n{ bundlerApp }:\n\nbundlerApp {\n  pname = \"mdl\";\n  gemdir = .\/.;\n  exes = [ \"mdl\" ];\n}\n\nAll that's left to do is to generate the corresponding Gemfile.lock and gemset.nix as described above in the Using an\nexisting Gemfile section.\n\nPackaging executables that require wrappingSometimes your app will depend on other executables at runtime, and tries to\nfind it through the PATH environment variable.\n\nIn this case, you can provide a postBuild hook to bundlerApp that wraps the gem in another script that prefixes the\nPATH.\n\nOf course you could also make a custom gemConfig if you know exactly how to patch it, but it's usually much easier to\nmaintain with a simple wrapper so the patch doesn't have to be adjusted for each version.\n\nHere's another example:\n\n{ lib, bundlerApp, makeWrapper, git, gnutar, gzip }:\n\nbundlerApp {\n  pname = \"r10k\";\n  gemdir = .\/.;\n  exes = [ \"r10k\" ];\n\n  buildInputs = [ makeWrapper ];\n\n  postBuild = ''\n    wrapProgram $out\/bin\/r10k --prefix PATH : ${lib.makeBinPath [ git gnutar gzip ]}\n  '';\n}\n" }
,{ "url": "languages-frameworks\/rust\/", "title": "Rust", "text": "RustTo install the rust compiler and cargo put\n\nenvironment.systemPackages = [\n  rustc\n  cargo\n];\n\ninto your configuration.nix or bring them into scope with nix-shell -p rustc cargo.\n\nFor other versions such as daily builds (beta and nightly), use either rustup from nixpkgs (which will manage the rust\ninstallation in your home directory), or use Mozilla's Rust nightlies overlay.\n\nCompiling Rust applications with CargoRust applications are packaged by using the buildRustPackage helper from\nrustPlatform:\n\n{ lib, rustPlatform }:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"ripgrep\";\n  version = \"12.1.1\";\n\n  src = fetchFromGitHub {\n    owner = \"BurntSushi\";\n    repo = pname;\n    rev = version;\n    sha256 = \"1hqps7l5qrjh9f914r5i6kmcz6f1yb951nv4lby0cjnp5l253kps\";\n  };\n\n  cargoSha256 = \"03wf9r2csi6jpa7v5sw5lpxkrk4wfzwmzx7k3991q3bdjzcwnnwp\";\n\n  meta = with lib; {\n    description = \"A fast line-oriented regex search tool, similar to ag and ack\";\n    homepage = \"https:\/\/github.com\/BurntSushi\/ripgrep\";\n    license = licenses.unlicense;\n    maintainers = [ maintainers.tailhook ];\n  };\n}\n\nbuildRustPackage requires either the cargoSha256 or the cargoHash attribute which is computed over all crate sources of\nthis package. cargoHash256 is used for traditional Nix SHA-256 hashes, such as the one in the example above. cargoHash\nshould instead be used for SRI hashes. For example:\n\n  cargoHash = \"sha256-l1vL2ZdtDRxSGvP0X\/l3nMw8+6WF67KPutJEzUROjg8=\";\n\nBoth types of hashes are permitted when contributing to nixpkgs. The Cargo hash is obtained by inserting a fake checksum\ninto the expression and building the package once. The correct checksum can then be taken from the failed build. A fake\nhash can be used for cargoSha256 as follows:\n\n  cargoSha256 = lib.fakeSha256;\n\nFor cargoHash you can use:\n\n  cargoHash = lib.fakeHash;\n\nPer the instructions in the Cargo Book best practices guide, Rust applications should always commit the Cargo.lock file\nin git to ensure a reproducible build. However, a few packages do not, and Nix depends on this file, so if it is missing\nyou can use cargoPatches to apply it in the patchPhase. Consider sending a PR upstream with a note to the maintainer\ndescribing why it's important to include in the application.\n\nThe fetcher will verify that the Cargo.lock file is in sync with the src attribute, and fail the build if not. It will\nalso will compress the vendor directory into a tar.gz archive.\n\nThe tarball with vendored dependencies contains a directory with the package's name, which is normally composed of pname\nand version. This means that the vendored dependencies hash (cargoSha256\/cargoHash) is dependent on the package name and\nversion. The cargoDepsName attribute can be used to use another name for the directory of vendored dependencies. For\nexample, the hash can be made invariant to the version by setting cargoDepsName to pname:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"broot\";\n  version = \"1.2.0\";\n\n  src = fetchCrate {\n    inherit pname version;\n    sha256 = \"1mqaynrqaas82f5957lx31x80v74zwmwmjxxlbywajb61vh00d38\";\n  };\n\n  cargoHash = \"sha256-JmBZcDVYJaK1cK05cxx5BrnGWp4t8ca6FLUbvIot67s=\";\n  cargoDepsName = pname;\n\n  # ...\n}\n\nImporting a Cargo.lock fileUsing cargoSha256 or cargoHash is tedious when using buildRustPackage within a project, since\nit requires that the hash is updated after every change to Cargo.lock. Therefore, buildRustPackage also supports\nvendoring dependencies directly from a Cargo.lock file using the cargoLock argument. For example:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = .\/Cargo.lock;\n  }\n\n  # ...\n}\n\nThis will retrieve the dependencies using fixed-output derivations from the specified lockfile.\n\nThe output hash of each dependency that uses a git source must be specified in the outputHashes attribute. For example:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = .\/Cargo.lock;\n    outputHashes = {\n      \"finalfusion-0.14.0\" = \"17f4bsdzpcshwh74w5z119xjy2if6l2wgyjy56v621skr2r8y904\";\n    };\n  }\n\n  # ...\n}\n\nIf you do not specify an output hash for a git dependency, building the package will fail and inform you of which crate\nneeds to be added. To find the correct hash, you can first use lib.fakeSha256 or lib.fakeHash as a stub hash. Building\nthe package (and thus the vendored dependencies) will then inform you of the correct hash.\n\nCross compilationBy default, Rust packages are compiled for the host platform, just like any other package is. The\n--target passed to rust tools is computed from this. By default, it takes the stdenv.hostPlatform.config and replaces\ncomponents where they are known to differ. But there are ways to customize the argument:\n\n  - To choose a different target by name, define stdenv.hostPlatform.rustc.config as that name (a string), and that name\n    will be used instead.\n    \n    For example:\n    \n    import <nixpkgs> {\n      crossSystem = (import <nixpkgs\/lib>).systems.examples.armhf-embedded \/\/ {\n        rustc.config = \"thumbv7em-none-eabi\";\n      };\n    }\n\n    will result in:\n    \n    --target thumbv7em-none-eabi\n\n  - To pass a completely custom target, define stdenv.hostPlatform.rustc.config with its name, and\n    stdenv.hostPlatform.rustc.platform with the value. The value will be serialized to JSON in a file called\n    ${stdenv.hostPlatform.rustc.config}.json, and the path of that file will be used instead.\n    \n    For example:\n    \n    import <nixpkgs> {\n      crossSystem = (import <nixpkgs\/lib>).systems.examples.armhf-embedded \/\/ {\n        rustc.config = \"thumb-crazy\";\n        rustc.platform = { foo = \"\"; bar = \"\"; };\n      };\n    }\n\n    will result in:\n    \n    --target \/nix\/store\/asdfasdfsadf-thumb-crazy.json # contains {\"foo\":\"\",\"bar\":\"\"}\n\nFinally, as an ad-hoc escape hatch, a computed target (string or JSON file path) can be passed directly to\nbuildRustPackage:\n\npkgs.rustPlatform.buildRustPackage {\n  \/* ... *\/\n  target = \"x86_64-fortanix-unknown-sgx\";\n}\n\nThis is useful to avoid rebuilding Rust tools, since they are actually target agnostic and don't need to be rebuilt. But\nin the future, we should always build the Rust tools and standard library crates separately so there is no reason not to\ntake the stdenv.hostPlatform.rustc-modifying approach, and the ad-hoc escape hatch to buildRustPackage can be removed.\n\nNote that currently custom targets aren't compiled with std, so cargo test will fail. This can be ignored by adding\ndoCheck = false; to your derivation.\n\nRunning package testsWhen using buildRustPackage, the checkPhase is enabled by default and runs cargo test on the\npackage to build. To make sure that we don't compile the sources twice and to actually test the artifacts that will be\nused at runtime, the tests will be ran in the release mode by default.\n\nHowever, in some cases the test-suite of a package doesn't work properly in the release mode. For these situations, the\nmode for checkPhase can be changed like so:\n\nrustPlatform.buildRustPackage {\n  \/* ... *\/\n  checkType = \"debug\";\n}\n\nPlease note that the code will be compiled twice here: once in release mode for the buildPhase, and again in debug mode\nfor the checkPhase.\n\nTest flags, e.g., --features xxx\/yyy, can be passed to cargo test via the cargoTestFlags attribute.\n\nAnother attribute, called checkFlags, is used to pass arguments to the test binary itself, as stated\n(here)[https:\/\/doc.rust-lang.org\/cargo\/commands\/cargo-test.html].\n\nTests relying on the structure of the target\/ directorySome tests may rely on the structure of the target\/ directory.\nThose tests are likely to fail because we use cargo --target during the build. This means that the artifacts are stored\nin target\/<architecture>\/release\/, rather than in target\/release\/.\n\nThis can only be worked around by patching the affected tests accordingly.\n\nDisabling package-testsIn some instances, it may be necessary to disable testing altogether (with doCheck = false;):\n\n  - If no tests exist -- the checkPhase should be explicitly disabled to skip unnecessary build steps to speed up the\n    build.\n  - If tests are highly impure (e.g. due to network usage).\n\nThere will obviously be some corner-cases not listed above where it's sensible to disable tests. The above are just\nguidelines, and exceptions may be granted on a case-by-case basis.\n\nHowever, please check if it's possible to disable a problematic subset of the test suite and leave a comment explaining\nyour reasoning.\n\nSetting test-threadsbuildRustPackage will use parallel test threads by default, sometimes it may be necessary to disable\nthis so the tests run consecutively.\n\nrustPlatform.buildRustPackage {\n  \/* ... *\/\n  dontUseCargoParallelTests = true;\n}\n\nBuilding a package in debug modeBy default, buildRustPackage will use release mode for builds. If a package should be\nbuilt in debug mode, it can be configured like so:\n\nrustPlatform.buildRustPackage {\n  \/* ... *\/\n  buildType = \"debug\";\n}\n\nIn this scenario, the checkPhase will be ran in debug mode as well.\n\nCustom build\/install-proceduresSome packages may use custom scripts for building\/installing, e.g. with a Makefile. In\nthese cases, it's recommended to override the buildPhase\/installPhase\/checkPhase.\n\nOtherwise, some steps may fail because of the modified directory structure of target\/.\n\nBuilding a crate with an absent or out-of-date Cargo.lock filebuildRustPackage needs a Cargo.lock file to get all\ndependencies in the source code in a reproducible way. If it is missing or out-of-date one can use the cargoPatches\nattribute to update or add it.\n\nrustPlatform.buildRustPackage rec {\n  (...)\n  cargoPatches = [\n    # a patch file to add\/update Cargo.lock in the source code\n    .\/add-Cargo.lock.patch\n  ];\n}\n\nCompiling non-Rust packages that include Rust codeSeveral non-Rust packages incorporate Rust code for performance- or\nsecurity-sensitive parts. rustPlatform exposes several functions and hooks that can be used to integrate Cargo in\nnon-Rust packages.\n\nVendoring of dependenciesSince network access is not allowed in sandboxed builds, Rust crate dependencies need to be\nretrieved using a fetcher. rustPlatform provides the fetchCargoTarball fetcher, which vendors all dependencies of a\ncrate. For example, given a source path src containing Cargo.toml and Cargo.lock, fetchCargoTarball can be used as\nfollows:\n\ncargoDeps = rustPlatform.fetchCargoTarball {\n  inherit src;\n  hash = \"sha256-BoHIN\/519Top1NUBjpB\/oEMqi86Omt3zTQcXFWqrek0=\";\n};\n\nThe src attribute is required, as well as a hash specified through one of the sha256 or hash attributes. The following\noptional attributes can also be used:\n\n  - name: the name that is used for the dependencies tarball. If name is not specified, then the name cargo-deps will be\n    used.\n  - sourceRoot: when the Cargo.lock\/Cargo.toml are in a subdirectory, sourceRoot specifies the relative path to these\n    files.\n  - patches: patches to apply before vendoring. This is useful when the Cargo.lock\/Cargo.toml files need to be patched\n    before vendoring.\n\nIf a Cargo.lock file is available, you can alternatively use the importCargoLock function. In contrast to\nfetchCargoTarball, this function does not require a hash (unless git dependencies are used) and fetches every dependency\nas a separate fixed-output derivation. importCargoLock can be used as follows:\n\ncargoDeps = rustPlatform.importCargoLock {\n  lockFile = .\/Cargo.lock;\n};\n\nIf the Cargo.lock file includes git dependencies, then their output hashes need to be specified since they are not\navailable through the lock file. For example:\n\ncargoDeps = rustPlatform.importCargoLock {\n  lockFile = .\/Cargo.lock;\n  outputHashes = {\n    \"rand-0.8.3\" = \"0ya2hia3cn31qa8894s3av2s8j5bjwb6yq92k0jsnlx7jid0jwqa\";\n  };\n};\n\nIf you do not specify an output hash for a git dependency, building cargoDeps will fail and inform you of which crate\nneeds to be added. To find the correct hash, you can first use lib.fakeSha256 or lib.fakeHash as a stub hash. Building\ncargoDeps will then inform you of the correct hash.\n\nHooksrustPlatform provides the following hooks to automate Cargo builds:\n\n  - cargoSetupHook: configure Cargo to use depenencies vendored through fetchCargoTarball. This hook uses the cargoDeps\n    environment variable to find the vendored dependencies. If a project already vendors its dependencies, the variable\n    cargoVendorDir can be used instead. When the Cargo.toml\/Cargo.lock files are not in sourceRoot, then the optional\n    cargoRoot is used to specify the Cargo root directory relative to sourceRoot.\n  - cargoBuildHook: use Cargo to build a crate. If the crate to be built is a crate in e.g. a Cargo workspace, the\n    relative path to the crate to build can be set through the optional buildAndTestSubdir environment variable.\n    Additional Cargo build flags can be passed through cargoBuildFlags.\n  - maturinBuildHook: use Maturin to build a Python wheel. Similar to cargoBuildHook, the optional variable\n    buildAndTestSubdir can be used to build a crate in a Cargo workspace. Additional maturin flags can be passed through\n    maturinBuildFlags.\n  - cargoCheckHook: run tests using Cargo. The build type for checks can be set using cargoCheckType. Additional flags\n    can be passed to the tests using checkFlags and checkFlagsArray. By default, tests are run in parallel. This can be\n    disabled by setting dontUseCargoParallelTests.\n  - cargoInstallHook: install binaries and static\/shared libraries that were built using cargoBuildHook.\n\nExamplesPython package using setuptools-rustFor Python packages using setuptools-rust, you can use fetchCargoTarball and\ncargoSetupHook to retrieve and set up Cargo dependencies. The build itself is then performed by buildPythonPackage.\n\nThe following example outlines how the tokenizers Python package is built. Since the Python package is in the\nsource\/bindings\/python directory of the tokenizers project's source archive, we use sourceRoot to point the tooling to\nthis directory:\n\n{ fetchFromGitHub\n, buildPythonPackage\n, rustPlatform\n, setuptools-rust\n}:\n\nbuildPythonPackage rec {\n  pname = \"tokenizers\";\n  version = \"0.10.0\";\n\n  src = fetchFromGitHub {\n    owner = \"huggingface\";\n    repo = pname;\n    rev = \"python-v${version}\";\n    hash = \"sha256-rQ2hRV52naEf6PvRsWVCTN7B1oXAQGmnpJw4iIdhamw=\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src sourceRoot;\n    name = \"${pname}-${version}\";\n    hash = \"sha256-BoHIN\/519Top1NUBjpB\/oEMqi86Omt3zTQcXFWqrek0=\";\n  };\n\n  sourceRoot = \"source\/bindings\/python\";\n\n  nativeBuildInputs = [ setuptools-rust ] ++ (with rustPlatform; [\n    cargoSetupHook\n    rust.cargo\n    rust.rustc\n  ]);\n\n  # ...\n}\n\nIn some projects, the Rust crate is not in the main Python source directory. In such cases, the cargoRoot attribute can\nbe used to specify the crate's directory relative to sourceRoot. In the following example, the crate is in src\/rust, as\nspecified in the cargoRoot attribute. Note that we also need to specify the correct path for fetchCargoTarball.\n\n\n{ buildPythonPackage\n, fetchPypi\n, rustPlatform\n, setuptools-rust\n, openssl\n}:\n\nbuildPythonPackage rec {\n  pname = \"cryptography\";\n  version = \"3.4.2\"; # Also update the hash in vectors.nix\n\n  src = fetchPypi {\n    inherit pname version;\n    sha256 = \"1i1mx5y9hkyfi9jrrkcw804hmkcglxi6rmf7vin7jfnbr2bf4q64\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src;\n    sourceRoot = \"${pname}-${version}\/${cargoRoot}\";\n    name = \"${pname}-${version}\";\n    hash = \"sha256-PS562W4L1NimqDV2H0jl5vYhL08H9est\/pbIxSdYVfo=\";\n  };\n\n  cargoRoot = \"src\/rust\";\n\n  # ...\n}\n\nPython package using maturinPython packages that use Maturin can be built with fetchCargoTarball, cargoSetupHook, and\nmaturinBuildHook. For example, the following (partial) derivation builds the retworkx Python package. fetchCargoTarball\nand cargoSetupHook are used to fetch and set up the crate dependencies. maturinBuildHook is used to perform the build.\n\n{ lib\n, buildPythonPackage\n, rustPlatform\n, fetchFromGitHub\n}:\n\nbuildPythonPackage rec {\n  pname = \"retworkx\";\n  version = \"0.6.0\";\n\n  src = fetchFromGitHub {\n    owner = \"Qiskit\";\n    repo = \"retworkx\";\n    rev = version;\n    sha256 = \"11n30ldg3y3y6qxg3hbj837pnbwjkqw3nxq6frds647mmmprrd20\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src;\n    name = \"${pname}-${version}\";\n    hash = \"sha256-heOBK8qi2nuc\/Ib+I\/vLzZ1fUUD\/G\/KTw9d7M4Hz5O0=\";\n  };\n\n  format = \"pyproject\";\n\n  nativeBuildInputs = with rustPlatform; [ cargoSetupHook maturinBuildHook ];\n\n  # ...\n}\n\nCompiling Rust crates using Nix instead of CargoSimple operationWhen run, cargo build produces a file called Cargo.lock,\ncontaining pinned versions of all dependencies. Nixpkgs contains a tool called carnix (nix-env -iA nixos.carnix), which\ncan be used to turn a Cargo.lock into a Nix expression.\n\nThat Nix expression calls rustc directly (hence bypassing Cargo), and can be used to compile a crate and all its\ndependencies. Here is an example for a minimal hello crate:\n\n$ cargo new hello\n$ cd hello\n$ cargo build\n     Compiling hello v0.1.0 (file:\/\/\/tmp\/hello)\n     Finished dev [unoptimized + debuginfo] target(s) in 0.20 secs\n$ carnix -o hello.nix --src .\/. Cargo.lock --standalone\n$ nix-build hello.nix -A hello_0_1_0\n\nNow, the file produced by the call to carnix, called hello.nix, looks like:\n\n# Generated by carnix 0.6.5: carnix -o hello.nix --src .\/. Cargo.lock --standalone\n{ stdenv, buildRustCrate, fetchgit }:\nlet kernel = stdenv.buildPlatform.parsed.kernel.name;\n    # ... (content skipped)\nin\nrec {\n  hello = f: hello_0_1_0 { features = hello_0_1_0_features { hello_0_1_0 = f; }; };\n  hello_0_1_0_ = { dependencies?[], buildDependencies?[], features?[] }: buildRustCrate {\n    crateName = \"hello\";\n    version = \"0.1.0\";\n    authors = [ \"pe@pijul.org <pe@pijul.org>\" ];\n    src = .\/.;\n    inherit dependencies buildDependencies features;\n  };\n  hello_0_1_0 = { features?(hello_0_1_0_features {}) }: hello_0_1_0_ {};\n  hello_0_1_0_features = f: updateFeatures f (rec {\n        hello_0_1_0.default = (f.hello_0_1_0.default or true);\n    }) [ ];\n}\n\nIn particular, note that the argument given as --src is copied verbatim to the source. If we look at a more complicated\ndependencies, for instance by adding a single line libc=\"*\" to our Cargo.toml, we first need to run cargo build to\nupdate the Cargo.lock. Then, carnix needs to be run again, and produces the following nix file:\n\n# Generated by carnix 0.6.5: carnix -o hello.nix --src .\/. Cargo.lock --standalone\n{ stdenv, buildRustCrate, fetchgit }:\nlet kernel = stdenv.buildPlatform.parsed.kernel.name;\n    # ... (content skipped)\nin\nrec {\n  hello = f: hello_0_1_0 { features = hello_0_1_0_features { hello_0_1_0 = f; }; };\n  hello_0_1_0_ = { dependencies?[], buildDependencies?[], features?[] }: buildRustCrate {\n    crateName = \"hello\";\n    version = \"0.1.0\";\n    authors = [ \"pe@pijul.org <pe@pijul.org>\" ];\n    src = .\/.;\n    inherit dependencies buildDependencies features;\n  };\n  libc_0_2_36_ = { dependencies?[], buildDependencies?[], features?[] }: buildRustCrate {\n    crateName = \"libc\";\n    version = \"0.2.36\";\n    authors = [ \"The Rust Project Developers\" ];\n    sha256 = \"01633h4yfqm0s302fm0dlba469bx8y6cs4nqc8bqrmjqxfxn515l\";\n    inherit dependencies buildDependencies features;\n  };\n  hello_0_1_0 = { features?(hello_0_1_0_features {}) }: hello_0_1_0_ {\n    dependencies = mapFeatures features ([ libc_0_2_36 ]);\n  };\n  hello_0_1_0_features = f: updateFeatures f (rec {\n    hello_0_1_0.default = (f.hello_0_1_0.default or true);\n    libc_0_2_36.default = true;\n  }) [ libc_0_2_36_features ];\n  libc_0_2_36 = { features?(libc_0_2_36_features {}) }: libc_0_2_36_ {\n    features = mkFeatures (features.libc_0_2_36 or {});\n  };\n  libc_0_2_36_features = f: updateFeatures f (rec {\n    libc_0_2_36.default = (f.libc_0_2_36.default or true);\n    libc_0_2_36.use_std =\n      (f.libc_0_2_36.use_std or false) ||\n      (f.libc_0_2_36.default or false) ||\n      (libc_0_2_36.default or false);\n  }) [];\n}\n\nHere, the libc crate has no src attribute, so buildRustCrate will fetch it from crates.io. A sha256 attribute is still\nneeded for Nix purity.\n\nHandling external dependenciesSome crates require external libraries. For crates from crates.io, such libraries can be\nspecified in defaultCrateOverrides package in nixpkgs itself.\n\nStarting from that file, one can add more overrides, to add features or build inputs by overriding the hello crate in a\nseperate file.\n\nwith import <nixpkgs> {};\n((import .\/hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides \/\/ {\n    hello = attrs: { buildInputs = [ openssl ]; };\n  };\n}\n\nHere, crateOverrides is expected to be a attribute set, where the key is the crate name without version number and the\nvalue a function. The function gets all attributes passed to buildRustCrate as first argument and returns a set that\ncontains all attribute that should be overwritten.\n\nFor more complicated cases, such as when parts of the crate's derivation depend on the crate's version, the attrs\nargument of the override above can be read, as in the following example, which patches the derivation:\n\nwith import <nixpkgs> {};\n((import .\/hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides \/\/ {\n    hello = attrs: lib.optionalAttrs (lib.versionAtLeast attrs.version \"1.0\")  {\n      postPatch = ''\n        substituteInPlace lib\/zoneinfo.rs \\\n          --replace \"\/usr\/share\/zoneinfo\" \"${tzdata}\/share\/zoneinfo\"\n      '';\n    };\n  };\n}\n\nAnother situation is when we want to override a nested dependency. This actually works in the exact same way, since the\ncrateOverrides parameter is forwarded to the crate's dependencies. For instance, to override the build inputs for crate\nlibc in the example above, where libc is a dependency of the main crate, we could do:\n\nwith import <nixpkgs> {};\n((import hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides \/\/ {\n    libc = attrs: { buildInputs = []; };\n  };\n}\n\nOptions and phases configurationActually, the overrides introduced in the previous section are more general. A number of\nother parameters can be overridden:\n\n  - The version of rustc used to compile the crate:\n    \n    (hello {}).override { rust = pkgs.rust; };\n\n  - Whether to build in release mode or debug mode (release mode by default):\n    \n    (hello {}).override { release = false; };\n\n  - Whether to print the commands sent to rustc when building (equivalent to --verbose in cargo:\n    \n    (hello {}).override { verbose = false; };\n\n  - Extra arguments to be passed to rustc:\n    \n    (hello {}).override { extraRustcOpts = \"-Z debuginfo=2\"; };\n\n  - Phases, just like in any other derivation, can be specified using the following attributes: preUnpack, postUnpack,\n    prePatch, patches, postPatch, preConfigure (in the case of a Rust crate, this is run before calling the \"build\"\n    script), postConfigure (after the \"build\" script),preBuild, postBuild, preInstall and postInstall. As an example,\n    here is how to create a new module before running the build script:\n    \n    (hello {}).override {\n      preConfigure = ''\n         echo \"pub const PATH=\\\"${hi.out}\\\";\" >> src\/path.rs\"\n      '';\n    };\n\nFeaturesOne can also supply features switches. For example, if we want to compile diesel_cli only with the postgres\nfeature, and no default features, we would write:\n\n(callPackage .\/diesel.nix {}).diesel {\n  default = false;\n  postgres = true;\n}\n\nWhere diesel.nix is the file generated by Carnix, as explained above.\n\nSetting Up nix-shellOftentimes you want to develop code from within nix-shell. Unfortunately buildRustCrate does not\nsupport common nix-shell operations directly (see this issue) so we will use stdenv.mkDerivation instead.\n\nUsing the example hello project above, we want to do the following:\n\n  - Have access to cargo and rustc\n  - Have the openssl library available to a crate through it's normal compilation mechanism (pkg-config).\n\nA typical shell.nix might look like:\n\nwith import <nixpkgs> {};\n\nstdenv.mkDerivation {\n  name = \"rust-env\";\n  nativeBuildInputs = [\n    rustc cargo\n\n    # Example Build-time Additional Dependencies\n    pkg-config\n  ];\n  buildInputs = [\n    # Example Run-time Additional Dependencies\n    openssl\n  ];\n\n  # Set Environment Variables\n  RUST_BACKTRACE = 1;\n}\n\nYou should now be able to run the following:\n\n$ nix-shell --pure\n$ cargo build\n$ cargo test\n\nControlling Rust Version Inside nix-shellTo control your rust version (i.e. use nightly) from within shell.nix (or other\nnix expressions) you can use the following shell.nix\n\n# Latest Nightly\nwith import <nixpkgs> {};\nlet src = fetchFromGitHub {\n      owner = \"mozilla\";\n      repo = \"nixpkgs-mozilla\";\n      # commit from: 2019-05-15\n      rev = \"9f35c4b09fd44a77227e79ff0c1b4b6a69dff533\";\n      sha256 = \"18h0nvh55b5an4gmlgfbvwbyqj91bklf1zymis6lbdh75571qaz0\";\n   };\nin\nwith import \"${src.out}\/rust-overlay.nix\" pkgs pkgs;\nstdenv.mkDerivation {\n  name = \"rust-env\";\n  buildInputs = [\n    # Note: to use stable, just replace `nightly` with `stable`\n    latest.rustChannels.nightly.rust\n\n    # Add some extra dependencies from `pkgs`\n    pkg-config openssl\n  ];\n\n  # Set Environment Variables\n  RUST_BACKTRACE = 1;\n}\n\nNow run:\n\n$ rustc --version\nrustc 1.26.0-nightly (188e693b3 2018-03-26)\n\nTo see that you are using nightly.\n\nUsing the Rust nightlies overlayMozilla provides an overlay for nixpkgs to bring a nightly version of Rust into scope.\nThis overlay can also be used to install recent unstable or stable versions of Rust, if desired.\n\nRust overlay installationYou can use this overlay by either changing your local nixpkgs configuration, or by adding the\noverlay declaratively in a nix expression, e.g. in configuration.nix. For more information see the manual on installing\noverlays.\n\nImperative rust overlay installationClone nixpkgs-mozilla, and create a symbolic link to the file rust-overlay.nix in\nthe ~\/.config\/nixpkgs\/overlays directory.\n\n$ git clone https:\/\/github.com\/mozilla\/nixpkgs-mozilla.git\n$ mkdir -p ~\/.config\/nixpkgs\/overlays\n$ ln -s $(pwd)\/nixpkgs-mozilla\/rust-overlay.nix ~\/.config\/nixpkgs\/overlays\/rust-overlay.nix\n\nDeclarative rust overlay installationAdd the following to your configuration.nix, home-configuration.nix, shell.nix, or\nsimilar:\n\n{ pkgs ? import <nixpkgs> {\n    overlays = [\n      (import (builtins.fetchTarball https:\/\/github.com\/mozilla\/nixpkgs-mozilla\/archive\/master.tar.gz))\n      # Further overlays go here\n    ];\n  };\n};\n\nNote that this will fetch the latest overlay version when rebuilding your system.\n\nRust overlay usageThe overlay contains attribute sets corresponding to different versions of the rust toolchain, such\nas:\n\n  - latest.rustChannels.stable\n  - latest.rustChannels.nightly\n  - a function rustChannelOf, called as (rustChannelOf { date = \"2018-04-11\"; channel = \"nightly\"; }), or...\n  - (nixpkgs.rustChannelOf { rustToolchain = .\/rust-toolchain; }) if you have a local rust-toolchain file (see\n    https:\/\/github.com\/mozilla\/nixpkgs-mozilla#using-in-nix-expressions for an example)\n\nEach of these contain packages such as rust, which contains your usual rust development tools with the respective\ntoolchain chosen. For example, you might want to add latest.rustChannels.stable.rust to the list of packages in your\nconfiguration.\n\nImperatively, the latest stable version can be installed with the following command:\n\n$ nix-env -Ai nixpkgs.latest.rustChannels.stable.rust\n\nOr using the attribute with nix-shell:\n\n$ nix-shell -p nixpkgs.latest.rustChannels.stable.rust\n\nSubstitute the nixpkgs prefix with nixos on NixOS. To install the beta or nightly channel, \"stable\" should be\nsubstituted by \"nightly\" or \"beta\", or use the function provided by this overlay to pull a version based on a build\ndate.\n\nThe overlay automatically updates itself as it uses the same source as rustup.\n" }
,{ "url": "languages-frameworks\/texlive\/", "title": "TeX Live", "text": "TeX LiveSince release 15.09 there is a new TeX Live packaging that lives entirely under attribute texlive.\n\nUser's guide  - For basic usage just pull texlive.combined.scheme-basic for an environment with basic LaTeX support.\n\n  - It typically won't work to use separately installed packages together. Instead, you can build a custom set of\n    packages like this:\n    \n    texlive.combine {\n      inherit (texlive) scheme-small collection-langkorean algorithms cm-super;\n    }\n\n  - There are all the schemes, collections and a few thousand packages, as defined upstream (perhaps with tiny\n    differences).\n\n  - By default you only get executables and files needed during runtime, and a little documentation for the core\n    packages. To change that, you need to add pkgFilter function to combine.\n    \n    texlive.combine {\n      # inherit (texlive) whatever-you-want;\n      pkgFilter = pkg:\n        pkg.tlType == \"run\" || pkg.tlType == \"bin\" || pkg.pname == \"cm-super\";\n      # elem tlType [ \"run\" \"bin\" \"doc\" \"source\" ]\n      # there are also other attributes: version, name\n    }\n\n  - You can list packages e.g. by nix repl.\n    \n    $ nix repl\n    nix-repl> :l <nixpkgs>\n    nix-repl> texlive.collection-[TAB]\n\n  - Note that the wrapper assumes that the result has a chance to be useful. For example, the core executables should be\n    present, as well as some core data files. The supported way of ensuring this is by including some scheme, for\n    example scheme-basic, into the combination.\n\nCustom packagesYou may find that you need to use an external TeX package. A derivation for such package has to provide\ncontents of the \"texmf\" directory in its output and provide the tlType attribute. Here is a (very verbose) example:\n\nwith import <nixpkgs> {};\n\nlet\n  foiltex_run = stdenvNoCC.mkDerivation {\n    pname = \"latex-foiltex\";\n    version = \"2.1.4b\";\n    passthru.tlType = \"run\";\n\n    srcs = [\n      (fetchurl {\n        url = \"http:\/\/mirrors.ctan.org\/macros\/latex\/contrib\/foiltex\/foiltex.dtx\";\n        sha256 = \"07frz0krpz7kkcwlayrwrj2a2pixmv0icbngyw92srp9fp23cqpz\";\n      })\n      (fetchurl {\n        url = \"http:\/\/mirrors.ctan.org\/macros\/latex\/contrib\/foiltex\/foiltex.ins\";\n        sha256 = \"09wkyidxk3n3zvqxfs61wlypmbhi1pxmjdi1kns9n2ky8ykbff99\";\n      })\n    ];\n\n    unpackPhase = ''\n      runHook preUnpack\n\n      for _src in $srcs; do\n        cp \"$_src\" $(stripHash \"$_src\")\n      done\n\n      runHook postUnpack\n    '';\n\n    nativeBuildInputs = [ texlive.combined.scheme-small ];\n\n    dontConfigure = true;\n\n    buildPhase = ''\n      runHook preBuild\n\n      # Generate the style files\n      latex foiltex.ins\n\n      runHook postBuild\n    '';\n\n    installPhase = ''\n      runHook preInstall\n\n      path=\"$out\/tex\/latex\/foiltex\"\n      mkdir -p \"$path\"\n      cp *.{cls,def,clo} \"$path\/\"\n\n      runHook postInstall\n    '';\n\n    meta = with lib; {\n      description = \"A LaTeX2e class for overhead transparencies\";\n      license = licenses.unfreeRedistributable;\n      maintainers = with maintainers; [ veprbl ];\n      platforms = platforms.all;\n    };\n  };\n  foiltex = { pkgs = [ foiltex_run ]; };\n\n  latex_with_foiltex = texlive.combine {\n    inherit (texlive) scheme-small;\n    inherit foiltex;\n  };\nin\n  runCommand \"test.pdf\" {\n    nativeBuildInputs = [ latex_with_foiltex ];\n  } ''\ncat >test.tex <<EOF\n\\documentclass{foils}\n\n\\title{Presentation title}\n\\date{}\n\n\\begin{document}\n\\maketitle\n\\end{document}\nEOF\n  pdflatex test.tex\n  cp test.pdf $out\n''\n" }
,{ "url": "languages-frameworks\/titanium\/", "title": "Titanium", "text": "TitaniumThe Nixpkgs repository contains facilities to deploy a variety of versions of the Titanium SDK versions, a\ncross-platform mobile app development framework using JavaScript as an implementation language, and includes a function\nabstraction making it possible to build Titanium applications for Android and iOS devices from source code.\n\nNot all Titanium features supported -- currently, it can only be used to build Android and iOS apps.\n\nBuilding a Titanium appWe can build a Titanium app from source for Android or iOS and for debugging or release purposes\nby invoking the titaniumenv.buildApp {} function:\n\ntitaniumenv.buildApp {\n  name = \"myapp\";\n  src = .\/myappsource;\n\n  preBuild = \"\";\n  target = \"android\"; # or 'iphone'\n  tiVersion = \"7.1.0.GA\";\n  release = true;\n\n  androidsdkArgs = {\n    platformVersions = [ \"25\" \"26\" ];\n  };\n  androidKeyStore = .\/keystore;\n  androidKeyAlias = \"myfirstapp\";\n  androidKeyStorePassword = \"secret\";\n\n  xcodeBaseDir = \"\/Applications\/Xcode.app\";\n  xcodewrapperArgs = {\n    version = \"9.3\";\n  };\n  iosMobileProvisioningProfile = .\/myprovisioning.profile;\n  iosCertificateName = \"My Company\";\n  iosCertificate = .\/mycertificate.p12;\n  iosCertificatePassword = \"secret\";\n  iosVersion = \"11.3\";\n  iosBuildStore = false;\n\n  enableWirelessDistribution = true;\n  installURL = \"\/installipa.php\";\n}\n\nThe titaniumenv.buildApp {} function takes the following parameters:\n\n  - The name parameter refers to the name in the Nix store.\n  - The src parameter refers to the source code location of the app that needs to be built.\n  - preRebuild contains optional build instructions that are carried out before the build starts.\n  - target indicates for which device the app must be built. Currently only 'android' and 'iphone' (for iOS) are\n    supported.\n  - tiVersion can be used to optionally override the requested Titanium version in tiapp.xml. If not specified, it will\n    use the version in tiapp.xml.\n  - release should be set to true when building an app for submission to the Google Playstore or Apple Appstore.\n    Otherwise, it should be false.\n\nWhen the target has been set to android, we can configure the following parameters:\n\n  - The androidSdkArgs parameter refers to an attribute set that propagates all parameters to the\n    androidenv.composeAndroidPackages {} function. This can be used to install all relevant Android plugins that may be\n    needed to perform the Android build. If no parameters are given, it will deploy the platform SDKs for API-levels 25\n    and 26 by default.\n\nWhen the release parameter has been set to true, you need to provide parameters to sign the app:\n\n  - androidKeyStore is the path to the keystore file\n  - androidKeyAlias is the key alias\n  - androidKeyStorePassword refers to the password to open the keystore file.\n\nWhen the target has been set to iphone, we can configure the following parameters:\n\n  - The xcodeBaseDir parameter refers to the location where Xcode has been installed. When none value is given, the\n    above value is the default.\n  - The xcodewrapperArgs parameter passes arbitrary parameters to the xcodeenv.composeXcodeWrapper {} function. This\n    can, for example, be used to adjust the default version of Xcode.\n\nWhen release has been set to true, you also need to provide the following parameters:\n\n  - iosMobileProvisioningProfile refers to a mobile provisioning profile needed for signing.\n  - iosCertificateName refers to the company name in the P12 certificate.\n  - iosCertificate refers to the path to the P12 file.\n  - iosCertificatePassword contains the password to open the P12 file.\n  - iosVersion refers to the iOS SDK version to use. It defaults to the latest version.\n  - iosBuildStore should be set to true when building for the Apple Appstore submission. For enterprise or ad-hoc builds\n    it should be set to false.\n\nWhen enableWirelessDistribution has been enabled, you must also provide the path of the PHP script (installURL) (that is\nincluded with the iOS build environment) to enable wireless ad-hoc installations.\n\nEmulating or simulating the appIt is also possible to simulate the correspond iOS simulator build by using\nxcodeenv.simulateApp {} and emulate an Android APK by using androidenv.emulateApp {}.\n" }
,{ "url": "languages-frameworks\/vim\/", "title": "Vim", "text": "VimBoth Neovim and Vim can be configured to include your favorite plugins and additional libraries.\n\nLoading can be deferred; see examples.\n\nAt the moment we support three different methods for managing plugins:\n\n  - Vim packages (recommend)\n  - VAM (=vim-addon-manager)\n  - Pathogen\n  - vim-plug\n\nCustom configurationAdding custom .vimrc lines can be done using the following code:\n\nvim_configurable.customize {\n  # `name` specifies the name of the executable and package\n  name = \"vim-with-plugins\";\n\n  vimrcConfig.customRC = ''\n    set hidden\n  '';\n}\n\nThis configuration is used when Vim is invoked with the command specified as name, in this case vim-with-plugins.\n\nFor Neovim the configure argument can be overridden to achieve the same:\n\nneovim.override {\n  configure = {\n    customRC = ''\n      # here your custom configuration goes!\n    '';\n  };\n}\n\nIf you want to use neovim-qt as a graphical editor, you can configure it by overriding Neovim in an overlay or passing\nit an overridden Neovimn:\n\nneovim-qt.override {\n  neovim = neovim.override {\n    configure = {\n      customRC = ''\n        # your custom configuration\n      '';\n    };\n  };\n}\n\nManaging plugins with Vim packagesTo store you plugins in Vim packages (the native Vim plugin manager, see :help\npackages) the following example can be used:\n\nvim_configurable.customize {\n  vimrcConfig.packages.myVimPackage = with pkgs.vimPlugins; {\n    # loaded on launch\n    start = [ youcompleteme fugitive ];\n    # manually loadable by calling `:packadd $plugin-name`\n    # however, if a Vim plugin has a dependency that is not explicitly listed in\n    # opt that dependency will always be added to start to avoid confusion.\n    opt = [ phpCompletion elm-vim ];\n    # To automatically load a plugin when opening a filetype, add vimrc lines like:\n    # autocmd FileType php :packadd phpCompletion\n  };\n}\n\nmyVimPackage is an arbitrary name for the generated package. You can choose any name you like. For Neovim the syntax is:\n\nneovim.override {\n  configure = {\n    customRC = ''\n      # here your custom configuration goes!\n    '';\n    packages.myVimPackage = with pkgs.vimPlugins; {\n      # see examples below how to use custom packages\n      start = [ ];\n      # If a Vim plugin has a dependency that is not explicitly listed in\n      # opt that dependency will always be added to start to avoid confusion.\n      opt = [ ];\n    };\n  };\n}\n\nThe resulting package can be added to packageOverrides in ~\/.nixpkgs\/config.nix to make it installable:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myVim = vim_configurable.customize {\n      # `name` specifies the name of the executable and package\n      name = \"vim-with-plugins\";\n      # add here code from the example section\n    };\n    myNeovim = neovim.override {\n      configure = {\n      # add here code from the example section\n      };\n    };\n  };\n}\n\nAfter that you can install your special grafted myVim or myNeovim packages.\n\nWhat if your favourite Vim plugin isn’t already packaged?If one of your favourite plugins isn't packaged, you can\npackage it yourself:\n\n{ config, pkgs, ... }:\n\nlet\n  easygrep = pkgs.vimUtils.buildVimPlugin {\n    name = \"vim-easygrep\";\n    src = pkgs.fetchFromGitHub {\n      owner = \"dkprice\";\n      repo = \"vim-easygrep\";\n      rev = \"d0c36a77cc63c22648e792796b1815b44164653a\";\n      sha256 = \"0y2p5mz0d5fhg6n68lhfhl8p4mlwkb82q337c22djs4w5zyzggbc\";\n    };\n  };\nin\n{\n  environment.systemPackages = [\n    (\n      pkgs.neovim.override {\n        configure = {\n          packages.myPlugins = with pkgs.vimPlugins; {\n          start = [\n            vim-go # already packaged plugin\n            easygrep # custom package\n          ];\n          opt = [];\n        };\n        # ...\n      };\n     }\n    )\n  ];\n}\n\nManaging plugins with vim-plugTo use vim-plug to manage your Vim plugins the following example can be used:\n\nvim_configurable.customize {\n  vimrcConfig.packages.myVimPackage = with pkgs.vimPlugins; {\n    # loaded on launch\n    plug.plugins = [ youcompleteme fugitive phpCompletion elm-vim ];\n  };\n}\n\nFor Neovim the syntax is:\n\nneovim.override {\n  configure = {\n    customRC = ''\n      # here your custom configuration goes!\n    '';\n    plug.plugins = with pkgs.vimPlugins; [\n      vim-go\n    ];\n  };\n}\n\nManaging plugins with VAMHandling dependencies of Vim pluginsVAM introduced .json files supporting dependencies without\nversioning assuming that \"using latest version\" is ok most of the time.\n\nExampleFirst create a vim-scripts file having one plugin name per line. Example:\n\n\"tlib\"\n{'name': 'vim-addon-sql'}\n{'filetype_regex': '\\%(vim)$', 'names': ['reload', 'vim-dev-plugin']}\n\nSuch vim-scripts file can be read by VAM as well like this:\n\ncall vam#Scripts(expand('~\/.vim-scripts'), {})\n\nCreate a default.nix file:\n\n{ nixpkgs ? import <nixpkgs> {}, compiler ? \"ghc7102\" }:\nnixpkgs.vim_configurable.customize { name = \"vim\"; vimrcConfig.vam.pluginDictionaries = [ \"vim-addon-vim2nix\" ]; }\n\nCreate a generate.vim file:\n\nActivateAddons vim-addon-vim2nix\nlet vim_scripts = \"vim-scripts\"\ncall nix#ExportPluginsForNix({\n\\  'path_to_nixpkgs': eval('{\"'.substitute(substitute(substitute($NIX_PATH, ':', ',', 'g'), '=',':', 'g'), '\\([:,]\\)', '\"\\1\"',\"g\").'\"}')[\"nixpkgs\"],\n\\  'cache_file': '\/tmp\/vim2nix-cache',\n\\  'try_catch': 0,\n\\  'plugin_dictionaries': [\"vim-addon-manager\"]+map(readfile(vim_scripts), 'eval(v:val)')\n\\ })\n\nThen run\n\nnix-shell -p vimUtils.vim_with_vim2nix --command \"vim -c 'source generate.vim'\"\n\nYou should get a Vim buffer with the nix derivations (output1) and vam.pluginDictionaries (output2). You can add your\nVim to your system's configuration file like this and start it by \"vim-my\":\n\nmy-vim =\n  let plugins = let inherit (vimUtils) buildVimPluginFrom2Nix; in {\n    copy paste output1 here\n  }; in vim_configurable.customize {\n    name = \"vim-my\";\n\n    vimrcConfig.vam.knownPlugins = plugins; # optional\n    vimrcConfig.vam.pluginDictionaries = [\n       copy paste output2 here\n    ];\n\n    # Pathogen would be\n    # vimrcConfig.pathogen.knownPlugins = plugins; # plugins\n    # vimrcConfig.pathogen.pluginNames = [\"tlib\"];\n  };\n\nSample output1:\n\n\"reload\" = buildVimPluginFrom2Nix { # created by nix#NixDerivation\n  name = \"reload\";\n  src = fetchgit {\n    url = \"git:\/\/github.com\/xolox\/vim-reload\";\n    rev = \"0a601a668727f5b675cb1ddc19f6861f3f7ab9e1\";\n    sha256 = \"0vb832l9yxj919f5hfg6qj6bn9ni57gnjd3bj7zpq7d4iv2s4wdh\";\n  };\n  dependencies = [\"nim-misc\"];\n\n};\n[...]\n\nSample output2:\n\n[\n  ''vim-addon-manager''\n  ''tlib''\n  { \"name\" = ''vim-addon-sql''; }\n  { \"filetype_regex\" = ''\\%(vim)$$''; \"names\" = [ ''reload'' ''vim-dev-plugin'' ]; }\n]\n\nAdding new plugins to nixpkgsNix expressions for Vim plugins are stored in pkgs\/misc\/vim-plugins. For the vast majority\nof plugins, Nix expressions are automatically generated by running .\/update.py. This creates a generated.nix file based\non the plugins listed in vim-plugin-names. Plugins are listed in alphabetical order in vim-plugin-names using the format\n[github username]\/[repository]. For example https:\/\/github.com\/scrooloose\/nerdtree becomes scrooloose\/nerdtree.\n\nSome plugins require overrides in order to function properly. Overrides are placed in overrides.nix. Overrides are most\noften required when a plugin requires some dependencies, or extra steps are required during the build process. For\nexample deoplete-fish requires both deoplete-nvim and vim-fish, and so the following override was added:\n\ndeoplete-fish = super.deoplete-fish.overrideAttrs(old: {\n  dependencies = with super; [ deoplete-nvim vim-fish ];\n});\n\nSometimes plugins require an override that must be changed when the plugin is updated. This can cause issues when Vim\nplugins are auto-updated but the associated override isn't updated. For these plugins, the override should be written so\nthat it specifies all information required to install the plugin, and running .\/update.py doesn't change the derivation\nfor the plugin. Manually updating the override is required to update these types of plugins. An example of such a plugin\nis LanguageClient-neovim.\n\nTo add a new plugin, run .\/update.py --add \"[owner]\/[name]\". NOTE: This script automatically commits to your git\nrepository. Be sure to check out a fresh branch before running.\n\nFinally, there are some plugins that are also packaged in nodePackages because they have Javascript-related build steps,\nsuch as running webpack. Those plugins are not listed in vim-plugin-names or managed by update.py at all, and are\nincluded separately in overrides.nix. Currently, all these plugins are related to the coc.nvim ecosystem of Language\nServer Protocol integration with vim\/neovim.\n\nUpdating plugins in nixpkgsRun the update script with a GitHub API token that has at least public_repo access. Running\nthe script without the token is likely to result in rate-limiting (429 errors). For steps on creating an API token,\nplease refer to GitHub's token documentation.\n\nGITHUB_API_TOKEN=my_token .\/pkgs\/misc\/vim-plugins\/update.py\n\nAlternatively, set the number of processes to a lower count to avoid rate-limiting.\n\n.\/pkgs\/misc\/vim-plugins\/update.py --proc 1\n\nImportant repositories  - vim-pi is a plugin repository from VAM plugin manager meant to be used by others as well used\n    by\n\n  - vim2nix which generates the .nix code\n" }
,{ "url": "builders\/packages\/cataclysm-dda\/", "title": "Cataclysm: Dark Days Ahead", "text": "Cataclysm: Dark Days AheadHow to install Cataclysm DDATo install the latest stable release of Cataclysm DDA to your\nprofile, execute nix-env -f \"<nixpkgs>\" -iA cataclysm-dda. For the curses build (build without tiles), install\ncataclysmDDA.stable.curses. Note: cataclysm-dda is an alias to cataclysmDDA.stable.tiles.\n\nIf you like access to a development build of your favorite git revision, override cataclysm-dda-git (or\ncataclysmDDA.git.curses if you like curses build):\n\ncataclysm-dda-git.override {\n  version = \"YYYY-MM-DD\";\n  rev = \"YOUR_FAVORITE_REVISION\";\n  sha256 = \"CHECKSUM_OF_THE_REVISION\";\n}\n\nThe sha256 checksum can be obtained by\n\nnix-prefetch-url --unpack \"https:\/\/github.com\/CleverRaven\/Cataclysm-DDA\/archive\/${YOUR_FAVORITE_REVISION}.tar.gz\"\n\nThe default configuration directory is ~\/.cataclysm-dda. If you prefer $XDG_CONFIG_HOME\/cataclysm-dda, override the\nderivation:\n\ncataclysm-dda.override {\n  useXdgDir = true;\n}\n\nImportant note for overriding packagesAfter applying overrideAttrs, you need to fix passthru.pkgs and passthru.withMods\nattributes either manually or by using attachPkgs:\n\nlet\n  # You enabled parallel building.\n  myCDDA = cataclysm-dda-git.overrideAttrs (_: {\n    enableParallelBuilding = true;\n  });\n\n  # Unfortunately, this refers to the package before overriding and\n  # parallel building is still disabled.\n  badExample = myCDDA.withMods (_: []);\n\n  inherit (cataclysmDDA) attachPkgs pkgs wrapCDDA;\n\n  # You can fix it by hand\n  goodExample1 = myCDDA.overrideAttrs (old: {\n    passthru = old.passthru \/\/ {\n      pkgs = pkgs.override { build = goodExample1; };\n      withMods = wrapCDDA goodExample1;\n    };\n  });\n\n  # or by using a helper function `attachPkgs`.\n  goodExample2 = attachPkgs pkgs myCDDA;\nin\n\n# badExample                     # parallel building disabled\n# goodExample1.withMods (_: [])  # parallel building enabled\ngoodExample2.withMods (_: [])    # parallel building enabled\n\nCustomizing with modsTo install Cataclysm DDA with mods of your choice, you can use withMods attribute:\n\ncataclysm-dda.withMods (mods: with mods; [\n  tileset.UndeadPeople\n])\n\nAll mods, soundpacks, and tilesets available in nixpkgs are found in cataclysmDDA.pkgs.\n\nHere is an example to modify existing mods and\/or add more mods not available in nixpkgs:\n\nlet\n  customMods = self: super: lib.recursiveUpdate super {\n    # Modify existing mod\n    tileset.UndeadPeople = super.tileset.UndeadPeople.overrideAttrs (old: {\n      # If you like to apply a patch to the tileset for example\n      patches = [ .\/path\/to\/your.patch ];\n    });\n\n    # Add another mod\n    mod.Awesome = cataclysmDDA.buildMod {\n      modName = \"Awesome\";\n      version = \"0.x\";\n      src = fetchFromGitHub {\n        owner = \"Someone\";\n        repo = \"AwesomeMod\";\n        rev = \"...\";\n        sha256 = \"...\";\n      };\n      # Path to be installed in the unpacked source (default: \".\")\n      modRoot = \"contents\/under\/this\/path\/will\/be\/installed\";\n    };\n\n    # Add another soundpack\n    soundpack.Fantastic = cataclysmDDA.buildSoundPack {\n      # ditto\n    };\n\n    # Add another tileset\n    tileset.SuperDuper = cataclysmDDA.buildTileSet {\n      # ditto\n    };\n  };\nin\ncataclysm-dda.withMods (mods: with mods.extend customMods; [\n  tileset.UndeadPeople\n  mod.Awesome\n  soundpack.Fantastic\n  tileset.SuperDuper\n])\n" }
,{ "url": "builders\/packages\/eclipse\/", "title": "Eclipse", "text": "EclipseThe Nix expressions related to the Eclipse platform and IDE are in pkgs\/applications\/editors\/eclipse.\n\nNixpkgs provides a number of packages that will install Eclipse in its various forms. These range from the bare-bones\nEclipse Platform to the more fully featured Eclipse SDK or Scala-IDE packages and multiple version are often available.\nIt is possible to list available Eclipse packages by issuing the command:\n\n$ nix-env -f '<nixpkgs>' -qaP -A eclipses --description\n\nOnce an Eclipse variant is installed it can be run using the eclipse command, as expected. From within Eclipse it is\nthen possible to install plugins in the usual manner by either manually specifying an Eclipse update site or by\ninstalling the Marketplace Client plugin and using it to discover and install other plugins. This installation method\nprovides an Eclipse installation that closely resemble a manually installed Eclipse.\n\nIf you prefer to install plugins in a more declarative manner then Nixpkgs also offer a number of Eclipse plugins that\ncan be installed in an Eclipse environment. This type of environment is created using the function eclipseWithPlugins\nfound inside the nixpkgs.eclipses attribute set. This function takes as argument { eclipse, plugins ? [], jvmArgs ? [] }\nwhere eclipse is a one of the Eclipse packages described above, plugins is a list of plugin derivations, and jvmArgs is\na list of arguments given to the JVM running the Eclipse. For example, say you wish to install the latest Eclipse\nPlatform with the popular Eclipse Color Theme plugin and also allow Eclipse to use more RAM. You could then add\n\npackageOverrides = pkgs: {\n  myEclipse = with pkgs.eclipses; eclipseWithPlugins {\n    eclipse = eclipse-platform;\n    jvmArgs = [ \"-Xmx2048m\" ];\n    plugins = [ plugins.color-theme ];\n  };\n}\n\nto your Nixpkgs configuration (~\/.config\/nixpkgs\/config.nix) and install it by running nix-env -f '<nixpkgs>' -iA\nmyEclipse and afterward run Eclipse as usual. It is possible to find out which plugins are available for installation\nusing eclipseWithPlugins by running\n\n$ nix-env -f '<nixpkgs>' -qaP -A eclipses.plugins --description\n\nIf there is a need to install plugins that are not available in Nixpkgs then it may be possible to define these plugins\noutside Nixpkgs using the buildEclipseUpdateSite and buildEclipsePlugin functions found in the nixpkgs.eclipses.plugins\nattribute set. Use the buildEclipseUpdateSite function to install a plugin distributed as an Eclipse update site. This\nfunction takes { name, src } as argument where src indicates the Eclipse update site archive. All Eclipse features and\nplugins within the downloaded update site will be installed. When an update site archive is not available then the\nbuildEclipsePlugin function can be used to install a plugin that consists of a pair of feature and plugin JARs. This\nfunction takes an argument { name, srcFeature, srcPlugin } where srcFeature and srcPlugin are the feature and plugin\nJARs, respectively.\n\nExpanding the previous example with two plugins using the above functions we have\n\npackageOverrides = pkgs: {\n  myEclipse = with pkgs.eclipses; eclipseWithPlugins {\n    eclipse = eclipse-platform;\n    jvmArgs = [ \"-Xmx2048m\" ];\n    plugins = [\n      plugins.color-theme\n      (plugins.buildEclipsePlugin {\n        name = \"myplugin1-1.0\";\n        srcFeature = fetchurl {\n          url = \"http:\/\/…\/features\/myplugin1.jar\";\n          sha256 = \"123…\";\n        };\n        srcPlugin = fetchurl {\n          url = \"http:\/\/…\/plugins\/myplugin1.jar\";\n          sha256 = \"123…\";\n        };\n      });\n      (plugins.buildEclipseUpdateSite {\n        name = \"myplugin2-1.0\";\n        src = fetchurl {\n          stripRoot = false;\n          url = \"http:\/\/…\/myplugin2.zip\";\n          sha256 = \"123…\";\n        };\n      });\n    ];\n  };\n}\n" }
,{ "url": "builders\/packages\/elm\/", "title": "Elm", "text": "ElmTo start a development environment do\n\nnix-shell -p elmPackages.elm elmPackages.elm-format\n\nTo update the Elm compiler, see nixpkgs\/pkgs\/development\/compilers\/elm\/README.md.\n\nTo package Elm applications, read about elm2nix.\n" }
,{ "url": "builders\/packages\/emacs\/", "title": "Emacs", "text": "EmacsConfiguring EmacsThe Emacs package comes with some extra helpers to make it easier to configure.\nemacs.pkgs.withPackages allows you to manage packages from ELPA. This means that you will not have to install that\npackages from within Emacs. For instance, if you wanted to use company counsel, flycheck, ivy, magit, projectile, and\nuse-package you could use this as a ~\/.config\/nixpkgs\/config.nix override:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myEmacs = emacs.pkgs.withPackages (epkgs: (with epkgs.melpaStablePackages; [\n      company\n      counsel\n      flycheck\n      ivy\n      magit\n      projectile\n      use-package\n    ]));\n  }\n}\n\nYou can install it like any other packages via nix-env -iA myEmacs. However, this will only install those packages. It\nwill not configure them for us. To do this, we need to provide a configuration file. Luckily, it is possible to do this\nfrom within Nix! By modifying the above example, we can make Emacs load a custom config file. The key is to create a\npackage that provide a default.el file in \/share\/emacs\/site-start\/. Emacs knows to load this file automatically when it\nstarts.\n\n{\n  packageOverrides = pkgs: with pkgs; rec {\n    myEmacsConfig = writeText \"default.el\" ''\n      ;; initialize package\n\n      (require 'package)\n      (package-initialize 'noactivate)\n      (eval-when-compile\n        (require 'use-package))\n\n      ;; load some packages\n\n      (use-package company\n        :bind (\"<C-tab>\" . company-complete)\n        :diminish company-mode\n        :commands (company-mode global-company-mode)\n        :defer 1\n        :config\n        (global-company-mode))\n\n      (use-package counsel\n        :commands (counsel-descbinds)\n        :bind (([remap execute-extended-command] . counsel-M-x)\n               (\"C-x C-f\" . counsel-find-file)\n               (\"C-c g\" . counsel-git)\n               (\"C-c j\" . counsel-git-grep)\n               (\"C-c k\" . counsel-ag)\n               (\"C-x l\" . counsel-locate)\n               (\"M-y\" . counsel-yank-pop)))\n\n      (use-package flycheck\n        :defer 2\n        :config (global-flycheck-mode))\n\n      (use-package ivy\n        :defer 1\n        :bind ((\"C-c C-r\" . ivy-resume)\n               (\"C-x C-b\" . ivy-switch-buffer)\n               :map ivy-minibuffer-map\n               (\"C-j\" . ivy-call))\n        :diminish ivy-mode\n        :commands ivy-mode\n        :config\n        (ivy-mode 1))\n\n      (use-package magit\n        :defer\n        :if (executable-find \"git\")\n        :bind ((\"C-x g\" . magit-status)\n               (\"C-x G\" . magit-dispatch-popup))\n        :init\n        (setq magit-completing-read-function 'ivy-completing-read))\n\n      (use-package projectile\n        :commands projectile-mode\n        :bind-keymap (\"C-c p\" . projectile-command-map)\n        :defer 5\n        :config\n        (projectile-global-mode))\n    '';\n\n    myEmacs = emacs.pkgs.withPackages (epkgs: (with epkgs.melpaStablePackages; [\n      (runCommand \"default.el\" {} ''\n         mkdir -p $out\/share\/emacs\/site-lisp\n         cp ${myEmacsConfig} $out\/share\/emacs\/site-lisp\/default.el\n       '')\n      company\n      counsel\n      flycheck\n      ivy\n      magit\n      projectile\n      use-package\n    ]));\n  };\n}\n\nThis provides a fairly full Emacs start file. It will load in addition to the user's presonal config. You can always\ndisable it by passing -q to the Emacs command.\n\nSometimes emacs.pkgs.withPackages is not enough, as this package set has some priorities imposed on packages (with the\nlowest priority assigned to Melpa Unstable, and the highest for packages manually defined in\npkgs\/top-level\/emacs-packages.nix). But you can't control this priorities when some package is installed as a\ndependency. You can override it on per-package-basis, providing all the required dependencies manually - but it's\ntedious and there is always a possibility that an unwanted dependency will sneak in through some other package. To\ncompletely override such a package you can use overrideScope'.\n\noverrides = self: super: rec {\n  haskell-mode = self.melpaPackages.haskell-mode;\n  ...\n};\n((emacsPackagesFor emacs).overrideScope' overrides).emacs.pkgs.withPackages\n  (p: with p; [\n    # here both these package will use haskell-mode of our own choice\n    ghc-mod\n    dante\n  ])\n" }
,{ "url": "builders\/packages\/firefox\/", "title": "Firefox", "text": "FirefoxBuild wrapped Firefox with extensions and policiesThe wrapFirefox function allows to pass policies, preferences\nand extension that are available to firefox. With the help of fetchFirefoxAddon this allows build a firefox version that\nalready comes with addons pre-installed:\n\n{\n  myFirefox = wrapFirefox firefox-unwrapped {\n    nixExtensions = [\n      (fetchFirefoxAddon {\n        name = \"ublock\"; # Has to be unique!\n        url = \"https:\/\/addons.mozilla.org\/firefox\/downloads\/file\/3679754\/ublock_origin-1.31.0-an+fx.xpi\";\n        sha256 = \"1h768ljlh3pi23l27qp961v1hd0nbj2vasgy11bmcrlqp40zgvnr\";\n      })\n    ];\n\n    extraPolicies = {\n      CaptivePortal = false;\n      DisableFirefoxStudies = true;\n      DisablePocket = true;\n      DisableTelemetry = true;\n      DisableFirefoxAccounts = true;\n      FirefoxHome = {\n        Pocket = false;\n        Snippets = false;\n      };\n       UserMessaging = {\n         ExtensionRecommendations = false;\n         SkipOnboarding = true;\n       };\n    };\n\n    extraPrefs = ''\n      \/\/ Show more ssl cert infos\n      lockPref(\"security.identityblock.show_extended_validation\", true);\n    '';\n  };\n}\n\nIf nixExtensions != null then all manually installed addons will be uninstalled from your browser profile. To view\navailable enterprise policies visit enterprise policies or type into the Firefox url bar: about:policies#documentation.\nNix installed addons do not have a valid signature, which is why signature verification is disabled. This does not\ncompromise security because downloaded addons are checksumed and manual addons can't be installed. Also make sure that\nthe name field of fetchFirefoxAddon is unique. If you remove an addon from the nixExtensions array, rebuild and start\nFirefox the removed addon will be completly removed with all of its settings.\n\nTroubleshootingIf addons do not appear installed although they have been defined in your nix configuration file reset\nthe local addon state of your Firefox profile by clicking help -> restart with addons disabled -> restart -> refresh\nfirefox. This can happen if you switch from manual addon mode to nix addon mode and then back to manual mode and then\nagain to nix addon mode.\n" }
,{ "url": "builders\/packages\/fish\/", "title": "Fish", "text": "FishFish is a \"smart and user-friendly command line shell\" with support for plugins.\n\nVendor Fish scriptsAny package may ship its own Fish completions, configuration snippets, and functions. Those should be\ninstalled to $out\/share\/fish\/vendor_{completions,conf,functions}.d respectively.\n\nWhen the programs.fish.enable and programs.fish.vendor.{completions,config,functions}.enable options from the NixOS Fish\nmodule are set to true, those paths are symlinked in the current system environment and automatically loaded by Fish.\n\nPackaging Fish pluginsWhile packages providing standalone executables belong to the top level, packages which have the\nsole purpose of extending Fish belong to the fishPlugins scope and should be registered in\npkgs\/shells\/fish\/plugins\/default.nix.\n\nThe buildFishPlugin utility function can be used to automatically copy Fish scripts from\n$src\/{completions,conf,conf.d,functions} to the standard vendor installation paths. It also sets up the test environment\nso that the optional checkPhase is executed in a Fish shell with other already packaged plugins and package-local Fish\nfunctions specified in checkPlugins and checkFunctionDirs respectively.\n\nSee pkgs\/shells\/fish\/plugins\/pure.nix for an example of Fish plugin package using buildFishPlugin and running unit tests\nwith the fishtape test runner.\n\nFish wrapperThe wrapFish package is a wrapper around Fish which can be used to create Fish shells initialised with some\nplugins as well as completions, configuration snippets and functions sourced from the given paths. This provides a\nconvenient way to test Fish plugins and scripts without having to alter the environment.\n\nwrapFish {\n  pluginPkgs = with fishPlugins; [ pure foreign-env ];\n  completionDirs = [];\n  functionDirs = [];\n  confDirs = [ \"\/path\/to\/some\/fish\/init\/dir\/\" ];\n}\n" }
,{ "url": "builders\/packages\/kakoune\/", "title": "Kakoune", "text": "KakouneKakoune can be built to autoload plugins:\n\n(kakoune.override {\n  plugins = with pkgs.kakounePlugins; [ parinfer-rust ];\n})\n" }
,{ "url": "builders\/packages\/linux\/", "title": "Linux kernel", "text": "Linux kernelThe Nix expressions to build the Linux kernel are in pkgs\/os-specific\/linux\/kernel.\n\nThe function that builds the kernel has an argument kernelPatches which should be a list of {name, patch, extraConfig}\nattribute sets, where name is the name of the patch (which is included in the kernel’s meta.description attribute),\npatch is the patch itself (possibly compressed), and extraConfig (optional) is a string specifying extra options to be\nconcatenated to the kernel configuration file (.config).\n\nThe kernel derivation exports an attribute features specifying whether optional functionality is or isn’t enabled. This\nis used in NixOS to implement kernel-specific behaviour. For instance, if the kernel has the iwlwifi feature (i.e. has\nbuilt-in support for Intel wireless chipsets), then NixOS doesn’t have to build the external iwlwifi package:\n\nmodulesTree = [kernel]\n  ++ pkgs.lib.optional (!kernel.features ? iwlwifi) kernelPackages.iwlwifi\n  ++ ...;\n\nHow to add a new (major) version of the Linux kernel to Nixpkgs:\n\n1.  Copy the old Nix expression (e.g. linux-2.6.21.nix) to the new one (e.g. linux-2.6.22.nix) and update it.\n\n2.  Add the new kernel to all-packages.nix (e.g., create an attribute kernel_2_6_22).\n\n3.  Now we’re going to update the kernel configuration. First unpack the kernel. Then for each supported platform (i686,\n    x86_64, uml) do the following:\n    \n    1.  Make an copy from the old config (e.g. config-2.6.21-i686-smp) to the new one (e.g. config-2.6.22-i686-smp).\n    \n    2.  Copy the config file for this platform (e.g. config-2.6.22-i686-smp) to .config in the kernel source tree.\n    \n    3.  Run make oldconfig ARCH={i386,x86_64,um} and answer all questions. (For the uml configuration, also add\n        SHELL=bash.) Make sure to keep the configuration consistent between platforms (i.e. don’t enable some feature on\n        i686 and disable it on x86_64).\n    \n    4.  If needed you can also run make menuconfig:\n        \n        $ nix-env -i ncurses\n        $ export NIX_CFLAGS_LINK=-lncurses\n        $ make menuconfig ARCH=arch\n\n    5.  Copy .config over the new config file (e.g. config-2.6.22-i686-smp).\n\n4.  Test building the kernel: nix-build -A kernel_2_6_22. If it compiles, ship it! For extra credit, try booting NixOS\n    with it.\n\n5.  It may be that the new kernel requires updating the external kernel modules and kernel-dependent packages listed in\n    the linuxPackagesFor function in all-packages.nix (such as the NVIDIA drivers, AUFS, etc.). If the updated packages\n    aren’t backwards compatible with older kernels, you may need to keep the older versions around.\n" }
,{ "url": "builders\/packages\/locales\/", "title": "Locales", "text": "LocalesTo allow simultaneous use of packages linked against different versions of glibc with different locale archive\nformats Nixpkgs patches glibc to rely on LOCALE_ARCHIVE environment variable.\n\nOn non-NixOS distributions this variable is obviously not set. This can cause regressions in language support or even\ncrashes in some Nixpkgs-provided programs. The simplest way to mitigate this problem is exporting the LOCALE_ARCHIVE\nvariable pointing to ${glibcLocales}\/lib\/locale\/locale-archive. The drawback (and the reason this is not the default) is\nthe relatively large (a hundred MiB) size of the full set of locales. It is possible to build a custom set of locales by\noverriding parameters allLocales and locales of the package.\n" }
,{ "url": "builders\/packages\/nginx\/", "title": "Nginx", "text": "NginxNginx is a reverse proxy and lightweight webserver.\n\nETags on static files served from the Nix storeHTTP has a couple different mechanisms for caching to prevent clients\nfrom having to download the same content repeatedly if a resource has not changed since the last time it was requested.\nWhen nginx is used as a server for static files, it implements the caching mechanism based on the Last-Modified response\nheader automatically; unfortunately, it works by using filesystem timestamps to determine the value of the Last-Modified\nheader. This doesn't give the desired behavior when the file is in the Nix store, because all file timestamps are set\nto 0 (for reasons related to build reproducibility).\n\nFortunately, HTTP supports an alternative (and more effective) caching mechanism: the ETag response header. The value of\nthe ETag header specifies some identifier for the particular content that the server is sending (e.g. a hash). When a\nclient makes a second request for the same resource, it sends that value back in an If-None-Match header. If the ETag\nvalue is unchanged, then the server does not need to resend the content.\n\nAs of NixOS 19.09, the nginx package in Nixpkgs is patched such that when nginx serves a file out of \/nix\/store, the\nhash in the store path is used as the ETag header in the HTTP response, thus providing proper caching functionality.\nThis happens automatically; you do not need to do modify any configuration to get this behavior.\n" }
,{ "url": "builders\/packages\/opengl\/", "title": "OpenGL", "text": "OpenGLOpenGL support varies depending on which hardware is used and which drivers are available and loaded.\n\nBroadly, we support both GL vendors: Mesa and NVIDIA.\n\nNixOS DesktopThe NixOS desktop or other non-headless configurations are the primary target for OpenGL libraries and\napplications. The current solution for discovering which drivers are available is based on libglvnd. libglvnd performs\n\"vendor-neutral dispatch\", trying a variety of techniques to find the system's GL implementation. In practice, this will\nbe either via standard GLX for X11 users or EGL for Wayland users, and supporting either NVIDIA or Mesa extensions.\n\nNix on GNU\/LinuxIf you are using a non-NixOS GNU\/Linux\/X11 desktop with free software video drivers, consider launching\nOpenGL-dependent programs from Nixpkgs with Nixpkgs versions of libglvnd and mesa.drivers in LD_LIBRARY_PATH. For Mesa\ndrivers, the Linux kernel version doesn't have to match nixpkgs.\n\nFor proprietary video drivers you might have luck with also adding the corresponding video driver package.\n" }
,{ "url": "builders\/packages\/shell-helpers\/", "title": "Interactive shell helpers", "text": "Interactive shell helpersSome packages provide the shell integration to be more useful. But unlike other systems, nix\ndoesn't have a standard share directory location. This is why a bunch PACKAGE-share scripts are shipped that print the\nlocation of the corresponding shared folder. Current list of such packages is as following:\n\n  - fzf : fzf-share\n\nE.g. fzf can then used in the .bashrc like this:\n\nsource \"$(fzf-share)\/completion.bash\"\nsource \"$(fzf-share)\/key-bindings.bash\"\n" }
,{ "url": "builders\/packages\/steam\/", "title": "Steam", "text": "SteamSteam in NixSteam is distributed as a .deb file, for now only as an i686 package (the amd64 package only has\ndocumentation). When unpacked, it has a script called steam that in Ubuntu (their target distro) would go to \/usr\/bin.\nWhen run for the first time, this script copies some files to the user's home, which include another script that is the\nultimate responsible for launching the steam binary, which is also in $HOME.\n\nNix problems and constraints:\n\n  - We don't have \/bin\/bash and many scripts point there. Similarly for \/usr\/bin\/python.\n  - We don't have the dynamic loader in \/lib.\n  - The steam.sh script in $HOME can not be patched, as it is checked and rewritten by steam.\n  - The steam binary cannot be patched, it's also checked.\n\nThe current approach to deploy Steam in NixOS is composing a FHS-compatible chroot environment, as documented here. This\nallows us to have binaries in the expected paths without disrupting the system, and to avoid patching them to work in a\nnon FHS environment.\n\nHow to playUse programs.steam.enable = true; if you want to add steam to systemPackages and also enable a few\nworkarrounds aswell as Steam controller support or other Steam supported controllers such as the DualShock 4 or Nintendo\nSwitch Pr.\n\nTroubleshooting  - Steam fails to start. What do I do?\n    \n    Try to run\n    \n    strace steam\n\n    to see what is causing steam to fail.\n\n  - Using the FOSS Radeon or nouveau (nvidia) drivers\n    \n      - The newStdcpp parameter was removed since NixOS 17.09 and should not be needed anymore.\n    \n      - Steam ships statically linked with a version of libcrypto that conflics with the one dynamically loaded by\n        radeonsi_dri.so. If you get the error\n        \n        steam.sh: line 713: 7842 Segmentation fault (core dumped)\n\n        have a look at this pull request.\n\n  - Java\n    \n    1.  There is no java in steam chrootenv by default. If you get a message like\n    \n    \/home\/foo\/.local\/share\/Steam\/SteamApps\/common\/towns\/towns.sh: line 1: java: command not found\n\n    you need to add\n    \n    steam.override { withJava = true; };\n\nsteam-runThe FHS-compatible chroot used for steam can also be used to run other linux games that expect a FHS\nenvironment. To do it, add\n\npkgs.steam.override ({\n  nativeOnly = true;\n  newStdcpp = true;\n}).run\n\nto your configuration, rebuild, and run the game with\n\nsteam-run .\/foo\n" }
,{ "url": "builders\/packages\/urxvt\/", "title": "Urxvt", "text": "UrxvtUrxvt, also known as rxvt-unicode, is a highly customizable terminal emulator.\n\nConfiguring urxvtIn nixpkgs, urxvt is provided by the package rxvt-unicode. It can be configured to include your choice\nof plugins, reducing its closure size from the default configuration which includes all available plugins. To make use\nof this functionality, use an overlay or directly install an expression that overrides its configuration, such as\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    plugins = with availablePlugins; [ perls resize-font vtwheel ];\n  };\n}\n\nIf the configure function returns an attrset without the plugins attribute, availablePlugins will be used automatically.\n\nIn order to add plugins but also keep all default plugins installed, it is possible to use the following method:\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    plugins = (builtins.attrValues availablePlugins) ++ [ custom-plugin ];\n  };\n}\n\nTo get a list of all the plugins available, open the Nix REPL and run\n\n$ nix repl\n:l <nixpkgs>\nmap (p: p.name) pkgs.rxvt-unicode.plugins\n\nAlternatively, if your shell is bash or zsh and have completion enabled, simply type nixpkgs.rxvt-unicode.plugins.<tab>.\n\nIn addition to plugins the options extraDeps and perlDeps can be used to install extra packages. extraDeps can be used,\nfor example, to provide xsel (a clipboard manager) to the clipboard plugin, without installing it globally:\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    pluginsDeps = [ xsel ];\n  };\n}\n\nperlDeps is a handy way to provide Perl packages to your custom plugins (in $HOME\/.urxvt\/ext). For example, if you need\nAnyEvent you can do:\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    perlDeps = with perlPackages; [ AnyEvent ];\n  };\n}\n\nPackaging urxvt pluginsUrxvt plugins resides in pkgs\/applications\/misc\/rxvt-unicode-plugins. To add a new plugin create\nan expression in a subdirectory and add the package to the set in\npkgs\/applications\/misc\/rxvt-unicode-plugins\/default.nix.\n\nA plugin can be any kind of derivation, the only requirement is that it should always install perl scripts in\n$out\/lib\/urxvt\/perl. Look for existing plugins for examples.\n\nIf the plugin is itself a perl package that needs to be imported from other plugins or scripts, add the following\npassthrough:\n\npassthru.perlPackages = [ \"self\" ];\n\nThis will make the urxvt wrapper pick up the dependency and set up the perl path accordingly.\n" }
,{ "url": "builders\/packages\/weechat\/", "title": "Weechat", "text": "WeechatWeechat can be configured to include your choice of plugins, reducing its closure size from the default\nconfiguration which includes all available plugins. To make use of this functionality, install an expression that\noverrides its configuration such as\n\nweechat.override {configure = {availablePlugins, ...}: {\n    plugins = with availablePlugins; [ python perl ];\n  }\n}\n\nIf the configure function returns an attrset without the plugins attribute, availablePlugins will be used automatically.\n\nThe plugins currently available are python, perl, ruby, guile, tcl and lua.\n\nThe python and perl plugins allows the addition of extra libraries. For instance, the inotify.py script in\nweechat-scripts requires D-Bus or libnotify, and the fish.py script requires pycrypto. To use these scripts, use the\nplugin's withPackages attribute:\n\nweechat.override { configure = {availablePlugins, ...}: {\n    plugins = with availablePlugins; [\n            (python.withPackages (ps: with ps; [ pycrypto python-dbus ]))\n        ];\n    };\n}\n\nIn order to also keep all default plugins installed, it is possible to use the following method:\n\nweechat.override { configure = { availablePlugins, ... }: {\n  plugins = builtins.attrValues (availablePlugins \/\/ {\n    python = availablePlugins.python.withPackages (ps: with ps; [ pycrypto python-dbus ]);\n  });\n}; }\n\nWeeChat allows to set defaults on startup using the --run-command. The configure method can be used to pass commands to\nthe program:\n\nweechat.override {\n  configure = { availablePlugins, ... }: {\n    init = ''\n      \/set foo bar\n      \/server add freenode chat.freenode.org\n    '';\n  };\n}\n\nFurther values can be added to the list of commands when running weechat --run-command \"your-commands\".\n\nAdditionally it's possible to specify scripts to be loaded when starting weechat. These will be loaded before the\ncommands from init:\n\nweechat.override {\n  configure = { availablePlugins, ... }: {\n    scripts = with pkgs.weechatScripts; [\n      weechat-xmpp weechat-matrix-bridge wee-slack\n    ];\n    init = ''\n      \/set plugins.var.python.jabber.key \"val\"\n    '':\n  };\n}\n\nIn nixpkgs there's a subpackage which contains derivations for WeeChat scripts. Such derivations expect a\npassthru.scripts attribute which contains a list of all scripts inside the store path. Furthermore all scripts have to\nlive in $out\/share. An exemplary derivation looks like this:\n\n{ stdenv, fetchurl }:\n\nstdenv.mkDerivation {\n  name = \"exemplary-weechat-script\";\n  src = fetchurl {\n    url = \"https:\/\/scripts.tld\/your-scripts.tar.gz\";\n    sha256 = \"...\";\n  };\n  passthru.scripts = [ \"foo.py\" \"bar.lua\" ];\n  installPhase = ''\n    mkdir $out\/share\n    cp foo.py $out\/share\n    cp bar.lua $out\/share\n  '';\n}\n" }
,{ "url": "builders\/packages\/xorg\/", "title": "X.org", "text": "X.orgThe Nix expressions for the X.org packages reside in pkgs\/servers\/x11\/xorg\/default.nix. This file is automatically\ngenerated from lists of tarballs in an X.org release. As such it should not be modified directly; rather, you should\nmodify the lists, the generator script or the file pkgs\/servers\/x11\/xorg\/overrides.nix, in which you can override or add\nto the derivations produced by the generator.\n\nKatamari TarballsX.org upstream releases used to include katamari releases, which included a holistic recommended\nversion for each tarball, up until 7.7. To create a list of tarballs in a katamari release:\n\nexport release=\"X11R7.7\"\nexport url=\"mirror:\/\/xorg\/$release\/src\/everything\/\"\ncat $(PRINT_PATH=1 nix-prefetch-url $url | tail -n 1) \\\n  | perl -e 'while (<>) { if (\/(href|HREF)=\"([^\"]*.bz2)\"\/) { print \"$ENV{'url'}$2\\n\"; }; }' \\\n  | sort > \"tarballs-$release.list\"\n\nIndividual TarballsThe upstream release process for X11R7.8 does not include a planned katamari. Instead, each component\nof X.org is released as its own tarball. We maintain pkgs\/servers\/x11\/xorg\/tarballs.list as a list of tarballs for each\nindividual package. This list includes X.org core libraries and protocol descriptions, extra newer X11 interface\nlibraries, like xorg.libxcb, and classic utilities which are largely unused but still available if needed, like\nxorg.imake.\n\nGenerating Nix ExpressionsThe generator is invoked as follows:\n\ncd pkgs\/servers\/x11\/xorg\n<tarballs.list perl .\/generate-expr-from-tarballs.pl\n\nFor each of the tarballs in the .list files, the script downloads it, unpacks it, and searches its configure.ac and\n*.pc.in files for dependencies. This information is used to generate default.nix. The generator caches downloaded\ntarballs between runs. Pay close attention to the NOT FOUND: $NAME messages at the end of the run, since they may\nindicate missing dependencies. (Some might be optional dependencies, however.)\n\nOverriding the GeneratorIf the expression for a package requires derivation attributes that the generator cannot figure\nout automatically (say, patches or a postInstall hook), you should modify pkgs\/servers\/x11\/xorg\/overrides.nix.\n" }
,{ "url": "contributing\/quick-start\/", "title": "Quick Start to Adding a Package", "text": "Quick Start to Adding a PackageTo add a package to Nixpkgs:\n\n1.  Checkout the Nixpkgs source tree:\n    \n    $ git clone https:\/\/github.com\/NixOS\/nixpkgs\n    $ cd nixpkgs\n\n2.  Find a good place in the Nixpkgs tree to add the Nix expression for your package. For instance, a library package\n    typically goes into pkgs\/development\/libraries\/pkgname, while a web browser goes into\n    pkgs\/applications\/networking\/browsers\/pkgname. See Coding conventions for some hints on the tree organisation.\n    Create a directory for your package, e.g.\n    \n    $ mkdir pkgs\/development\/libraries\/libfoo\n\n3.  In the package directory, create a Nix expression — a piece of code that describes how to build the package. In this\n    case, it should be a function that is called with the package dependencies as arguments, and returns a build of the\n    package in the Nix store. The expression should usually be called default.nix.\n    \n    $ emacs pkgs\/development\/libraries\/libfoo\/default.nix\n    $ git add pkgs\/development\/libraries\/libfoo\/default.nix\n\n    You can have a look at the existing Nix expressions under pkgs\/ to see how it’s done. Here are some good ones:\n    \n      - GNU Hello: pkgs\/applications\/misc\/hello\/default.nix. Trivial package, which specifies some meta attributes which\n        is good practice.\n    \n      - GNU cpio: pkgs\/tools\/archivers\/cpio\/default.nix. Also a simple package. The generic builder in stdenv does\n        everything for you. It has no dependencies beyond stdenv.\n    \n      - GNU Multiple Precision arithmetic library (GMP): pkgs\/development\/libraries\/gmp\/5.1.x.nix. Also done by the\n        generic builder, but has a dependency on m4.\n    \n      - Pan, a GTK-based newsreader: pkgs\/applications\/networking\/newsreaders\/pan\/default.nix. Has an optional\n        dependency on gtkspell, which is only built if spellCheck is true.\n    \n      - Apache HTTPD: pkgs\/servers\/http\/apache-httpd\/2.4.nix. A bunch of optional features, variable substitutions in\n        the configure flags, a post-install hook, and miscellaneous hackery.\n    \n      - Thunderbird: pkgs\/applications\/networking\/mailreaders\/thunderbird\/default.nix. Lots of dependencies.\n    \n      - JDiskReport, a Java utility: pkgs\/tools\/misc\/jdiskreport\/default.nix. Nixpkgs doesn’t have a decent stdenv for\n        Java yet so this is pretty ad-hoc.\n    \n      - XML::Simple, a Perl module: pkgs\/top-level\/perl-packages.nix (search for the XMLSimple attribute). Most Perl\n        modules are so simple to build that they are defined directly in perl-packages.nix; no need to make a separate\n        file for them.\n    \n      - Adobe Reader: pkgs\/applications\/misc\/adobe-reader\/default.nix. Shows how binary-only packages can be supported.\n        In particular the builder uses patchelf to set the RUNPATH and ELF interpreter of the executables so that the\n        right libraries are found at runtime.\n    \n    Some notes:\n    \n      - All meta attributes are optional, but it’s still a good idea to provide at least the description, homepage and\n        license.\n    \n      - You can use nix-prefetch-url url to get the SHA-256 hash of source distributions. There are similar commands as\n        nix-prefetch-git and nix-prefetch-hg available in nix-prefetch-scripts package.\n    \n      - A list of schemes for mirror:\/\/ URLs can be found in pkgs\/build-support\/fetchurl\/mirrors.nix.\n    \n    The exact syntax and semantics of the Nix expression language, including the built-in function, are described in the\n    Nix manual in the chapter on writing Nix expressions.\n\n4.  Add a call to the function defined in the previous step to pkgs\/top-level\/all-packages.nix with some descriptive\n    name for the variable, e.g. libfoo.\n    \n    $ emacs pkgs\/top-level\/all-packages.nix\n\n    The attributes in that file are sorted by category (like “Development \/ Libraries”) that more-or-less correspond to\n    the directory structure of Nixpkgs, and then by attribute name.\n\n5.  To test whether the package builds, run the following command from the root of the nixpkgs source tree:\n    \n    $ nix-build -A libfoo\n\n    where libfoo should be the variable name defined in the previous step. You may want to add the flag -K to keep the\n    temporary build directory in case something fails. If the build succeeds, a symlink .\/result to the package in the\n    Nix store is created.\n\n6.  If you want to install the package into your profile (optional), do\n    \n    $ nix-env -f . -iA libfoo\n\n7.  Optionally commit the new package and open a pull request to nixpkgs, or use the Patches category on Discourse for\n    sending a patch without a GitHub account.\n" }
,{ "url": "contributing\/coding-conventions\/", "title": "Coding conventions", "text": "Coding conventionsSyntax  - Use 2 spaces of indentation per indentation level in Nix expressions, 4 spaces in shell\n    scripts.\n\n  - Do not use tab characters, i.e. configure your editor to use soft tabs. For instance, use (setq-default\n    indent-tabs-mode nil) in Emacs. Everybody has different tab settings so it’s asking for trouble.\n\n  - Use lowerCamelCase for variable names, not UpperCamelCase. Note, this rule does not apply to package attribute\n    names, which instead follow the rules in Coding conventions.\n\n  - Function calls with attribute set arguments are written as\n    \n    foo {\n      arg = ...;\n    }\n\n    not\n    \n    foo\n    {\n      arg = ...;\n    }\n\n    Also fine is\n    \n    foo { arg = ...; }\n\n    if it's a short call.\n\n  - In attribute sets or lists that span multiple lines, the attribute names or list elements should be aligned:\n    \n    # A long list.\n    list = [\n      elem1\n      elem2\n      elem3\n    ];\n    \n    # A long attribute set.\n    attrs = {\n      attr1 = short_expr;\n      attr2 =\n        if true then big_expr else big_expr;\n    };\n    \n    # Combined\n    listOfAttrs = [\n      {\n        attr1 = 3;\n        attr2 = \"fff\";\n      }\n      {\n        attr1 = 5;\n        attr2 = \"ggg\";\n      }\n    ];\n\n  - Short lists or attribute sets can be written on one line:\n    \n    # A short list.\n    list = [ elem1 elem2 elem3 ];\n    \n    # A short set.\n    attrs = { x = 1280; y = 1024; };\n\n  - Breaking in the middle of a function argument can give hard-to-read code, like\n    \n    someFunction { x = 1280;\n      y = 1024; } otherArg\n      yetAnotherArg\n\n    (especially if the argument is very large, spanning multiple lines).\n    \n    Better:\n    \n    someFunction\n      { x = 1280; y = 1024; }\n      otherArg\n      yetAnotherArg\n\n    or\n    \n    let res = { x = 1280; y = 1024; };\n    in someFunction res otherArg yetAnotherArg\n\n  - The bodies of functions, asserts, and withs are not indented to prevent a lot of superfluous indentation levels,\n    i.e.\n    \n    { arg1, arg2 }:\n    assert system == \"i686-linux\";\n    stdenv.mkDerivation { ...\n\n    not\n    \n    { arg1, arg2 }:\n      assert system == \"i686-linux\";\n        stdenv.mkDerivation { ...\n\n  - Function formal arguments are written as:\n    \n    { arg1, arg2, arg3 }:\n\n    but if they don't fit on one line they're written as:\n    \n    { arg1, arg2, arg3\n    , arg4, ...\n    , # Some comment...\n      argN\n    }:\n\n  - Functions should list their expected arguments as precisely as possible. That is, write\n    \n    { stdenv, fetchurl, perl }: ...\n\n    instead of\n    \n    args: with args; ...\n\n    or\n    \n    { stdenv, fetchurl, perl, ... }: ...\n\n    For functions that are truly generic in the number of arguments (such as wrappers around mkDerivation) that have\n    some required arguments, you should write them using an @-pattern:\n    \n    { stdenv, doCoverageAnalysis ? false, ... } @ args:\n    \n    stdenv.mkDerivation (args \/\/ {\n      ... if doCoverageAnalysis then \"bla\" else \"\" ...\n    })\n\n    instead of\n    \n    args:\n    \n    args.stdenv.mkDerivation (args \/\/ {\n      ... if args ? doCoverageAnalysis && args.doCoverageAnalysis then \"bla\" else \"\" ...\n    })\n\n  - Unnecessary string conversions should be avoided. Do\n    \n    rev = version;\n\n    instead of\n    \n    rev = \"${version}\";\n\n  - Arguments should be listed in the order they are used, with the exception of lib, which always goes first.\n\nPackage namingThe key words must, must not, required, shall, shall not, should, should not, recommended, may, and\noptional in this section are to be interpreted as described in RFC 2119. Only emphasized words are to be interpreted in\nthis way.\n\nIn Nixpkgs, there are generally three different names associated with a package:\n\n  - The name attribute of the derivation (excluding the version part). This is what most users see, in particular when\n    using nix-env.\n\n  - The variable name used for the instantiated package in all-packages.nix, and when passing it as a dependency to\n    other functions. Typically this is called the package attribute name. This is what Nix expression authors see. It\n    can also be used when installing using nix-env -iA.\n\n  - The filename for (the directory containing) the Nix expression.\n\nMost of the time, these are the same. For instance, the package e2fsprogs has a name attribute \"e2fsprogs-version\", is\nbound to the variable name e2fsprogs in all-packages.nix, and the Nix expression is in\npkgs\/os-specific\/linux\/e2fsprogs\/default.nix.\n\nThere are a few naming guidelines:\n\n  - The name attribute should be identical to the upstream package name.\n\n  - The name attribute must not contain uppercase letters — e.g., \"mplayer-1.0rc2\" instead of \"MPlayer-1.0rc2\".\n\n  - The version part of the name attribute must start with a digit (following a dash) — e.g., \"hello-0.3.1rc2\".\n\n  - If a package is not a release but a commit from a repository, then the version part of the name must be the date of\n    that (fetched) commit. The date must be in \"YYYY-MM-DD\" format. Also append \"unstable\" to the name - e.g.,\n    \"pkgname-unstable-2014-09-23\".\n\n  - Dashes in the package name should be preserved in new variable names, rather than converted to underscores or camel\n    cased — e.g., http-parser instead of http_parser or httpParser. The hyphenated style is preferred in all three\n    package names.\n\n  - If there are multiple versions of a package, this should be reflected in the variable names in all-packages.nix,\n    e.g. json-c-0-9 and json-c-0-11. If there is an obvious “default” version, make an attribute like json-c =\n    json-c-0-9;. See also Coding conventions\n\nFile naming and organisationNames of files and directories should be in lowercase, with dashes between words — not in\ncamel case. For instance, it should be all-packages.nix, not allPackages.nix or AllPackages.nix.\n\nHierarchyEach package should be stored in its own directory somewhere in the pkgs\/ tree, i.e. in\npkgs\/category\/subcategory\/...\/pkgname. Below are some rules for picking the right category for a package. Many packages\nfall under several categories; what matters is the primary purpose of a package. For example, the libxml2 package builds\nboth a library and some tools; but it’s a library foremost, so it goes under pkgs\/development\/libraries.\n\nWhen in doubt, consider refactoring the pkgs\/ tree, e.g. creating new categories or splitting up an existing category.\n\nIf it’s used to support software development:\n\n  - If it’s a library used by other packages:\n    \n      - development\/libraries (e.g. libxml2)\n\n  - If it’s a compiler:\n    \n      - development\/compilers (e.g. gcc)\n\n  - If it’s an interpreter:\n    \n      - development\/interpreters (e.g. guile)\n\n  - If it’s a (set of) development tool(s):\n    \n      - If it’s a parser generator (including lexers):\n        \n          - development\/tools\/parsing (e.g. bison, flex)\n    \n      - If it’s a build manager:\n        \n          - development\/tools\/build-managers (e.g. gnumake)\n    \n      - Else:\n        \n          - development\/tools\/misc (e.g. binutils)\n\n  - Else:\n    \n      - development\/misc\n\nIf it’s a (set of) tool(s):\n\n(A tool is a relatively small program, especially one intended to be used non-interactively.)\n\n  - If it’s for networking:\n    \n      - tools\/networking (e.g. wget)\n\n  - If it’s for text processing:\n    \n      - tools\/text (e.g. diffutils)\n\n  - If it’s a system utility, i.e., something related or essential to the operation of a system:\n    \n      - tools\/system (e.g. cron)\n\n  - If it’s an archiver (which may include a compression function):\n    \n      - tools\/archivers (e.g. zip, tar)\n\n  - If it’s a compression program:\n    \n      - tools\/compression (e.g. gzip, bzip2)\n\n  - If it’s a security-related program:\n    \n      - tools\/security (e.g. nmap, gnupg)\n\n  - Else:\n    \n      - tools\/misc\n\nIf it’s a shell:\n\n  - shells (e.g. bash)\n\nIf it’s a server:\n\n  - If it’s a web server:\n    \n      - servers\/http (e.g. apache-httpd)\n\n  - If it’s an implementation of the X Windowing System:\n    \n      - servers\/x11 (e.g. xorg — this includes the client libraries and programs)\n\n  - Else:\n    \n      - servers\/misc\n\nIf it’s a desktop environment:\n\n  - desktops (e.g. kde, gnome, enlightenment)\n\nIf it’s a window manager:\n\n  - applications\/window-managers (e.g. awesome, stumpwm)\n\nIf it’s an application:\n\nA (typically large) program with a distinct user interface, primarily used interactively.\n\n  - If it’s a version management system:\n    \n      - applications\/version-management (e.g. subversion)\n\n  - If it’s a terminal emulator:\n    \n      - applications\/terminal-emulators (e.g. alacritty or rxvt or termite)\n\n  - If it’s for video playback \/ editing:\n    \n      - applications\/video (e.g. vlc)\n\n  - If it’s for graphics viewing \/ editing:\n    \n      - applications\/graphics (e.g. gimp)\n\n  - If it’s for networking:\n    \n      - If it’s a mailreader:\n        \n          - applications\/networking\/mailreaders (e.g. thunderbird)\n    \n      - If it’s a newsreader:\n        \n          - applications\/networking\/newsreaders (e.g. pan)\n    \n      - If it’s a web browser:\n        \n          - applications\/networking\/browsers (e.g. firefox)\n    \n      - Else:\n        \n          - applications\/networking\/misc\n\n  - Else:\n    \n      - applications\/misc\n\nIf it’s data (i.e., does not have a straight-forward executable semantics):\n\n  - If it’s a font:\n    \n      - data\/fonts\n\n  - If it’s an icon theme:\n    \n      - data\/icons\n\n  - If it’s related to SGML\/XML processing:\n    \n      - If it’s an XML DTD:\n        \n          - data\/sgml+xml\/schemas\/xml-dtd (e.g. docbook)\n    \n      - If it’s an XSLT stylesheet:\n        \n        (Okay, these are executable...)\n        \n          - data\/sgml+xml\/stylesheets\/xslt (e.g. docbook-xsl)\n\n  - If it’s a theme for a desktop environment, a window manager or a display manager:\n    \n      - data\/themes\n\nIf it’s a game:\n\n  - games\n\nElse:\n\n  - misc\n\nVersioningBecause every version of a package in Nixpkgs creates a potential maintenance burden, old versions of a\npackage should not be kept unless there is a good reason to do so. For instance, Nixpkgs contains several versions of\nGCC because other packages don’t build with the latest version of GCC. Other examples are having both the latest stable\nand latest pre-release version of a package, or to keep several major releases of an application that differ\nsignificantly in functionality.\n\nIf there is only one version of a package, its Nix expression should be named e2fsprogs\/default.nix. If there are\nmultiple versions, this should be reflected in the filename, e.g. e2fsprogs\/1.41.8.nix and e2fsprogs\/1.41.9.nix. The\nversion in the filename should leave out unnecessary detail. For instance, if we keep the latest Firefox 2.0.x and 3.5.x\nversions in Nixpkgs, they should be named firefox\/2.0.nix and firefox\/3.5.nix, respectively (which, at a given point,\nmight contain versions 2.0.0.20 and 3.5.4). If a version requires many auxiliary files, you can use a subdirectory for\neach version, e.g. firefox\/2.0\/default.nix and firefox\/3.5\/default.nix.\n\nAll versions of a package must be included in all-packages.nix to make sure that they evaluate correctly.\n\nFetching SourcesThere are multiple ways to fetch a package source in nixpkgs. The general guideline is that you should\npackage reproducible sources with a high degree of availability. Right now there is only one fetcher which has mirroring\nsupport and that is fetchurl. Note that you should also prefer protocols which have a corresponding proxy environment\nvariable.\n\nYou can find many source fetch helpers in pkgs\/build-support\/fetch*.\n\nIn the file pkgs\/top-level\/all-packages.nix you can find fetch helpers, these have names on the form fetchFrom*. The\nintention of these are to provide snapshot fetches but using the same api as some of the version controlled fetchers\nfrom pkgs\/build-support\/. As an example going from bad to good:\n\n  - Bad: Uses git:\/\/ which won't be proxied.\n    \n    src = fetchgit {\n      url = \"git:\/\/github.com\/NixOS\/nix.git\";\n      rev = \"1f795f9f44607cc5bec70d1300150bfefcef2aae\";\n      sha256 = \"1cw5fszffl5pkpa6s6wjnkiv6lm5k618s32sp60kvmvpy7a2v9kg\";\n    }\n\n  - Better: This is ok, but an archive fetch will still be faster.\n    \n    src = fetchgit {\n      url = \"https:\/\/github.com\/NixOS\/nix.git\";\n      rev = \"1f795f9f44607cc5bec70d1300150bfefcef2aae\";\n      sha256 = \"1cw5fszffl5pkpa6s6wjnkiv6lm5k618s32sp60kvmvpy7a2v9kg\";\n    }\n\n  - Best: Fetches a snapshot archive and you get the rev you want.\n    \n    src = fetchFromGitHub {\n      owner = \"NixOS\";\n      repo = \"nix\";\n      rev = \"1f795f9f44607cc5bec70d1300150bfefcef2aae\";\n      sha256 = \"1i2yxndxb6yc9l6c99pypbd92lfq5aac4klq7y2v93c9qvx2cgpc\";\n    }\n\n    Find the value to put as sha256 by running nix run -f '<nixpkgs>' nix-prefetch-github -c nix-prefetch-github\n    --rev 1f795f9f44607cc5bec70d1300150bfefcef2aae NixOS nix or nix-prefetch-url --unpack\n    https:\/\/github.com\/NixOS\/nix\/archive\/1f795f9f44607cc5bec70d1300150bfefcef2aae.tar.gz.\n\nObtaining source hashPreferred source hash type is sha256. There are several ways to get it.\n\n1.  Prefetch URL (with nix-prefetch-XXX URL, where XXX is one of url, git, hg, cvs, bzr, svn). Hash is printed to\n    stdout.\n\n2.  Prefetch by package source (with nix-prefetch-url '<nixpkgs>' -A PACKAGE.src, where PACKAGE is package attribute\n    name). Hash is printed to stdout.\n    \n    This works well when you've upgraded existing package version and want to find out new hash, but is useless if\n    package can't be accessed by attribute or package has multiple sources (.srcs, architecture-dependent sources, etc).\n\n3.  Upstream provided hash: use it when upstream provides sha256 or sha512 (when upstream provides md5, don't use it,\n    compute sha256 instead).\n    \n    A little nuance is that nix-prefetch-* tools produce hash encoded with base32, but upstream usually provides\n    hexadecimal (base16) encoding. Fetchers understand both formats. Nixpkgs does not standardize on any one format.\n    \n    You can convert between formats with nix-hash, for example:\n    \n    $ nix-hash --type sha256 --to-base32 HASH\n\n4.  Extracting hash from local source tarball can be done with sha256sum. Use nix-prefetch-url file:\/\/\/path\/to\/tarball\n    if you want base32 hash.\n\n5.  Fake hash: set fake hash in package expression, perform build and extract correct hash from error Nix prints.\n    \n    For package updates it is enough to change one symbol to make hash fake. For new packages, you can use\n    lib.fakeSha256, lib.fakeSha512 or any other fake hash.\n    \n    This is last resort method when reconstructing source URL is non-trivial and nix-prefetch-url -A isn’t applicable\n    (for example, one of kodi dependencies). The easiest way then would be replace hash with a fake one and rebuild. Nix\n    build will fail and error message will contain desired hash.\n\nThis method has security problems. Check below for details. \n\nObtaining hashes securelyLet's say Man-in-the-Middle (MITM) sits close to your network. Then instead of fetching source\nyou can fetch malware, and instead of source hash you get hash of malware. Here are security considerations for this\nscenario:\n\n  - http:\/\/ URLs are not secure to prefetch hash from;\n\n  - hashes from upstream (in method 3) should be obtained via secure protocol;\n\n  - https:\/\/ URLs are secure in methods 1, 2, 3;\n\n  - https:\/\/ URLs are not secure in method 5. When obtaining hashes with fake hash method, TLS checks are disabled. So\n    refetch source hash from several different networks to exclude MITM scenario. Alternatively, use fake hash method to\n    make Nix error, but instead of extracting hash from error, extract https:\/\/ URL and prefetch it with method 1.\n\nPatchesPatches available online should be retrieved using fetchpatch.\n\npatches = [\n  (fetchpatch {\n    name = \"fix-check-for-using-shared-freetype-lib.patch\";\n    url = \"http:\/\/git.ghostscript.com\/?p=ghostpdl.git;a=patch;h=8f5d285\";\n    sha256 = \"1f0k043rng7f0rfl9hhb89qzvvksqmkrikmm38p61yfx51l325xr\";\n  })\n];\n\nOtherwise, you can add a .patch file to the nixpkgs repository. In the interest of keeping our maintenance burden to a\nminimum, only patches that are unique to nixpkgs should be added in this way.\n\npatches = [ .\/0001-changes.patch ];\n\nIf you do need to do create this sort of patch file, one way to do so is with git:\n\n1.  Move to the root directory of the source code you're patching.\n    \n    $ cd the\/program\/source\n\n2.  If a git repository is not already present, create one and stage all of the source files.\n    \n    $ git init\n    $ git add .\n\n3.  Edit some files to make whatever changes need to be included in the patch.\n\n4.  Use git to create a diff, and pipe the output to a patch file:\n    \n    $ git diff > nixpkgs\/pkgs\/the\/package\/0001-changes.patch\n\nIf a patch is available online but does not cleanly apply, it can be modified in some fixed ways by using additional\noptional arguments for fetchpatch:\n\n  - stripLen: Remove the first stripLen components of pathnames in the patch.\n  - extraPrefix: Prefix pathnames by this string.\n  - excludes: Exclude files matching this pattern.\n  - includes: Include only files matching this pattern.\n  - revert: Revert the patch.\n\nNote that because the checksum is computed after applying these effects, using or modifying these arguments will have no\neffect unless the sha256 argument is changed as well.\n\nPackage testsTests are important to ensure quality and make reviews and automatic updates easy.\n\nNix package tests are a lightweight alternative to NixOS module tests. They can be used to create simple integration\ntests for packages while the module tests are used to test services or programs with a graphical user interface on a\nNixOS VM. Unittests that are included in the source code of a package should be executed in the checkPhase.\n\nWriting package testsThis is an example using the phoronix-test-suite package with the current best practices.\n\nAdd the tests in passthru.tests to the package definition like this:\n\n{ stdenv, lib, fetchurl, callPackage }:\n\nstdenv.mkDerivation {\n  …\n\n  passthru.tests = {\n    simple-execution = callPackage .\/tests.nix { };\n  };\n\n  meta = { … };\n}\n\nCreate tests.nix in the package directory:\n\n{ runCommand, phoronix-test-suite }:\n\nlet\n  inherit (phoronix-test-suite) pname version;\nin\n\nrunCommand \"${pname}-tests\" { meta.timeout = 3; }\n  ''\n    # automatic initial setup to prevent interactive questions\n    ${phoronix-test-suite}\/bin\/phoronix-test-suite enterprise-setup >\/dev\/null\n    # get version of installed program and compare with package version\n    if [[ `${phoronix-test-suite}\/bin\/phoronix-test-suite version` != *\"${version}\"*  ]]; then\n      echo \"Error: program version does not match package version\"\n      exit 1\n    fi\n    # run dummy command\n    ${phoronix-test-suite}\/bin\/phoronix-test-suite dummy_module.dummy-command >\/dev\/null\n    # needed for Nix to register the command as successful\n    touch $out\n  ''\n\nRunning package testsYou can run these tests with:\n\n$ cd path\/to\/nixpkgs\n$ nix-build -A phoronix-test-suite.tests\n\nExamples of package testsHere are examples of package tests:\n\n  - Jasmin compile test\n  - Lobster compile test\n  - Spacy annotation test\n  - Libtorch test\n  - Multiple tests for nanopb\n" }
,{ "url": "contributing\/submitting-changes\/", "title": "Submitting changes", "text": "Submitting changesMaking patches  - Read Manual (How to write packages for Nix).\n\n  - Fork the Nixpkgs repository on GitHub.\n\n  - Create a branch for your future fix.\n    \n      - You can make branch from a commit of your local nixos-version. That will help you to avoid additional local\n        compilations. Because you will receive packages from binary cache. For example\n        \n        $ nixos-version --hash\n        0998212\n        $ git checkout 0998212\n        $ git checkout -b 'fix\/pkg-name-update'\n\n      - Please avoid working directly on the master branch.\n\n  - Make commits of logical units.\n\n  - If you removed pkgs or made some major NixOS changes, write about it in the release notes for the next stable\n    release. For example nixos\/doc\/manual\/release-notes\/rl-2003.xml.\n\n  - Check for unnecessary whitespace with git diff --check before committing.\n\n  - Format the commit in a following way:\n    \n    (pkg-name | nixos\/<module>): (from -> to | init at version | refactor | etc)\n    Additional information.\n\n      - Examples:\n          - nginx: init at 2.0.1\n          - firefox: 54.0.1 -> 55.0\n          - nixos\/hydra: add bazBaz option\n          - nixos\/nginx: refactor config generation\n\n  - Test your changes. If you work with\n    \n      - nixpkgs:\n        \n          - update pkg\n              - nix-env -i pkg-name -f <path to your local nixpkgs folder>\n          - add pkg\n              - Make sure it’s in pkgs\/top-level\/all-packages.nix\n              - nix-env -i pkg-name -f <path to your local nixpkgs folder>\n          - If you don’t want to install pkg in you profile.\n              - nix-build -A pkg-attribute-name <path to your local nixpkgs folder>\/default.nix and check results in the\n                folder result. It will appear in the same directory where you did nix-build.\n          - If you did nix-env -i pkg-name you can do nix-env -e pkg-name to uninstall it from your system.\n    \n      - NixOS and its modules:\n        \n          - You can add new module to your NixOS configuration file (usually it’s \/etc\/nixos\/configuration.nix). And do\n            sudo nixos-rebuild test -I nixpkgs=<path to your local nixpkgs folder> --fast.\n\n  - If you have commits pkg-name: oh, forgot to insert whitespace: squash commits in this case. Use git rebase -i.\n\n  - Rebase your branch against current master.\n\nSubmitting changes  - Push your changes to your fork of nixpkgs.\n  - Create the pull request\n  - Follow the contribution guidelines.\n\nSubmitting security fixesSecurity fixes are submitted in the same way as other changes and thus the same guidelines\napply.\n\n  - If a new version fixing the vulnerability has been released, update the package;\n\n  - If the security fix comes in the form of a patch and a CVE is available, then add the patch to the Nixpkgs tree, and\n    apply it to the package. The name of the patch should be the CVE identifier, so e.g. CVE-2019-13636.patch; If a\n    patch is fetched the name needs to be set as well, e.g.:\n    \n    (fetchpatch {\n      name = \"CVE-2019-11068.patch\";\n      url = \"https:\/\/gitlab.gnome.org\/GNOME\/libxslt\/commit\/e03553605b45c88f0b4b2980adfbbb8f6fca2fd6.patch\";\n      sha256 = \"0pkpb4837km15zgg6h57bncp66d5lwrlvkr73h0lanywq7zrwhj8\";\n    })\n\nIf a security fix applies to both master and a stable release then, similar to regular changes, they are preferably\ndelivered via master first and cherry-picked to the release branch.\n\nCritical security fixes may by-pass the staging branches and be delivered directly to release branches such as master\nand release-*.\n\nDeprecating\/removing packagesThere is currently no policy when to remove a package.\n\nBefore removing a package, one should try to find a new maintainer or fix smaller issues first.\n\nSteps to remove a package from NixpkgsWe use jbidwatcher as an example for a discontinued project here.\n\n1.  Have Nixpkgs checked out locally and up to date.\n\n2.  Create a new branch for your change, e.g. git checkout -b jbidwatcher\n\n3.  Remove the actual package including its directory, e.g. rm -rf pkgs\/applications\/misc\/jbidwatcher\n\n4.  Remove the package from the list of all packages (pkgs\/top-level\/all-packages.nix).\n\n5.  Add an alias for the package name in pkgs\/top-level\/aliases.nix (There is also pkgs\/misc\/vim-plugins\/aliases.nix.\n    Package sets typically do not have aliases, so we can't add them there.)\n    \n    For example in this case:\n    \n    jbidwatcher = throw \"jbidwatcher was discontinued in march 2021\"; # added 2021-03-15\n\n    The throw message should explain in short why the package was removed for users that still have it installed.\n\n6.  Test if the changes introduced any issues by running nix-env -qaP -f . --show-trace. It should show the list of\n    packages without errors.\n\n7.  Commit the changes. Explain again why the package was removed. If it was declared discontinued upstream, add a link\n    to the source.\n    \n    $ git add pkgs\/applications\/misc\/jbidwatcher\/default.nix pkgs\/top-level\/all-packages.nix pkgs\/top-level\/aliases.nix\n    $ git commit\n\n    Example commit message:\n    \n    jbidwatcher: remove\n    \n    project was discontinued in march 2021. the program does not work anymore because ebay changed the login.\n    \n    https:\/\/web.archive.org\/web\/20210315205723\/http:\/\/www.jbidwatcher.com\/\n\n8.  Push changes to your GitHub fork with git push\n\n9.  Create a pull request against Nixpkgs. Mention the package maintainer.\n\nThis is how the pull request looks like in this case: https:\/\/github.com\/NixOS\/nixpkgs\/pull\/116470\n\nPull Request TemplateThe pull request template helps determine what steps have been made for a contribution so far, and\nwill help guide maintainers on the status of a change. The motivation section of the PR should include any extra details\nthe title does not address and link any existing issues related to the pull request.\n\nWhen a PR is created, it will be pre-populated with some checkboxes detailed below:\n\nTested using sandboxingWhen sandbox builds are enabled, Nix will setup an isolated environment for each build process.\nIt is used to remove further hidden dependencies set by the build environment to improve reproducibility. This includes\naccess to the network during the build outside of fetch* functions and files outside the Nix store. Depending on the\noperating system access to other resources are blocked as well (ex. inter process communication is isolated on Linux);\nsee sandbox in Nix manual for details.\n\nSandboxing is not enabled by default in Nix due to a small performance hit on each build. In pull requests for nixpkgs\npeople are asked to test builds with sandboxing enabled (see Tested using sandboxing in the pull request template)\nbecause inhttps:\/\/nixos.org\/hydra\/ sandboxing is also used.\n\nDepending if you use NixOS or other platforms you can use one of the following methods to enable sandboxing before\nbuilding the package:\n\n  - Globally enable sandboxing on NixOS: add the following to configuration.nix\n    \n    nix.useSandbox = true;\n\n  - Globally enable sandboxing on non-NixOS platforms: add the following to: \/etc\/nix\/nix.conf\n    \n    sandbox = true\n\nBuilt on platform(s)Many Nix packages are designed to run on multiple platforms. As such, it’s important to let the\nmaintainer know which platforms your changes have been tested on. It’s not always practical to test a change on all\nplatforms, and is not required for a pull request to be merged. Only check the systems you tested the build on in this\nsection.\n\nTested via one or more NixOS test(s) if existing and applicable for the change (look inside nixos\/tests)Packages with\nautomated tests are much more likely to be merged in a timely fashion because it doesn’t require as much manual testing\nby the maintainer to verify the functionality of the package. If there are existing tests for the package, they should\nbe run to verify your changes do not break the tests. Tests can only be run on Linux. For more details on writing and\nrunning tests, see the section in the NixOS manual.\n\nTested compilation of all pkgs that depend on this change using nixpkgs-reviewIf you are updating a package’s version,\nyou can use nixpkgs-review to make sure all packages that depend on the updated package still compile correctly. The\nnixpkgs-review utility can look for and build all dependencies either based on uncommited changes with the wip option or\nspecifying a github pull request number.\n\nreview changes from pull request number 12345:\n\nnix run nixpkgs.nixpkgs-review -c nixpkgs-review pr 12345\n\nreview uncommitted changes:\n\nnix run nixpkgs.nixpkgs-review -c nixpkgs-review wip\n\nreview changes from last commit:\n\nnix run nixpkgs.nixpkgs-review -c nixpkgs-review rev HEAD\n\nTested execution of all binary files (usually in .\/result\/bin\/)It’s important to test any executables generated by a\nbuild when you change or create a package in nixpkgs. This can be done by looking in .\/result\/bin and running any files\nin there, or at a minimum, the main executable for the package. For example, if you make a change to texlive, you\nprobably would only check the binaries associated with the change you made rather than testing all of them.\n\nMeets Nixpkgs contribution standardsThe last checkbox is fits CONTRIBUTING.md. The contributing document has detailed\ninformation on standards the Nix community has for commit messages, reviews, licensing of contributions you make to the\nproject, etc... Everyone should read and understand the standards the community has for contributing before submitting a\npull request.\n\nHotfixing pull requests  - Make the appropriate changes in you branch.\n  - Don’t create additional commits, do\n      - git rebase -i\n      - git push --force to your branch.\n\nCommit policy  - Commits must be sufficiently tested before being merged, both for the master and staging branches.\n  - Hydra builds for master and staging should not be used as testing platform, it’s a build farm for changes that have\n    been already tested.\n  - When changing the bootloader installation process, extra care must be taken. Grub installations cannot be rolled\n    back, hence changes may break people’s installations forever. For any non-trivial change to the bootloader please\n    file a PR asking for review, especially from @edolstra.\n\nStaging Workflow\n\nThis GitHub Action brings changes from master to staging-next and from staging-next to staging every 6 hours.\n\nMaster branchThe master branch is the main development branch. It should only see non-breaking commits that do not cause\nmass rebuilds.\n\nStaging branchThe staging branch is a development branch where mass-rebuilds go. It should only see non-breaking\nmass-rebuild commits. That means it is not to be used for testing, and changes must have been well tested already. If\nthe branch is already in a broken state, please refrain from adding extra new breakages.\n\nStaging-next branchThe staging-next branch is for stabilizing mass-rebuilds submitted to the staging branch prior to\nmerging them into master. Mass-rebuilds must go via the staging branch. It must only see non-breaking commits that are\nfixing issues blocking it from being merged into the master  branch.\n\nIf the branch is already in a broken state, please refrain from adding extra new breakages. Stabilize it for a few days\nand then merge into master.\n\nStable release branchesFor cherry-picking a commit to a stable release branch (“backporting”), use git cherry-pick -x\n<original commit> so that the original commit id is included in the commit.\n\nAdd a reason for the backport by using git cherry-pick -xe <original commit> instead when it is not obvious from the\noriginal commit message. It is not needed when it's a minor version update that includes security and bug fixes but\ndon't add new features or when the commit fixes an otherwise broken package.\n\nFor backporting Pull Requests to stable branches, assign label backport <branch> to the original Pull Requests and\nautomation should take care of the rest once the Pull Requests is merged.\n\nHere is an example of a cherry-picked commit message with good reason description:\n\nzfs: Keep trying root import until it works\n\nWorks around #11003.\n\n(cherry picked from commit 98b213a11041af39b39473906b595290e2a4e2f9)\n\nReason: several people cannot boot with ZFS on NVMe\n\nOther examples of reasons are:\n\n  - Previously the build would fail due to, e.g., getaddrinfo not being defined\n  - The previous download links were all broken\n  - Crash when starting on some X11 systems\n\nAcceptable backport criteria\n\nThe stable branch does have some changes which cannot be backported. Most notable are breaking changes. The desire is to\nhave stable users be uninterrupted when updating packages.\n\nHowever, many changes are able to be backported, including:\n\n  - New Packages \/ Modules\n  - Security \/ Patch updates\n  - Version updates which include new functionality (but no breaking changes)\n  - Services which require a client to be up-to-date regardless. (E.g. spotify, steam, or discord)\n  - Security critical applications (E.g. firefox)\n" }
,{ "url": "contributing\/vulnerability-roundup\/", "title": "Vulnerability Roundup", "text": "Vulnerability RoundupIssuesVulnerable packages in Nixpkgs are managed using issues. Currently opened ones can be found\nusing the following:\n\ngithub.com\/NixOS\/nixpkgs\/issues?q=is:issue+is:open+\"Vulnerability+roundup\"\n\nEach issue correspond to a vulnerable version of a package; As a consequence:\n\n  - One issue can contain several CVEs;\n  - One CVE can be shared across several issues;\n  - A single package can be concerned by several issues.\n\nA \"Vulnerability roundup\" issue usually respects the following format:\n\n<link to relevant package search on search.nix.gsc.io>, <link to relevant files in Nixpkgs on GitHub>\n\n<list of related CVEs, their CVSS score, and the impacted NixOS version>\n\n<list of the scanned Nixpkgs versions>\n\n<list of relevant contributors>\n\nNote that there can be an extra comment containing links to previously reported (and still open) issues for the same\npackage.\n\nTriaging and FixingNote: An issue can be a \"false positive\" (i.e. automatically opened, but without the package it\nrefers to being actually vulnerable). If you find such a \"false positive\", comment on the issue an explanation of why it\nfalls into this category, linking as much information as the necessary to help maintainers double check.\n\nIf you are investigating a \"true positive\":\n\n  - Find the earliest patched version or a code patch in the CVE details;\n  - Is the issue already patched (version up-to-date or patch applied manually) in Nixpkgs's master branch?\n      - No:\n          - Submit a security fix;\n          - Once the fix is merged into master, submit the change to the vulnerable release branch(es);\n      - Yes: Backport the change to the vulnerable release branch(es).\n  - When the patch has made it into all the relevant branches (master, and the vulnerable releases), close the relevant\n    issue(s).\n" }
,{ "url": "contributing\/reviewing-contributions\/", "title": "Reviewing contributions", "text": "Reviewing contributions The following section is a draft, and the policy for reviewing is still being discussed in\nissues such as #11166 and #20836. \n\nThe Nixpkgs project receives a fairly high number of contributions via GitHub pull requests. Reviewing and approving\nthese is an important task and a way to contribute to the project.\n\nThe high change rate of Nixpkgs makes any pull request that remains open for too long subject to conflicts that will\nrequire extra work from the submitter or the merger. Reviewing pull requests in a timely manner and being responsive to\nthe comments is the key to avoid this issue. GitHub provides sort filters that can be used to see the most recently and\nthe least recently updated pull requests. We highly encourage looking at this list of ready to merge, unreviewed pull\nrequests.\n\nWhen reviewing a pull request, please always be nice and polite. Controversial changes can lead to controversial\nopinions, but it is important to respect every community member and their work.\n\nGitHub provides reactions as a simple and quick way to provide feedback to pull requests or any comments. The thumb-down\nreaction should be used with care and if possible accompanied with some explanation so the submitter has directions to\nimprove their contribution.\n\npull request reviews should include a list of what has been reviewed in a comment, so other reviewers and mergers can\nknow the state of the review.\n\nAll the review template samples provided in this section are generic and meant as examples. Their usage is optional and\nthe reviewer is free to adapt them to their liking.\n\nPackage updatesA package update is the most trivial and common type of pull request. These pull requests mainly consist\nof updating the version part of the package name and the source hash.\n\nIt can happen that non-trivial updates include patches or more complex changes.\n\nReviewing process:\n\n  - Ensure that the package versioning fits the guidelines.\n  - Ensure that the commit text fits the guidelines.\n  - Ensure that the package maintainers are notified.\n      - CODEOWNERS will make GitHub notify users based on the submitted changes, but it can happen that it misses some\n        of the package maintainers.\n  - Ensure that the meta field information is correct.\n      - License can change with version updates, so it should be checked to match the upstream license.\n      - If the package has no maintainer, a maintainer must be set. This can be the update submitter or a community\n        member that accepts to take maintainership of the package.\n  - Ensure that the code contains no typos.\n  - Building the package locally.\n      - pull requests are often targeted to the master or staging branch, and building the pull request locally when it\n        is submitted can trigger many source builds.\n    \n      - It is possible to rebase the changes on nixos-unstable or nixpkgs-unstable for easier review by running the\n        following commands from a nixpkgs clone.\n        \n        $ git fetch origin nixos-unstable\n        $ git fetch origin pull\/PRNUMBER\/head\n        $ git rebase --onto nixos-unstable BASEBRANCH FETCH_HEAD\n\n          - The first command fetches the nixos-unstable branch.\n          - The second command fetches the pull request changes, PRNUMBER is the number at the end of the pull request\n            title and BASEBRANCH the base branch of the pull request.\n          - The third command rebases the pull request changes to the nixos-unstable branch.\n    \n      - The nixpkgs-review tool can be used to review a pull request content in a single command. PRNUMBER should be\n        replaced by the number at the end of the pull request title. You can also provide the full github pull request\n        url.\n        \n        $ nix-shell -p nixpkgs-review --run \"nixpkgs-review pr PRNUMBER\"\n  - Running every binary.\n\nSample template for a package update review is provided below.\n\n##### Reviewed points\n\n- [ ] package name fits guidelines\n- [ ] package version fits guidelines\n- [ ] package build on ARCHITECTURE\n- [ ] executables tested on ARCHITECTURE\n- [ ] all depending packages build\n\n##### Possible improvements\n\n##### Comments\n\nNew packagesNew packages are a common type of pull requests. These pull requests consists in adding a new nix-expression\nfor a package.\n\nReview process:\n\n  - Ensure that the package versioning fits the guidelines.\n  - Ensure that the commit name fits the guidelines.\n  - Ensure that the meta fields contain correct information.\n      - License must match the upstream license.\n      - Platforms should be set (or the package will not get binary substitutes).\n      - Maintainers must be set. This can be the package submitter or a community member that accepts taking up\n        maintainership of the package.\n  - Report detected typos.\n  - Ensure the package source:\n      - Uses mirror URLs when available.\n      - Uses the most appropriate functions (e.g. packages from GitHub should use fetchFromGitHub).\n  - Building the package locally.\n  - Running every binary.\n\nSample template for a new package review is provided below.\n\n##### Reviewed points\n\n- [ ] package path fits guidelines\n- [ ] package name fits guidelines\n- [ ] package version fits guidelines\n- [ ] package build on ARCHITECTURE\n- [ ] executables tested on ARCHITECTURE\n- [ ] `meta.description` is set and fits guidelines\n- [ ] `meta.license` fits upstream license\n- [ ] `meta.platforms` is set\n- [ ] `meta.maintainers` is set\n- [ ] build time only dependencies are declared in `nativeBuildInputs`\n- [ ] source is fetched using the appropriate function\n- [ ] phases are respected\n- [ ] patches that are remotely available are fetched with `fetchpatch`\n\n##### Possible improvements\n\n##### Comments\n\nModule updatesModule updates are submissions changing modules in some ways. These often contains changes to the options\nor introduce new options.\n\nReviewing process:\n\n  - Ensure that the module maintainers are notified.\n      - CODEOWNERS will make GitHub notify users based on the submitted changes, but it can happen that it misses some\n        of the package maintainers.\n  - Ensure that the module tests, if any, are succeeding.\n  - Ensure that the introduced options are correct.\n      - Type should be appropriate (string related types differs in their merging capabilities, optionSet and string\n        types are deprecated).\n      - Description, default and example should be provided.\n  - Ensure that option changes are backward compatible.\n      - mkRenamedOptionModule and mkAliasOptionModule functions provide way to make option changes backward compatible.\n  - Ensure that removed options are declared with mkRemovedOptionModule\n  - Ensure that changes that are not backward compatible are mentioned in release notes.\n  - Ensure that documentations affected by the change is updated.\n\nSample template for a module update review is provided below.\n\n##### Reviewed points\n\n- [ ] changes are backward compatible\n- [ ] removed options are declared with `mkRemovedOptionModule`\n- [ ] changes that are not backward compatible are documented in release notes\n- [ ] module tests succeed on ARCHITECTURE\n- [ ] options types are appropriate\n- [ ] options description is set\n- [ ] options example is provided\n- [ ] documentation affected by the changes is updated\n\n##### Possible improvements\n\n##### Comments\n\nNew modulesNew modules submissions introduce a new module to NixOS.\n\nReviewing process:\n\n  - Ensure that the module tests, if any, are succeeding.\n  - Ensure that the introduced options are correct.\n      - Type should be appropriate (string related types differs in their merging capabilities, optionSet and string\n        types are deprecated).\n      - Description, default and example should be provided.\n  - Ensure that module meta field is present\n      - Maintainers should be declared in meta.maintainers.\n      - Module documentation should be declared with meta.doc.\n  - Ensure that the module respect other modules functionality.\n      - For example, enabling a module should not open firewall ports by default.\n\nSample template for a new module review is provided below.\n\n##### Reviewed points\n\n- [ ] module path fits the guidelines\n- [ ] module tests succeed on ARCHITECTURE\n- [ ] options have appropriate types\n- [ ] options have default\n- [ ] options have example\n- [ ] options have descriptions\n- [ ] No unneeded package is added to environment.systemPackages\n- [ ] meta.maintainers is set\n- [ ] module documentation is declared in meta.doc\n\n##### Possible improvements\n\n##### Comments\n\nOther submissionsOther type of submissions requires different reviewing steps.\n\nIf you consider having enough knowledge and experience in a topic and would like to be a long-term reviewer for related\nsubmissions, please contact the current reviewers for that topic. They will give you information about the reviewing\nprocess. The main reviewers for a topic can be hard to find as there is no list, but checking past pull requests to see\nwho reviewed or git-blaming the code to see who committed to that topic can give some hints.\n\nContainer system, boot system and library changes are some examples of the pull requests fitting this category.\n\nMerging pull requestsIt is possible for community members that have enough knowledge and experience on a special topic\nto contribute by merging pull requests.\n\nPlease see the discussion in GitHub nixpkgs issue #50105 for information on how to proceed to be granted this level of\naccess.\n\nIn a case a contributor definitively leaves the Nix community, they should create an issue or post on Discourse with\nreferences of packages and modules they maintain so the maintainership can be taken over by other contributors.\n" }
,{ "url": "contributing\/contributing-to-documentation\/", "title": "Contributing to this documentation", "text": "Contributing to this documentationThe sources of the Nixpkgs manual are in the doc subdirectory of the Nixpkgs\nrepository. The manual is still partially written in DocBook but it is progressively being converted to Markdown.\n\nYou can quickly check your edits with make:\n\n$ cd \/path\/to\/nixpkgs\/doc\n$ nix-shell\n[nix-shell]$ make\n\nIf you experience problems, run make debug to help understand the docbook errors.\n\nAfter making modifications to the manual, it's important to build it before committing. You can do that as follows:\n\n$ cd \/path\/to\/nixpkgs\/doc\n$ nix-shell\n[nix-shell]$ make clean\n[nix-shell]$ nix-build .\n\nIf the build succeeds, the manual will be in .\/result\/share\/doc\/nixpkgs\/manual.html.\n\nSyntaxAs per RFC 0072, all new documentation content should be written in CommonMark Markdown dialect.\n\nAdditionally, the following syntax extensions are currently used:\n\n  -  Explicitly defined anchors on headings, to allow linking to sections. These should be always used, to ensure the\n    anchors can be linked even when the heading text changes, and to prevent conflicts between automatically assigned\n    identifiers.\n    \n    It uses the widely compatible header attributes syntax:\n    \n    ## Syntax {#sec-contributing-markup}\n\n  -  Inline anchors, which allow linking arbitrary place in the text (e.g. individual list items, sentences…).\n    \n    They are defined using a hybrid of the link syntax with the attributes syntax known from headings, called bracketed\n    spans:\n    \n    - []{#ssec-gnome-hooks-glib} `glib` setup hook will populate `GSETTINGS_SCHEMAS_PATH` and then `wrapGAppsHook` will prepend it to `XDG_DATA_DIRS`.\n\n  -  If you omit a link text for a link pointing to a section, the text will be substituted automatically. For example,\n    [](#chap-contributing) will result in Contributing to this documentation.\n    \n    This syntax is taken from MyST.\n\n  -  Admonitions, set off from the text to bring attention to something.\n    \n    It uses pandoc’s fenced divs syntax:\n    \n    ::: {.warning}\n    This is a warning\n    :::\n\n    which renders as\n    \n        This is a warning. \n    \n    The following are supported:\n    \n      - caution\n      - important\n      - note\n      - tip\n      - warning\n\n  -  Definition lists, for defining a group of terms:\n    \n    pear\n    :   green or yellow bulbous fruit\n    \n    watermelon\n    :   green fruit with red flesh\n\n    which renders as\n    \n    pear : green or yellow bulbous fruit\n    \n    watermelon : green fruit with red flesh\n\nFor contributing to the legacy parts, please see DocBook: The Definitive Guide or the DocBook rocks! primer.\n" }
]